# OpenAI Agents SDK

The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm.

## Overview and Primitives

The Agents SDK has a very small set of primitives:

*   **Agents:** LLMs equipped with instructions and tools.
*   **Handoffs:** Allow agents to delegate to other agents for specific tasks.
*   **Guardrails:** Enable the inputs to agents to be validated.

In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in tracing that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.

## Why Use the Agents SDK?

The SDK has two driving design principles:

1.  Enough features to be worth using, but few enough primitives to make it quick to learn.
2.  Works great out of the box, but you can customize exactly what happens.

**Main Features:**

*   **Agent loop:** Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.
*   **Python-first:** Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.
*   **Handoffs:** A powerful feature to coordinate and delegate between multiple agents.
*   **Guardrails:** Run input validations and checks in parallel to your agents, breaking early if the checks fail.
*   **Function tools:** Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.
*   **Tracing:** Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.

## Installation

```bash
pip install openai-agents
```

## Quickstart

### Hello World Example

```python
from agents import Agent, Runner

# Note: If running this, ensure you set the OPENAI_API_KEY environment variable
# export OPENAI_API_KEY=sk-...

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Output:
# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
```

### Step-by-Step Guide

1.  **Create a project and virtual environment (You'll only need to do this once):**
    ```bash
    mkdir my_project
    cd my_project
    python -m venv .venv
    ```

2.  **Activate the virtual environment (Do this every time you start a new terminal session):**
    ```bash
    source .venv/bin/activate
    ```

3.  **Install the Agents SDK:**
    ```bash
    pip install openai-agents # or `uv add openai-agents`, etc
    ```

4.  **Set an OpenAI API key:**
    (If you don't have one, follow [these instructions](https://platform.openai.com/docs/quickstart?context=python) to create an OpenAI API key.)
    ```bash
    export OPENAI_API_KEY=sk-...
    ```

5.  **Create your first agent:**
    Agents are defined with instructions, a name, and optional config (such as `model_config`).
    ```python
    from agents import Agent

    agent = Agent(
        name="Math Tutor",
        instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
    )
    ```

6.  **Add a few more agents:**
    Additional agents can be defined in the same way. `handoff_descriptions` provide additional context for determining handoff routing.
    ```python
    from agents import Agent

    history_tutor_agent = Agent(
        name="History Tutor",
        handoff_description="Specialist agent for historical questions",
        instructions="You provide assistance with historical queries. Explain important events and context clearly.",
    )

    math_tutor_agent = Agent(
        name="Math Tutor",
        handoff_description="Specialist agent for math questions",
        instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
    )
    ```

7.  **Define your handoffs:**
    On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task.
    ```python
    triage_agent = Agent(
        name="Triage Agent",
        instructions="You determine which agent to use based on the user's homework question",
        handoffs=[history_tutor_agent, math_tutor_agent]
    )
    ```

8.  **Run the agent orchestration:**
    Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents.
    ```python
    from agents import Runner
    import asyncio # Added import

    async def main():
        result = await Runner.run(triage_agent, "What is the capital of France?")
        print(result.final_output)

    # Example execution (requires running within an async context or using asyncio.run)
    # if __name__ == "__main__":
    #     # Ensure OPENAI_API_KEY is set in the environment
    #     asyncio.run(main())
    ```

9.  **Add a guardrail:**
    You can define custom guardrails to run on the input or output.
    ```python
    from agents import GuardrailFunctionOutput, Agent, Runner, InputGuardrail # Added InputGuardrail
    from pydantic import BaseModel
    import asyncio # Added import

    class HomeworkOutput(BaseModel):
        is_homework: bool
        reasoning: str

    guardrail_agent = Agent(
        name="Guardrail check",
        instructions="Check if the user is asking about homework.",
        output_type=HomeworkOutput,
    )

    # Note: The input guardrail function signature requires ctx, agent, input_data
    async def homework_guardrail(ctx, agent, input_data):
        result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
        final_output = result.final_output_as(HomeworkOutput)
        # Tripwire logic determines if the agent run should be halted.
        # Here, we trigger if it *is* homework (example logic)
        return GuardrailFunctionOutput(
            output_info=final_output,
            tripwire_triggered=final_output.is_homework,
        )
    ```

10. **Put it all together:**
    Let's put it all together and run the entire workflow, using handoffs and the input guardrail.
    ```python
    from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
    from pydantic import BaseModel
    import asyncio

    class HomeworkOutput(BaseModel):
        is_homework: bool
        reasoning: str

    guardrail_agent = Agent(
        name="Guardrail check",
        instructions="Check if the user is asking about homework.",
        output_type=HomeworkOutput,
    )

    math_tutor_agent = Agent(
        name="Math Tutor",
        handoff_description="Specialist agent for math questions",
        instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
    )

    history_tutor_agent = Agent(
        name="History Tutor",
        handoff_description="Specialist agent for historical questions",
        instructions="You provide assistance with historical queries. Explain important events and context clearly.",
    )


    async def homework_guardrail(ctx, agent, input_data):
        # Ensure API key is set if running this inner agent call
        result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
        final_output = result.final_output_as(HomeworkOutput)
        return GuardrailFunctionOutput(
            output_info=final_output,
            tripwire_triggered=final_output.is_homework, # Trigger if it IS homework
        )

    triage_agent = Agent(
        name="Triage Agent",
        instructions="You determine which agent to use based on the user's homework question",
        handoffs=[history_tutor_agent, math_tutor_agent],
        input_guardrails=[
            InputGuardrail(guardrail_function=homework_guardrail),
        ],
    )

    async def main():
        print("--- Running History Question (should pass guardrail) ---")
        try: # Added try/except for potential guardrail trigger
            result = await Runner.run(triage_agent, "who was the first president of the united states?")
            print(result.final_output)
        except Exception as e:
            print(f"Run failed: {e}")

        print("\n--- Running Generic Question (should pass guardrail if logic triggers on homework only) ---")
        try: # Added try/except
            result = await Runner.run(triage_agent, "what is life")
            print(result.final_output)
        except Exception as e:
            print(f"Run failed: {e}")

        print("\n--- Running Math Homework (should trigger guardrail) ---")
        try: # Added try/except
            result_hw = await Runner.run(triage_agent, "Solve 2x+5=11")
            print(result_hw.final_output)
        except Exception as e:
            print(f"Run failed (likely guardrail): {e}")


    if __name__ == "__main__":
        # Ensure OPENAI_API_KEY is set in the environment before running
        import os
        if not os.getenv("OPENAI_API_KEY"):
            print("ERROR: OPENAI_API_KEY environment variable not set.")
        else:
            asyncio.run(main())
    ```

11. **View your traces:**
    To review what happened during your agent run, navigate to the **Trace viewer** in the OpenAI Dashboard to view traces of your agent runs.

### Next Steps (after Quickstart)

Learn how to build more complex agentic flows:

*   Learn about how to configure [Agents](#agents-1).
*   Learn about [running agents](#running-agents-1).
*   Learn about [tools](#tools-1), [guardrails](#guardrails-1), and [models](#models-1).

## Core Concepts in Detail

### Agents

Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.

*   **Basic configuration:** The most common properties of an agent you'll configure are:
    *   `instructions`: also known as a developer message or system prompt.
    *   `model`: which LLM to use, and optional `model_settings` to configure model tuning parameters like temperature, top_p, etc.
    *   `tools`: Tools that the agent can use to achieve its tasks.
    ```python
    from agents import Agent, ModelSettings, function_tool

    @function_tool
    def get_weather(city: str) -> str:
        """Gets the weather for a given city.""" # Added docstring for description
        return f"The weather in {city} is sunny"

    agent = Agent(
        name="Haiku agent",
        instructions="Always respond in haiku form",
        model="o3-mini", # Example model
        tools=[get_weather],
        model_settings=ModelSettings(temperature=0.7) # Example settings
    )
    ```

*   **Context:**
    *   Agents are generic on their context type (`Agent[TContext]`).
    *   Context is a dependency-injection tool: it's an object you create and pass to `Runner.run()`, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run.
    *   You can provide any Python object as the context.
    *   The context object is **not** sent to the LLM. It is purely a local object.
    ```python
    from dataclasses import dataclass
    from agents import Agent, RunContextWrapper, Runner, function_tool # Added imports
    import asyncio # Added import

    # Define a context class
    @dataclass
    class UserContext:
        uid: str
        is_pro_user: bool
        name: str # Added name for dynamic instructions example

        async def fetch_purchases(self): # Example method
            print(f"Fetching purchases for user {self.uid}")
            # In real life, fetch from DB or API
            # Example return structure
            class Purchase: pass
            return [] # Return empty list for example

    # Define an agent that uses this context type
    # Note the type hint Agent[UserContext]
    agent_with_context = Agent[UserContext](
        name="Contextual Agent",
        instructions="Use user context if needed.",
        model="o3-mini" # Specify a model
    )

    # Example tool using the context
    @function_tool
    async def get_user_info(wrapper: RunContextWrapper[UserContext]) -> str:
        """Retrieves information about the user from context."""
        user_id = wrapper.context.uid
        is_pro = wrapper.context.is_pro_user
        purchases = await wrapper.context.fetch_purchases()
        return f"User {user_id} (Pro: {is_pro}) has {len(purchases)} purchases."

    # Agent using the tool
    agent_using_tool_and_context = Agent[UserContext](
        name="Info Assistant",
        instructions="Use tools to get user information.",
        tools=[get_user_info],
        model="o3-mini" # Specify a model
    )

    # Example of running with context
    # async def main_context_example():
    #     user_ctx = UserContext(uid="user_abc", is_pro_user=True, name="Alice")
    #     result = await Runner.run(
    #         agent_using_tool_and_context,
    #         "Tell me about the current user.",
    #         context=user_ctx
    #     )
    #     print(result.final_output)

    # if __name__ == "__main__":
    #     # Ensure OPENAI_API_KEY is set
    #     # asyncio.run(main_context_example())
    ```
    (See also [Context Management](#context-management-1))

*   **Output types:**
    *   By default, agents produce plain text (i.e. `str`) outputs.
    *   If you want the agent to produce a particular type of output, you can use the `output_type` parameter.
    *   A common choice is to use Pydantic objects, but we support any type that can be wrapped in a Pydantic `TypeAdapter` - dataclasses, lists, `TypedDict`, etc.
    ```python
    from pydantic import BaseModel
    from agents import Agent

    class CalendarEvent(BaseModel):
        name: str
        date: str
        participants: list[str]

    agent_structured_output = Agent(
        name="Calendar extractor",
        instructions="Extract calendar events from text",
        output_type=CalendarEvent,
        model="o3-mini" # Specify model
    )
    ```
    *   **Note:** When you pass an `output_type`, that tells the model to use structured outputs instead of regular plain text responses.

*   **Handoffs:**
    *   Handoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant.
    *   This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task.
    *   Read more in the [handoffs documentation](#handoffs-1).
    ```python
    from agents import Agent

    # Assume these agents are defined elsewhere
    booking_agent = Agent(name="Booking Agent", instructions="...")
    refund_agent = Agent(name="Refund Agent", instructions="...")

    triage_agent_handoffs = Agent(
        name="Triage agent",
        instructions=(
            "Help the user with their questions."
            "If they ask about booking, handoff to the booking agent."
            "If they ask about refunds, handoff to the refund agent."
        ),
        handoffs=[booking_agent, refund_agent],
        model="o3-mini" # Specify model
    )
    ```

*   **Dynamic instructions:**
    *   In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function.
    *   The function will receive the agent context (`RunContextWrapper[TContext]`) and the agent instance (`Agent[TContext]`), and must return the prompt string.
    *   Both regular and async functions are accepted.
    ```python
    from agents import Agent, RunContextWrapper
    # Assuming UserContext is defined as above

    def dynamic_instructions_func(
        context: RunContextWrapper[UserContext], agent: Agent[UserContext]
    ) -> str:
        # Access context data to build the prompt
        return f"The user's name is {context.context.name}. Help them with their questions."

    agent_dynamic_instructions = Agent[UserContext](
        name="Dynamic Triage agent",
        instructions=dynamic_instructions_func, # Pass the function here
        model="o3-mini" # Specify model
        # Add tools, handoffs etc. as needed
    )
    ```

*   **Lifecycle events (hooks):**
    *   Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur.
    *   You can hook into the agent lifecycle with the `hooks` property. Subclass the `AgentHooks` class, and override the methods you're interested in.
    *   See [Lifecycle](#lifecycle-1) section.

*   **Guardrails:**
    *   Guardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance.
    *   Read more in the [guardrails documentation](#guardrails-1).

*   **Cloning/copying agents:**
    *   By using the `clone()` method on an agent, you can duplicate an `Agent`, and optionally change any properties you like.
    ```python
    from agents import Agent

    pirate_agent_clone = Agent(
        name="Pirate",
        instructions="Write like a pirate",
        model="o3-mini",
    )

    # Clone and modify
    robot_agent_clone = pirate_agent_clone.clone(
        name="Robot",
        instructions="Write like a robot",
        # model remains "o3-mini" unless specified otherwise
    )

    print(f"Original Agent: {pirate_agent_clone.name}, Instructions: {pirate_agent_clone.instructions}")
    print(f"Cloned Agent: {robot_agent_clone.name}, Instructions: {robot_agent_clone.instructions}")
    ```

*   **Forcing tool use:**
    *   Supplying a list of tools doesn't always mean the LLM will use a tool.
    *   You can force tool use by setting `ModelSettings.tool_choice`. Valid values are:
        1.  `"auto"` (default): LLM decides whether or not to use a tool.
        2.  `"required"`: Requires the LLM to use a tool (but it can intelligently decide which tool).
        3.  `"none"`: Requires the LLM to not use a tool.
        4.  Setting a specific string e.g. `"my_tool"`: Requires the LLM to use that specific tool.
    *   **Note:** To prevent infinite loops, the framework automatically resets `tool_choice` to `"auto"` after a tool call. This behavior is configurable via `agent.reset_tool_choice`. The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of `tool_choice`, ad infinitum.
    *   If you want the Agent to completely stop after a tool call (rather than continuing with `auto` mode), you can set `agent.tool_use_behavior="stop_on_first_tool"` which will directly use the tool output as the final response without further LLM processing.

### Running Agents

You can run agents via the `Runner` class. You have 3 options:

1.  `Runner.run()`: Runs async and returns a `RunResult`.
2.  `Runner.run_sync()`: A sync method that just runs `.run()` under the hood. Returns `RunResult`.
3.  `Runner.run_streamed()`: Runs async and returns a `RunResultStreaming`. It calls the LLM in streaming mode, and streams those events to you as they are received.

```python
from agents import Agent, Runner
import asyncio # Added import

async def main_runner():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant", model="o3-mini")

    # Ensure OPENAI_API_KEY is set
    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Output:
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance.

# if __name__ == "__main__":
#     asyncio.run(main_runner())
```

Read more in the [results guide](#results-1).

*   **The agent loop:**
    *   When you use the `run` method in `Runner`, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.
    *   The runner then runs a loop:
        1.  We call the LLM for the current agent, with the current input.
        2.  The LLM produces its output.
            *   a. If the LLM returns a `final_output`, the loop ends and we return the result.
            *   b. If the LLM does a handoff, we update the current agent and input, and re-run the loop.
            *   c. If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.
        3.  If we exceed the `max_turns` passed, we raise a `MaxTurnsExceeded` exception.
    *   **Note:** The rule for whether the LLM output is considered as a "final output" is that it produces text output with the desired type, and there are no tool calls.

*   **Streaming:**
    *   Streaming allows you to additionally receive streaming events as the LLM runs.
    *   Once the stream is done, the `RunResultStreaming` will contain the complete information about the run, including all the new outputs produced.
    *   You can call `.stream_events()` for the streaming events.
    *   Read more in the [streaming guide](#streaming-1).

*   **Run config (`run_config` parameter for `Runner.run*` methods):**
    *   Lets you configure some global settings for the agent run:
        *   `model`: Allows setting a global LLM model to use, irrespective of what model each Agent has.
        *   `model_provider`: A model provider for looking up model names, which defaults to `OpenAIProvider`.
        *   `model_settings`: Overrides agent-specific settings. For example, you can set a global temperature or top_p.
        *   `input_guardrails`, `output_guardrails`: A list of input or output guardrails to include on all runs.
        *   `handoff_input_filter`: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in `Handoff.input_filter` for more details.
        *   `tracing_disabled`: Allows you to disable tracing for the entire run.
        *   `trace_include_sensitive_data`: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.
        *   `workflow_name`, `trace_id`, `group_id`: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting `workflow_name`. The group ID is an optional field that lets you link traces across multiple runs.
        *   `trace_metadata`: Metadata to include on all traces.

*   **Conversations/chat threads:**
    *   Calling any of the `run` methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:
        1.  User turn: user enters text
        2.  Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.
    *   At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output.
    *   Either way, the user might then ask a followup question, in which case you can call the `run` method again.
    *   You can use the base `RunResultBase.to_input_list()` method to get the inputs for the next turn.
    ```python
    from agents import Agent, Runner, trace
    import asyncio
    import uuid # Added import

    async def main_conversation():
        agent = Agent(name="Assistant", instructions="Reply very concisely.", model="o3-mini")
        thread_id = str(uuid.uuid4()) # Generate a unique ID for the conversation thread

        # Ensure API key is set
        with trace(workflow_name="Conversation", group_id=thread_id):
            # First turn
            print("--- Turn 1 ---")
            result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
            print(f"Output: {result1.final_output}\n")
            # Expected: San Francisco

            # Second turn
            print("--- Turn 2 ---")
            # Get previous history and add new user message
            new_input = result1.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
            # Run the agent again with the updated input list
            result2 = await Runner.run(agent, new_input)
            print(f"Output: {result2.final_output}\n")
            # Expected: California

    # if __name__ == "__main__":
    #     asyncio.run(main_conversation())
    ```

*   **Exceptions:**
    *   The SDK raises exceptions in certain cases. The full list is in `agents.exceptions`. As an overview:
        *   `AgentsException`: Base class for all exceptions raised in the SDK.
        *   `MaxTurnsExceeded`: Raised when the run exceeds the `max_turns` passed to the run methods.
        *   `ModelBehaviorError`: Raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.
        *   `UserError`: Raised when you (the person writing code using the SDK) make an error using the SDK.
        *   `InputGuardrailTripwireTriggered`, `OutputGuardrailTripwireTriggered`: Raised when a guardrail is tripped.

### Results

When you call the `Runner.run` methods, you either get a:

*   `RunResult` if you call `run` or `run_sync`
*   `RunResultStreaming` if you call `run_streamed`

Both of these inherit from `RunResultBase`, which is where most useful information is present.

*   **Final output (`final_output` property):**
    *   Contains the final output of the *last* agent that ran. This is either:
        *   A `str`, if the last agent didn't have an `output_type` defined.
        *   An object of type `last_agent.output_type`, if the agent had an output type defined.
    *   **Note:** `final_output` is of type `Any`. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types. Use `result.final_output_as(ExpectedType)` for safer access.

*   **Inputs for the next turn (`to_input_list()` method):**
    *   You can use `result.to_input_list()` to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run.
    *   This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.

*   **Last agent (`last_agent` property):**
    *   Contains the last agent (`Agent` object) that ran.
    *   Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.

*   **New items (`new_items` property):**
    *   Contains the new items generated during the run. The items are `RunItem`s. A run item wraps the raw item generated by the LLM.
        *   `MessageOutputItem`: Indicates a message from the LLM. The raw item is the message generated.
        *   `HandoffCallItem`: Indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
        *   `HandoffOutputItem`: Indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
        *   `ToolCallItem`: Indicates that the LLM invoked a tool.
        *   `ToolCallOutputItem`: Indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
        *   `ReasoningItem`: Indicates a reasoning item from the LLM. The raw item is the reasoning generated.

*   **Other information:**
    *   **Guardrail results:** The `input_guardrail_results` and `output_guardrail_results` properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.
    *   **Raw responses:** The `raw_responses` property contains the `ModelResponse`s generated by the LLM.
    *   **Original input:** The `input` property contains the original input you provided to the `run` method. In most cases you won't need this, but it's available in case you do.

### Streaming

Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.

To stream, you can call `Runner.run_streamed()`, which will give you a `RunResultStreaming`. Calling `result.stream_events()` gives you an async stream of `StreamEvent` objects, which are described below.

*   **Raw response events (`RawResponsesStreamEvent`):**
    *   Raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a `type` (like `response.created`, `response.output_text.delta`, etc) and `data`.
    *   These events are useful if you want to stream response messages to the user as soon as they are generated.
    *   Example: Outputting text token-by-token.
    ```python
    import asyncio
    # Ensure openai is installed: pip install openai
    from openai.types.responses import ResponseTextDeltaEvent
    from agents import Agent, Runner

    async def main_stream_raw():
        agent = Agent(
            name="Joker",
            instructions="You are a helpful assistant.",
            model="o3-mini" # Use a model supporting streaming
        )

        # Ensure OPENAI_API_KEY is set
        print("--- Streaming Raw Text Deltas ---")
        result_stream = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
        async for event in result_stream.stream_events():
            if event.type == "raw_response_event":
                # Check the structure of event.data to safely access delta
                data = event.data
                if hasattr(data, 'delta') and isinstance(data.delta, str):
                    print(data.delta, end="", flush=True)
                elif hasattr(data, 'type') and data.type == 'response.output_text.delta' and hasattr(data, 'delta'):
                    # Handle potential nested structure if format changes slightly
                    print(data.delta, end="", flush=True)
        print("\n--- Stream Complete ---")
        # Wait for final result if needed
        # final_result = await result_stream
        # print(f"Final Result:\n{final_result.final_output}")

    # if __name__ == "__main__":
    #     import os
    #     if not os.getenv("OPENAI_API_KEY"):
    #         print("ERROR: OPENAI_API_KEY environment variable not set.")
    #     else:
    #         asyncio.run(main_stream_raw())
    ```

*   **Run item events (`RunItemStreamEvent`) and agent events (`AgentUpdatedStreamEvent`):**
    *   `RunItemStreamEvent`s are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of "message generated", "tool ran", etc, instead of each token.
    *   `AgentUpdatedStreamEvent` gives you updates when the current agent changes (e.g. as the result of a handoff).
    *   Example: Ignoring raw events and streaming semantic updates.
    ```python
    import asyncio
    import random
    from agents import Agent, ItemHelpers, Runner, function_tool

    @function_tool
    def how_many_jokes() -> int:
        """Returns a random number of jokes to tell."""
        print("[Tool Call] how_many_jokes called")
        num = random.randint(1, 3) # Reduced for brevity
        print(f"[Tool Call] Returning {num}")
        return num

    async def main_stream_semantic():
        agent = Agent(
            name="Joker",
            instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
            tools=[how_many_jokes],
            model="o3-mini" # Use model supporting tools
        )

        # Ensure OPENAI_API_KEY is set
        print("--- Streaming Semantic Events ---")
        result_stream = Runner.run_streamed(
            agent,
            input="Hello",
        )
        print("=== Run starting ===")

        async for event in result_stream.stream_events():
            # We'll ignore the raw responses event deltas
            if event.type == "raw_response_event":
                continue
            # When the agent updates, print that
            elif event.type == "agent_updated_stream_event":
                print(f"--- Agent updated: {event.new_agent.name} ---")
                continue
            # When items are generated, print them
            elif event.type == "run_item_stream_event":
                item = event.item
                if item.type == "tool_call_item":
                    # Safely access tool name if needed
                    tool_name = getattr(getattr(item, 'raw_item', None), 'name', 'UnknownTool')
                    print(f"-- Tool '{tool_name}' was called --")
                elif item.type == "tool_call_output_item":
                    output_val = getattr(item, 'output', 'N/A')
                    print(f"-- Tool output: {output_val} --")
                elif item.type == "message_output_item":
                    print(f"-- Message output:\n{ItemHelpers.text_message_output(item)}--")
                elif item.type == "reasoning_item":
                    print(f"-- Reasoning item created --") # Example
                elif item.type == "handoff_call_item":
                    print(f"-- Handoff requested --") # Example
                elif item.type == "handoff_output_item":
                    print(f"-- Handoff occurred --") # Example
                else:
                    print(f"-- Other item type: {item.type} --")
                    pass # Ignore other event types

        print("=== Run complete ===")
        # Wait for the final result to be available
        final_result = await result_stream
        print(f"\nFinal Output:\n{final_result.final_output}")


    # if __name__ == "__main__":
    #     import os
    #     if not os.getenv("OPENAI_API_KEY"):
    #         print("ERROR: OPENAI_API_KEY environment variable not set.")
    #     else:
    #         asyncio.run(main_stream_semantic())
    ```

### Tools

Tools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK:

*   **Hosted tools:** these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools.
*   **Function calling:** these allow you to use any Python function as a tool.
*   **Agents as tools:** this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them.

*   **Hosted tools:**
    *   OpenAI offers a few built-in tools when using the `OpenAIResponsesModel`:
        *   `WebSearchTool`: Lets an agent search the web.
        *   `FileSearchTool`: Allows retrieving information from your OpenAI Vector Stores.
        *   `ComputerTool`: Allows automating computer use tasks.
    ```python
    from agents import Agent, FileSearchTool, Runner, WebSearchTool
    import asyncio

    # NOTE: Requires OPENAI_API_KEY and a valid VECTOR_STORE_ID
    # Replace 'vs_...' with your actual Vector Store ID
    VECTOR_STORE_ID = "vs_..."

    agent_hosted_tools = Agent(
        name="Assistant",
        model="o3-mini", # Requires a model supporting Responses API
        tools=[
            WebSearchTool(),
            FileSearchTool(
                max_num_results=3,
                vector_store_ids=[VECTOR_STORE_ID],
            ),
            # ComputerTool(), # Requires specific setup not covered here
        ],
    )

    async def main_hosted_tools():
        # Ensure prerequisites are met (API key, vector store)
        try:
            result = await Runner.run(agent_hosted_tools, "Which coffee shop should I go to, taking into account my preferences and the weather today in SF?")
            print("--- Hosted Tools Run Result ---")
            print(result.final_output)
        except Exception as e:
            print(f"Error running hosted tools agent: {e}")
            print("Ensure your API key is valid and the Vector Store ID exists.")

    # if __name__ == "__main__":
    #     # asyncio.run(main_hosted_tools())
    ```

*   **Function tools:**
    *   You can use any Python function as a tool. The Agents SDK will setup the tool automatically:
        *   The name of the tool will be the name of the Python function (or you can provide a `name_override`).
        *   Tool description will be taken from the docstring of the function (or you can provide a `description_override`).
        *   The schema for the function inputs is automatically created from the function's arguments.
        *   Descriptions for each input are taken from the docstring of the function, unless disabled (`use_docstring_info=False`).
    *   We use Python's `inspect` module to extract the function signature, along with `griffe` to parse docstrings and `pydantic` for schema creation.
    ```python
    import json
    from typing_extensions import TypedDict, Any
    from agents import Agent, FunctionTool, RunContextWrapper, function_tool

    # Define a TypedDict for structured input
    class Location(TypedDict):
        lat: float
        long: float

    # Async function tool example
    @function_tool
    async def fetch_weather(location: Location) -> str:
        """Fetch the weather for a given location.

        Args:
            location: The location to fetch the weather for. Contains lat and long.
        """
        # In real life, we'd fetch the weather from a weather API
        print(f"[Tool Call] fetch_weather called with location: {location}")
        return f"sunny at {location['lat']}, {location['long']}"

    # Sync function tool example with context and name override
    @function_tool(name_override="fetch_data")
    def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
        """Read the contents of a file.

        Args:
            path: The path to the file to read.
            directory: The directory to read the file from. Optional.
        """
        # In real life, we'd read the file from the file system
        print(f"[Tool Call] read_file called for path: {path} in dir: {directory}. Context type: {type(ctx.context)}")
        return f"<file contents for {path}>"

    # Agent using these tools
    agent_function_tools = Agent(
        name="Assistant",
        instructions="Use tools to fetch weather or read files.",
        tools=[fetch_weather, read_file],
        model="o3-mini" # Use model supporting function calling
    )

    # Example of inspecting the generated tool schema
    print("--- Inspecting Generated Function Tool Schemas ---")
    for tool in agent_function_tools.tools:
        if isinstance(tool, FunctionTool):
            print(f"Tool Name: {tool.name}")
            print(f"Description: {tool.description}")
            print("Schema:")
            # Ensure params_json_schema is not None before dumping
            if tool.params_json_schema:
                print(json.dumps(tool.params_json_schema, indent=2))
            else:
                print("  (No parameters)")
            print("-" * 20)
    ```
    *(Expand to see output refers to running the inspection loop above)*

*   **Custom function tools:**
    *   Sometimes, you don't want to use a Python function as a tool. You can directly create a `FunctionTool` if you prefer.
    *   You'll need to provide:
        *   `name`
        *   `description`
        *   `params_json_schema`: The JSON schema for the arguments.
        *   `on_invoke_tool`: An async function that receives the context (`RunContextWrapper[Any]`) and the arguments as a JSON string, and must return the tool output as a string.
    ```python
    from typing import Any
    from pydantic import BaseModel
    from agents import RunContextWrapper, FunctionTool
    import json # Added import

    # Example function that the tool will call internally
    def do_some_work(data: str) -> str:
        print(f"[Custom Tool] Doing work with: {data}")
        return "work completed successfully"

    # Pydantic model for arguments schema
    class FunctionArgs(BaseModel):
        username: str
        age: int

    # Async function to handle the tool invocation
    async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
        """Parses args and calls the underlying work function."""
        print(f"[Custom Tool] Invoked with args: {args}. Context type: {type(ctx.context)}")
        try:
            parsed = FunctionArgs.model_validate_json(args)
            # Call the actual logic
            result = do_some_work(data=f"{parsed.username} is {parsed.age} years old")
            return result
        except Exception as e:
            print(f"[Custom Tool] Error parsing arguments: {e}")
            return f"Error processing arguments: {e}" # Return error message to LLM

    # Create the FunctionTool instance manually
    custom_tool = FunctionTool(
        name="process_user",
        description="Processes extracted user data provided as username and age.",
        params_json_schema=FunctionArgs.model_json_schema(), # Generate schema from Pydantic model
        on_invoke_tool=run_function, # Provide the async invocation handler
    )

    # Example Agent using this custom tool
    # agent_with_custom_tool = Agent(
    #     name="Custom Tool User Agent",
    #     instructions="Use the process_user tool when given user details.",
    #     tools=[custom_tool],
    #     model="o3-mini"
    # )

    # Example run (requires async context and API key)
    # async def main_custom_tool():
    #     result = await Runner.run(agent_with_custom_tool, "Process Bob who is 42.")
    #     print(f"Custom tool agent output: {result.final_output}")
    # if __name__ == "__main__":
    #     asyncio.run(main_custom_tool())
    ```

*   **Automatic argument and docstring parsing:**
    *   As mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:
        1.  The signature parsing is done via the `inspect` module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.
        2.  We use `griffe` to parse docstrings. Supported docstring formats are `google`, `sphinx` and `numpy`. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling `function_tool`. You can also disable docstring parsing by setting `use_docstring_info` to `False`.
    *   The code for the schema extraction lives in `agents.function_schema`.

*   **Agents as tools:**
    *   In some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.
    ```python
    from agents import Agent, Runner
    import asyncio

    # Define specialist agents
    spanish_agent_as_tool = Agent(
        name="Spanish agent",
        instructions="You translate the user's message to Spanish",
        model="o3-mini"
    )

    french_agent_as_tool = Agent(
        name="French agent",
        instructions="You translate the user's message to French",
        model="o3-mini"
    )

    # Define orchestrator agent using specialist agents as tools
    orchestrator_agent_as_tool = Agent(
        name="orchestrator_agent",
        instructions=(
            "You are a translation agent. You use the tools given to you to translate. "
            "If asked for multiple translations, you call the relevant tools."
        ),
        model="o3-mini", # Use a model good at tool use and orchestration
        tools=[
            spanish_agent_as_tool.as_tool(
                tool_name="translate_to_spanish", # Provide a name for the tool
                tool_description="Translate the user's message to Spanish", # Description for LLM
            ),
            french_agent_as_tool.as_tool(
                tool_name="translate_to_french",
                tool_description="Translate the user's message to French",
            ),
        ],
    )

    async def main_agents_as_tools():
        # Ensure API key is set
        print("--- Testing Agents as Tools ---")
        result_es = await Runner.run(orchestrator_agent_as_tool, input="Say 'Hello, how are you?' in Spanish.")
        print(f"Spanish Result: {result_es.final_output}")

        # Example asking for both
        # result_multi = await Runner.run(orchestrator_agent_as_tool, input="Say 'Good morning' in Spanish and French.")
        # print(f"Multi-language Result: {result_multi.final_output}")

    # if __name__ == "__main__":
    #     asyncio.run(main_agents_as_tools())
    ```
    *   **Customizing tool-agents:** The `agent.as_tool` function is a convenience method. It doesn't support all configuration though; for example, you can't set `max_turns`. For advanced use cases, use `Runner.run` directly in your tool implementation:
    ```python
    from agents import Agent, Runner, function_tool, RunConfig # Added RunConfig

    # Example of a tool that runs another agent with custom settings
    @function_tool
    async def run_my_agent_custom() -> str:
      """A tool that runs the 'My agent' with custom configs like max_turns."""
      print("[Tool Call] run_my_agent_custom invoked")

      # Define the agent to be run by the tool
      agent_to_run = Agent(name="My agent", instructions="Perform a specific sub-task...", model="o3-mini")

      # Example custom run config for the inner run
      custom_inner_run_config = RunConfig(workflow_name="InnerAgentRun")

      # Call Runner.run from within the tool function
      result = await Runner.run(
          agent_to_run,
          input="Execute the sub-task.", # Provide relevant input
          max_turns=5, # Custom max_turns
          run_config=custom_inner_run_config # Custom run config
      )

      print(f"[Tool Call] Inner agent finished with output: {result.final_output}")
      return str(result.final_output) # Return the result as a string

    # Example orchestrator using this advanced tool
    # orchestrator_advanced_tool = Agent(
    #     name="Advanced Orchestrator",
    #     instructions="Use the run_my_agent_custom tool.",
    #     tools=[run_my_agent_custom],
    #     model="o3-mini"
    # )
    # async def main_advanced_tool():
    #     res = await Runner.run(orchestrator_advanced_tool, "Run the custom agent tool.")
    #     print(f"Advanced Tool Orchestrator Output: {res.final_output}")
    # if __name__ == "__main__":
    #     asyncio.run(main_advanced_tool())
    ```

*   **Handling errors in function tools:**
    *   When you create a function tool via `@function_tool`, you can pass a `failure_error_function`. This is a function that provides an error response to the LLM in case the tool call crashes.
        *   By default (i.e. if you don't pass anything), it runs a `default_tool_error_function` which tells the LLM an error occurred.
        *   If you pass your own error function, it runs that instead, and sends the response to the LLM.
        *   If you explicitly pass `None`, then any tool call errors will be re-raised for you to handle. This could be a `ModelBehaviorError` if the model produced invalid JSON, or a `UserError` if your code crashed, etc.
    *   If you are manually creating a `FunctionTool` object, then you must handle errors inside the `on_invoke_tool` function.

### Model Context Protocol (MCP)

*   The Model Context Protocol (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs: MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.
*   The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.

*   **MCP servers:**
    *   Currently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:
        1.  `stdio` servers run as a subprocess of your application. You can think of them as running "locally".
        2.  `HTTP over SSE` servers run remotely. You connect to them via a URL.
    *   You can use the `MCPServerStdio` and `MCPServerSse` classes to connect to these servers.
    *   Example using the official MCP filesystem server:
    ```python
    # from agents.mcp import MCPServerStdio # Correct import path might differ
    # import asyncio
    # import os
    #
    # async def mcp_filesystem_example():
    #     # Ensure npx and the MCP server package are available in your environment
    #     samples_dir = "." # Directory the server should access
    #     if not os.path.exists(samples_dir):
    #         os.makedirs(samples_dir)
    #         # Create a dummy file for testing
    #         with open(os.path.join(samples_dir, "test.txt"), "w") as f:
    #             f.write("Hello from MCP!")
    #
    #     print("--- Starting MCP Filesystem Server ---")
    #     try:
    #         async with MCPServerStdio(
    #             params={
    #                 "command": "npx",
    #                 # Note: Using -y might install the package if not present
    #                 "args": ["-y", "@modelcontextprotocol/server-filesystem", samples_dir],
    #             }
    #         ) as server:
    #             print(f"Connected to MCP server: {server.name}")
    #             tools = await server.list_tools()
    #             print("Available MCP Tools:")
    #             for tool in tools:
    #                 # Assuming tool objects have name/description attributes
    #                 print(f"  - {getattr(tool, 'name', 'Unknown')}: {getattr(tool, 'description', 'No description')}")
    #
    #             # Example Agent using this server
    #             # mcp_agent = Agent(
    #             #     name="MCP Assistant",
    #             #     instructions="Use the tools to list files in the directory.",
    #             #     mcp_servers=[server],
    #             #     model="o3-mini" # Requires model supporting function calling
    #             # )
    #             # result = await Runner.run(mcp_agent, "List files.")
    #             # print(f"MCP Agent Output: {result.final_output}")
    #
    #     except FileNotFoundError:
    #         print("Error: 'npx' command not found. Ensure Node.js and npm/npx are installed and in PATH.")
    #     except Exception as e:
    #         print(f"An error occurred with the MCP server: {e}")
    #
    # if __name__ == "__main__":
    #     # asyncio.run(mcp_filesystem_example())
    ```

*   **Using MCP servers:**
    *   MCP servers can be added to Agents via `Agent(mcp_servers=[...])`.
    *   The Agents SDK will call `list_tools()` on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools.
    *   When the LLM calls a tool from an MCP server, the SDK calls `call_tool()` on that server.
    ```python
    # from agents import Agent
    # from agents.mcp import MCPServer # Assuming MCPServer base class import
    #
    # # Assume mcp_server_1 and mcp_server_2 are initialized MCPServer instances
    # mcp_server_1: MCPServer = ...
    # mcp_server_2: MCPServer = ...
    #
    # agent_with_mcp = Agent(
    #     name="Assistant",
    #     instructions="Use the tools to achieve the task",
    #     mcp_servers=[mcp_server_1, mcp_server_2],
    #     model="o3-mini" # Specify model
    # )
    ```

*   **Caching:**
    *   Every time an Agent runs, it calls `list_tools()` on the MCP server. This can be a latency hit, especially if the server is a remote server.
    *   To automatically cache the list of tools, you can pass `cache_tools_list=True` to both `MCPServerStdio` and `MCPServerSse`.
    *   You should only do this if you're certain the tool list will not change.
    *   If you want to invalidate the cache, you can call `invalidate_tools_cache()` on the servers.

*   **End-to-end examples:**
    *   View complete working examples at `examples/mcp`.

*   **Tracing:**
    *   Tracing automatically captures MCP operations, including:
        1.  Calls to the MCP server to list tools
        2.  MCP-related info on function calls

### Handoffs

Handoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc.

Handoffs are represented as tools to the LLM. So if there's a handoff to an agent named `Refund Agent`, the tool would be called `transfer_to_refund_agent` by default.

*   **Creating a handoff:**
    *   All agents have a `handoffs` param, which can either take an `Agent` directly, or a `Handoff` object that customizes the Handoff.
    *   You can create a handoff using the `handoff()` function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters.

*   **Basic Usage:** Here's how you can create a simple handoff:
    ```python
    from agents import Agent, handoff

    # Assume these agents are defined elsewhere
    billing_agent = Agent(name="Billing agent", instructions="...")
    refund_agent = Agent(name="Refund agent", instructions="...")

    # Using Agent directly in the list
    triage_agent_direct = Agent(
        name="Triage agent",
        handoffs=[billing_agent, refund_agent],
        model="o3-mini" # Specify model
    )

    # Using the handoff() function (functionally same as above in this simple case)
    triage_agent_func = Agent(
        name="Triage agent",
        handoffs=[billing_agent, handoff(refund_agent)], # refund_agent wrapped
        model="o3-mini" # Specify model
    )
    ```

*   **Customizing handoffs via the `handoff()` function:**
    *   The `handoff()` function lets you customize things.
        *   `agent`: This is the agent to which things will be handed off.
        *   `tool_name_override`: By default, the `Handoff.default_tool_name()` function is used, which resolves to `transfer_to_<agent_name>`. You can override this.
        *   `tool_description_override`: Override the default tool description from `Handoff.default_tool_description()`.
        *   `on_handoff`: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context (`RunContextWrapper[TContext]`), and can optionally also receive LLM generated input. The input data is controlled by the `input_type` param.
        *   `input_type`: The type of input expected by the handoff (optional, typically a Pydantic model).
        *   `input_filter`: This lets you filter the input received by the next agent. See [Input filters](#input-filters-1) below for more.
    ```python
    from agents import Agent, handoff, RunContextWrapper
    from pydantic import BaseModel # Added import
    import asyncio # Added import

    # Example callback without input data
    def on_handoff_callback_simple(ctx: RunContextWrapper[None]): # Context type depends on agent
        print(f"[Callback] Handoff invoked. Context type: {type(ctx.context)}")

    # Example callback requiring input data
    class HandoffInputDataExample(BaseModel):
        reason: str
        priority: int

    async def on_handoff_callback_with_input(ctx: RunContextWrapper[None], input_data: HandoffInputDataExample):
        print(f"[Callback] Handoff invoked with data: {input_data}. Context type: {type(ctx.context)}")

    # Define the target agent
    target_agent = Agent(name="Target Agent", instructions="Handle delegated tasks.")

    # Create customized handoff objects
    handoff_obj_custom_name = handoff(
        agent=target_agent,
        on_handoff=on_handoff_callback_simple,
        tool_name_override="delegate_to_target",
        tool_description_override="Pass the task to the Target Agent.",
    )

    handoff_obj_with_input = handoff(
        agent=target_agent,
        on_handoff=on_handoff_callback_with_input,
        input_type=HandoffInputDataExample, # Specify the input type
        tool_description_override="Delegate task with reason and priority." # Example override
    )

    # Example agent using these customized handoffs
    # delegator_agent = Agent(
    #     name="Delegator Agent",
    #     instructions="Delegate tasks using custom handoff tools.",
    #     handoffs=[handoff_obj_custom_name, handoff_obj_with_input],
    #     model="o3-mini"
    # )
    ```

*   **Handoff inputs:**
    *   In certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an "Escalation agent". You might want a reason to be provided, so you can log it. Use the `input_type` parameter with a Pydantic model.
    ```python
    from pydantic import BaseModel
    from agents import Agent, handoff, RunContextWrapper
    import asyncio # Added import

    class EscalationData(BaseModel):
        reason: str
        urgency: int = 1 # Example with default

    # Callback function that receives the input data
    async def on_escalation_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
        print(f"[Callback] Escalation agent called. Reason: '{input_data.reason}', Urgency: {input_data.urgency}")
        # Add logging or other actions here

    # Define the escalation agent
    escalation_agent_input = Agent(name="Escalation agent", instructions="Handle escalated issues.")

    # Create the handoff object specifying the input type and callback
    escalation_handoff_obj = handoff(
        agent=escalation_agent_input,
        on_handoff=on_escalation_handoff,
        input_type=EscalationData, # Link the Pydantic model here
        tool_description_override="Escalate the issue with reason and urgency." # Example description
    )

    # Example agent that can use this handoff
    # support_agent_escalate = Agent(
    #     name="Support Agent",
    #     instructions="Handle support requests. Escalate complex issues using the tool.",
    #     handoffs=[escalation_handoff_obj],
    #     model="o3-mini"
    # )
    ```

*   **Input filters:**
    *   When a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history.
    *   If you want to change this, you can set an `input_filter`. An input filter is a function that receives the existing input via a `HandoffInputData` object, and must return a new `HandoffInputData` object.
    *   There are some common patterns (for example removing all tool calls from the history), which are implemented for you in `agents.extensions.handoff_filters`.
    ```python
    from agents import Agent, handoff, HandoffInputData # Added HandoffInputData
    # Assuming the extensions module and filter exist:
    try:
        from agents.extensions import handoff_filters
    except ImportError:
        print("Warning: agents.extensions.handoff_filters not found. Skipping filter example.")
        handoff_filters = None # Define as None if not found

    # Define the target agent
    faq_agent_filtered = Agent(name="FAQ agent", instructions="Answer frequently asked questions.")

    # Create the handoff object with an input filter (if available)
    if handoff_filters:
        handoff_obj_filtered = handoff(
            agent=faq_agent_filtered,
            input_filter=handoff_filters.remove_all_tools,
            tool_description_override="Handoff to FAQ agent (history filtered)."
        )
        print("Created handoff with remove_all_tools filter.")

        # Example custom filter function
        def custom_filter(data: HandoffInputData) -> HandoffInputData:
            print("[Input Filter] Running custom filter...")
            # Example: Keep only the last user message and the handoff items
            last_user_message = None
            for item in reversed(data.input_history): # Assuming input_history is iterable
                 if item.get("role") == "user":
                      last_user_message = item
                      break
            # Combine last user message (if found) with new handoff items
            new_history = [last_user_message] if last_user_message else []
            # Note: This simplistic filter might lose crucial context.
            # Return a new HandoffInputData object
            return HandoffInputData(
                 input_history=tuple(new_history), # Must be tuple if modifying
                 pre_handoff_items=data.pre_handoff_items, # Keep these
                 new_items=data.new_items # Keep these
            )

        handoff_custom_filter = handoff(
            agent=faq_agent_filtered,
            input_filter=custom_filter,
            tool_description_override="Handoff to FAQ agent (custom filter)."
        )
        print("Created handoff with custom filter.")

    else:
        # Fallback if filters module is missing
        handoff_obj_filtered = handoff(agent=faq_agent_filtered)
        print("Created handoff without input filter (module not found).")

    # Example agent using filtered handoff
    # triage_agent_filtered = Agent(
    #     name="Triage Agent Filtered",
    #     instructions="Route to FAQ agent, filtering history.",
    #     handoffs=[handoff_obj_filtered],
    #     model="o3-mini"
    # )
    ```

*   **Recommended prompts:**
    *   To make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents.
    *   We have a suggested prefix in `agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`, or you can call `agents.extensions.handoff_prompt.prompt_with_handoff_instructions()` to automatically add recommended data to your prompts.
    ```python
    from agents import Agent
    # Assuming the extensions module and prompt helpers exist:
    try:
        from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX, prompt_with_handoff_instructions
        prompt_helpers_available = True
    except ImportError:
        print("Warning: agents.extensions.handoff_prompt not found. Skipping recommended prompt examples.")
        RECOMMENDED_PROMPT_PREFIX = "# Add handoff instructions here manually if needed.\n"
        prompt_helpers_available = False

    # Define a target agent for handoff examples
    target_agent_for_prompt = Agent(name="TargetAgentForPrompt", instructions="...")

    # Method 1: Manually add the prefix
    agent_with_prefix = Agent(
        name="Agent With Prefix",
        instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    You are a helpful agent.
    If the user asks about X, handoff to TargetAgentForPrompt.
    <Fill in the rest of your prompt here>.""",
        handoffs=[target_agent_for_prompt],
        model="o3-mini"
    )
    print("Created agent with manual prompt prefix.")

    # Method 2: Use the helper function (if available)
    if prompt_helpers_available:
        agent_with_helper = Agent(
            name="Agent With Helper",
            instructions=prompt_with_handoff_instructions(
                """You are a helpful agent.
    If the user asks about X, handoff to TargetAgentForPrompt.
    <Fill in the rest of your prompt here>."""
            ),
            handoffs=[target_agent_for_prompt],
            model="o3-mini"
        )
        print("Created agent with prompt helper function.")
    else:
        print("Skipping agent creation with prompt helper function (module not found).")

    ```

### Guardrails

Guardrails run in parallel to your agents, enabling you to do checks and validations of user input. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money.

There are two kinds of guardrails:

1.  **Input guardrails:** Run on the initial user input.
2.  **Output guardrails:** Run on the final agent output.

*   **Input guardrails:**
    *   Run in 3 steps:
        1.  First, the guardrail receives the same input passed to the agent.
        2.  Next, the guardrail function runs to produce a `GuardrailFunctionOutput`, which is then wrapped in an `InputGuardrailResult`.
        3.  Finally, we check if `.tripwire_triggered` is true. If true, an `InputGuardrailTripwireTriggered` exception is raised, so you can appropriately respond to the user or handle the exception.
    *   **Note:** Input guardrails are intended to run on user input, so an agent's guardrails only run if the agent is the *first* agent. You might wonder, why is the guardrails property on the agent instead of passed to `Runner.run`? It's because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.

*   **Output guardrails:**
    *   Run in 3 steps:
        1.  First, the guardrail receives the same final output produced by the agent. (Correction: receives *input* and *agent output*).
        2.  Next, the guardrail function runs to produce a `GuardrailFunctionOutput`, which is then wrapped in an `OutputGuardrailResult`.
        3.  Finally, we check if `.tripwire_triggered` is true. If true, an `OutputGuardrailTripwireTriggered` exception is raised, so you can appropriately respond to the user or handle the exception.
    *   **Note:** Output guardrails are intended to run on the final agent output, so an agent's guardrails only run if the agent is the *last* agent. Similar to the input guardrails, we do this because guardrails tend to be related to the actual Agent - you'd run different guardrails for different agents, so colocating the code is useful for readability.

*   **Tripwires:**
    *   If the input or output fails the guardrail, the Guardrail can signal this with a tripwire.
    *   As soon as we see a guardrail that has triggered the tripwires, we immediately raise a `{Input,Output}GuardrailTripwireTriggered` exception and halt the Agent execution.

*   **Implementing a guardrail:**
    *   You need to provide a function that receives input, and returns a `GuardrailFunctionOutput`. In this example, we'll do this by running an Agent under the hood.

    **Input Guardrail Implementation Example:**
    ```python
    from pydantic import BaseModel
    from agents import (
        Agent,
        GuardrailFunctionOutput,
        InputGuardrailTripwireTriggered,
        RunContextWrapper,
        Runner,
        TResponseInputItem,
        input_guardrail, # Decorator
        InputGuardrail # Class for agent list
    )
    import asyncio

    # Pydantic model for the guardrail agent's output
    class MathHomeworkOutputGuard(BaseModel): # Renamed class
        is_math_homework: bool
        reasoning: str

    # The agent that performs the guardrail check
    guardrail_agent_impl = Agent(
        name="Guardrail check",
        instructions="Check if the user is asking you to do their math homework.",
        output_type=MathHomeworkOutputGuard,
        model="o3-mini" # Specify a model
    )

    # The guardrail function itself, decorated
    @input_guardrail # Use the decorator
    async def math_guardrail_impl( # Renamed function
        ctx: RunContextWrapper[None], # Assuming None context for simplicity
        agent: Agent, # The agent the guardrail is attached to
        input_data: str | list[TResponseInputItem] # The input being checked
    ) -> GuardrailFunctionOutput:
        print(f"[Guardrail] Checking input: {input_data}")
        # Run the guardrail agent (requires API key)
        result = await Runner.run(guardrail_agent_impl, input_data, context=ctx.context)
        final_output = result.final_output_as(MathHomeworkOutputGuard)

        print(f"[Guardrail] Check result: {final_output.is_math_homework}, Reasoning: {final_output.reasoning}")
        # Define the output and trigger condition
        return GuardrailFunctionOutput(
            output_info=final_output, # The structured output from the guardrail agent
            tripwire_triggered=final_output.is_math_homework, # Halt if it IS math homework
        )

    # The main agent using the input guardrail
    agent_with_input_guard = Agent(
        name="Customer support agent",
        instructions="You are a customer support agent. You help customers with their questions.",
        input_guardrails=[math_guardrail_impl], # Attach the guardrail function here
        model="o3-mini" # Specify model
    )

    # Example execution demonstrating the guardrail
    async def main_input_guardrail_impl():
        print("--- Testing Input Guardrail Implementation ---")
        # This should trip the guardrail
        try:
            print("Testing math question (should be blocked)...")
            await Runner.run(agent_with_input_guard, "Hello, can you help me solve for x: 2x + 3 = 11?")
            print("Guardrail didn't trip - UNEXPECTED")
        except InputGuardrailTripwireTriggered as e:
            print(f"InputGuardrailTripwireTriggered as expected: Guardrail tripped.")
            # Access guardrail result if needed: print(e.guardrail_result)
        except Exception as e:
            print(f"An unexpected error occurred: {e}")

        # This should pass
        try:
            print("\nTesting non-math question (should pass)...")
            result_pass = await Runner.run(agent_with_input_guard, "What is your refund policy?")
            print(f"Guardrail passed. Agent output: {result_pass.final_output}")
        except InputGuardrailTripwireTriggered:
            print("Guardrail tripped unexpectedly for non-math question.")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")

    # if __name__ == "__main__":
    #     # Ensure OPENAI_API_KEY is set
    #     # asyncio.run(main_input_guardrail_impl())
    ```

    **Output Guardrail Implementation Example:** Output guardrails are similar.
    ```python
    from pydantic import BaseModel
    from agents import (
        Agent,
        GuardrailFunctionOutput,
        OutputGuardrailTripwireTriggered, # Specific exception
        RunContextWrapper,
        Runner,
        output_guardrail, # Decorator
        OutputGuardrail # Class for agent list
    )
    import asyncio

    # Pydantic model for the main agent's output
    class MainAgentOutput(BaseModel):
        response: str

    # Pydantic model for the guardrail agent's output
    class MathOutputCheckGuard(BaseModel): # Renamed class
        reasoning: str
        is_math: bool

    # The agent performing the output check
    guardrail_agent_output_impl = Agent( # Renamed
        name="Guardrail check for Math Output",
        instructions="Check if the provided text includes any math.",
        output_type=MathOutputCheckGuard,
        model="o3-mini" # Specify model
    )

    # The output guardrail function, decorated
    @output_guardrail # Use the decorator
    async def math_output_guardrail_impl( # Renamed function
        ctx: RunContextWrapper, # Context type depends on main agent
        agent: Agent, # The agent whose output is being checked
        output: MainAgentOutput # The actual output from the main agent
    ) -> GuardrailFunctionOutput:
        print(f"[Guardrail] Checking output: '{output.response}'")
        # Run the guardrail agent on the main agent's response text (requires API key)
        result = await Runner.run(guardrail_agent_output_impl, output.response, context=ctx.context)
        final_output = result.final_output_as(MathOutputCheckGuard)

        print(f"[Guardrail] Check result: {final_output.is_math}, Reasoning: {final_output.reasoning}")
        # Define the output and trigger condition
        return GuardrailFunctionOutput(
            output_info=final_output, # Output from the guardrail agent
            tripwire_triggered=final_output.is_math, # Halt if the main agent's output IS math
        )

    # The main agent using the output guardrail
    agent_with_output_guard = Agent(
        name="Customer support agent (Output Guarded)",
        instructions="You are a customer support agent. You help customers with their questions. Avoid complex math.",
        output_guardrails=[math_output_guardrail_impl], # Attach the output guardrail
        output_type=MainAgentOutput, # Agent must have defined output type matching guardrail input
        model="o3-mini" # Specify model
    )

    # Example execution
    async def main_output_guardrail_impl():
        print("--- Testing Output Guardrail Implementation ---")
        # This agent might generate math, triggering the guardrail
        try:
            print("Testing math question (output might be blocked)...")
            # Ask the agent a question that might lead to a math response
            await Runner.run(agent_with_output_guard, "What is 5 times 8?")
            print("Output guardrail didn't trip - check agent response if this is unexpected.")
        except OutputGuardrailTripwireTriggered as e:
            print(f"OutputGuardrailTripwireTriggered as expected: Guardrail tripped on agent output.")
            # Access guardrail result if needed: print(e.guardrail_result)
        except Exception as e:
            print(f"An unexpected error occurred: {e}")

        # This should produce non-math output and pass
        try:
            print("\nTesting non-math question (should pass output guardrail)...")
            result_pass = await Runner.run(agent_with_output_guard, "What's your company address?")
            print(f"Output guardrail passed. Agent output: {result_pass.final_output}")
        except OutputGuardrailTripwireTriggered:
            print("Output guardrail tripped unexpectedly.")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")

    # if __name__ == "__main__":
    #     # Ensure OPENAI_API_KEY is set
    #     # asyncio.run(main_output_guardrail_impl())
    ```

### Models

The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:

*   **Recommended:** The `OpenAIResponsesModel`, which calls OpenAI APIs using the new Responses API.
*   The `OpenAIChatCompletionsModel`, which calls OpenAI APIs using the Chat Completions API.

*   **Mixing and matching models:**
    *   Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks.
    *   When configuring an Agent, you can select a specific model by either:
        1.  Passing the name of an OpenAI model (e.g., `"gpt-4o-mini"`).
        2.  Passing any model name + a `ModelProvider` that can map that name to a `Model` instance.
        3.  Directly providing a `Model` implementation.
    *   **Note:** While our SDK supports both the `OpenAIResponsesModel` and the `OpenAIChatCompletionsModel` shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.
    ```python
    from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, ModelSettings # Added ModelSettings
    import asyncio

    # Agent using default (Responses API likely, depending on provider)
    spanish_agent_mixed = Agent(
        name="Spanish agent",
        instructions="You only speak Spanish.",
        model="o3-mini", # Uses Responses API via default OpenAIProvider
    )

    # Agent explicitly using Chat Completions API
    english_agent_mixed = Agent(
        name="English agent",
        instructions="You only speak English",
        model=OpenAIChatCompletionsModel(
            model="gpt-4o", # Specify model for Chat Completions
            openai_client=AsyncOpenAI() # Assumes key is in env
        ),
        model_settings=ModelSettings(temperature=0.1) # Example settings
    )

    # Triage agent using another model (Responses API via default provider)
    triage_agent_mixed_models = Agent(
        name="Triage agent",
        instructions="Handoff to the appropriate agent based on the language of the request.",
        handoffs=[spanish_agent_mixed, english_agent_mixed],
        model="gpt-3.5-turbo", # Model name string
    )

    async def main_mixed():
        # Ensure API key is set
        print("--- Testing Mixed Model Workflow ---")
        # Test Spanish handoff (should use o3-mini via Responses)
        result_es = await Runner.run(triage_agent_mixed_models, input="Hola, cmo ests?")
        print(f"Spanish agent output: {result_es.final_output}")

        # Test English handoff (should use gpt-4o via Chat Completions)
        result_en = await Runner.run(triage_agent_mixed_models, input="Hello, how are you?")
        print(f"English agent output: {result_en.final_output}")

    # if __name__ == "__main__":
    #     asyncio.run(main_mixed())
    ```
    *   When you want to further configure the model used for an agent, you can pass `ModelSettings`, which provides optional model configuration parameters such as temperature.
    ```python
    from agents import Agent, ModelSettings

    english_agent_settings = Agent(
        name="English agent",
        instructions="You only speak English",
        model="gpt-4o", # Specify model name
        model_settings=ModelSettings(temperature=0.1, top_p=0.9), # Pass ModelSettings instance
    )
    ```

*   **Using other LLM providers:**
    *   You can use other LLM providers in 3 ways (examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers)):
        1.  `set_default_openai_client`: Useful in cases where you want to globally use an instance of `AsyncOpenAI` as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the `base_url` and `api_key`. See a configurable example in `examples/model_providers/custom_example_global.py`.
        2.  `ModelProvider`: At the `Runner.run` level (`run_config`). This lets you say "use a custom model provider for all agents in this run". See a configurable example in `examples/model_providers/custom_example_provider.py`.
        3.  `Agent.model`: Lets you specify the model on a specific `Agent` instance. This enables you to mix and match different providers for different agents. See a configurable example in `examples/model_providers/custom_example_agent.py`.
    *   In cases where you do not have an API key from `platform.openai.com`, we recommend disabling tracing via `set_tracing_disabled()`, or setting up a different tracing processor.
    *   **Note:** In these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.

*   **Common issues with using other LLM providers:**
    *   **Tracing client error 401:** If you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:
        1.  Disable tracing entirely: `set_tracing_disabled(True)`.
        2.  Set an OpenAI key for tracing: `set_tracing_export_api_key(...)`. This API key will only be used for uploading traces, and must be from `platform.openai.com`.
        3.  Use a non-OpenAI trace processor. See the [tracing docs](#tracing-1).
    *   **Responses API support:** The SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:
        1.  Call `set_default_openai_api("chat_completions")`. This works if you are setting `OPENAI_API_KEY` and `OPENAI_BASE_URL` via environment vars.
        2.  Use `OpenAIChatCompletionsModel`. There are examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers).
    *   **Structured outputs support:** Some model providers don't have support for structured outputs. This sometimes results in an error that looks something like this: `BadRequestError: Error code: 400 - {'error': {'message': "'response_format.type' : value is not one of the allowed values ['text','json_object']", 'type': 'invalid_request_error'}}`
        *   This is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the `json_schema` to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.

### Context Management

Context is an overloaded term. There are two main classes of context you might care about:

1.  **Context available locally to your code:** This is data and dependencies you might need when tool functions run, during callbacks like `on_handoff`, in lifecycle hooks, etc.
2.  **Context available to LLMs:** This is data the LLM sees when generating a response.

*   **Local context:**
    *   This is represented via the `RunContextWrapper` class and the `context` property within it. The way this works is:
        1.  You create any Python object you want. A common pattern is to use a `dataclass` or a Pydantic object.
        2.  You pass that object to the various run methods (e.g. `Runner.run(..., context=whatever)`).
        3.  All your tool calls, lifecycle hooks etc will be passed a wrapper object, `RunContextWrapper[T]`, where `T` represents your context object type which you can access via `wrapper.context`.
    *   The most important thing to be aware of: **every agent, tool function, lifecycle etc for a given agent run must use the same type of context.**
    *   You can use the context for things like:
        *   Contextual data for your run (e.g. things like a username/uid or other information about the user)
        *   Dependencies (e.g. logger objects, data fetchers, etc)
        *   Helper functions
    *   **Note:** The context object is **not** sent to the LLM. It is purely a local object that you can read from, write to and call methods on it.
    ```python
    import asyncio
    from dataclasses import dataclass
    from agents import Agent, RunContextWrapper, Runner, function_tool

    # Define the context data structure
    @dataclass
    class UserInfoContext: # Renamed for clarity
        name: str
        uid: int

    # Define a tool that accesses the context
    @function_tool
    async def fetch_user_age_context(wrapper: RunContextWrapper[UserInfoContext]) -> str:
        """Fetches the user's age based on context."""
        print(f"[Tool] Accessing context for user: {wrapper.context.name} (UID: {wrapper.context.uid})")
        # In a real app, you might look up the age based on uid
        age = 47 # Example
        return f"User {wrapper.context.name} is {age} years old"

    async def main_local_context():
        # Create an instance of the context data
        user_info_instance = UserInfoContext(name="John", uid=123)

        # Define the agent, specifying the context type annotation
        agent_local_context = Agent[UserInfoContext](
            name="Assistant",
            instructions="Use tools to find user information.",
            tools=[fetch_user_age_context],
            model="o3-mini" # Specify model
        )

        # Run the agent, passing the context instance
        # Ensure API key is set
        print("--- Testing Local Context Usage ---")
        result = await Runner.run(
            starting_agent=agent_local_context,
            input="What is the age of the user?",
            context=user_info_instance, # Pass the context object here
        )

        print(f"Final Output: {result.final_output}")
        # Expected Output: The user John is 47 years old.

    # if __name__ == "__main__":
    #     asyncio.run(main_local_context())
    ```

*   **Agent/LLM context:**
    *   When an LLM is called, the only data it can see is from the conversation history.
    *   This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:
        1.  You can add it to the **Agent instructions**. This is also known as a "system prompt" or "developer message". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).
        2.  Add it to the **input when calling the `Runner.run` functions**. This is similar to the instructions tactic, but allows you to have messages that are lower in the chain of command.
        3.  Expose it via **function tools**. This is useful for on-demand context - the LLM decides when it needs some data, and can call the tool to fetch that data.
        4.  Use **retrieval or web search**. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for "grounding" the response in relevant contextual data.

### Orchestration

Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:

1.  **Allowing the LLM to make decisions:** This uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.
2.  **Orchestrating via code:** Determining the flow of agents via your code.

You can mix and match these patterns. Each has their own tradeoffs, described below.

*   **Orchestrating via LLM:**
    *   An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents.
    *   For example, a research agent could be equipped with tools like: Web search, File search/retrieval, Computer use, Code execution, Handoffs to specialized agents (planning, report writing).
    *   This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM.
    *   The most important tactics here are:
        1.  Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.
        2.  Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.
        3.  Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.
        4.  Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.
        5.  Invest in evals. This lets you train your agents to improve and get better at tasks.

*   **Orchestrating via code:**
    *   While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance.
    *   Common patterns here are:
        *   Using structured outputs to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.
        *   Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.
        *   Running the agent that performs the task in a `while` loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.
        *   Running multiple agents in parallel, e.g. via Python primitives like `asyncio.gather`. This is useful for speed when you have multiple tasks that don't depend on each other.
    *   We have a number of examples in `examples/agent_patterns`.

*   **Common agentic patterns:** (This folder contains examples of different common patterns for agents.)
    *   **Deterministic flows:** A common tactic is to break down a task into a series of smaller steps. Each task can be performed by an agent, and the output of one agent is used as input to the next. For example, if your task was to generate a story, you could break it down into: Generate an outline -> Generate the story -> Generate the ending. See `deterministic.py`.
    *   **Handoffs and routing:** In many situations, you have specialized sub-agents that handle specific tasks. You can use handoffs to route the task to the right agent. For example, a frontline agent receives a request, then hands off based on language. See `routing.py`.
    *   **Agents as tools:** The mental model for handoffs is that the new agent "takes over". You can also use agents as a tool - the tool agent runs on its own and returns the result to the original agent. Enables things like translating multiple languages at once. See `agents_as_tools.py`.
    *   **LLM-as-a-judge:** LLMs can often improve output quality with feedback. Generate a response with one model, use a second (potentially larger) model to provide feedback. Repeat until satisfied. See `llm_as_a_judge.py`.
    *   **Parallelization:** Running multiple agents in parallel is common for latency (independent tasks) or other reasons (generating multiple responses and picking the best). See `parallelization.py`.
    *   **Guardrails:** Related to parallelization, run input guardrails to validate inputs. Guardrails can have a "tripwire" - if triggered, execution stops immediately (useful for latency with fast guardrail/slow main agent). See `input_guardrails.py` and `output_guardrails.py`.

### Tracing

The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during development and in production.

*   **Note:** Tracing is enabled by default. There are two ways to disable tracing:
    1.  You can globally disable tracing by setting the env var `OPENAI_AGENTS_DISABLE_TRACING=1`.
    2.  You can disable tracing for a single run by setting `agents.run.RunConfig.tracing_disabled` to `True`.
*   **Note:** For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable.

*   **Traces and spans:**
    *   **Traces:** Represent a single end-to-end operation of a "workflow". They're composed of Spans. Traces have the following properties:
        *   `workflow_name`: This is the logical workflow or app. For example "Code generation" or "Customer service".
        *   `trace_id`: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format `trace_<32_alphanumeric>`.
        *   `group_id`: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID.
        *   `disabled`: If `True`, the trace will not be recorded.
        *   `metadata`: Optional metadata for the trace.
    *   **Spans:** Represent operations that have a start and end time. Spans have:
        *   `started_at` and `ended_at` timestamps.
        *   `trace_id`, to represent the trace they belong to.
        *   `parent_id`, which points to the parent Span of this Span (if any).
        *   `span_data`, which is information about the Span. For example, `AgentSpanData` contains information about the Agent, `GenerationSpanData` contains information about the LLM generation, etc.

*   **Default tracing:**
    *   By default, the SDK traces the following:
        *   The entire `Runner.{run, run_sync, run_streamed}()` is wrapped in a `trace()`.
        *   Each time an agent runs, it is wrapped in `agent_span()`.
        *   LLM generations are wrapped in `generation_span()`.
        *   Function tool calls are each wrapped in `function_span()`.
        *   Guardrails are wrapped in `guardrail_span()`.
        *   Handoffs are wrapped in `handoff_span()`.
        *   Audio inputs (speech-to-text) are wrapped in a `transcription_span()`.
        *   Audio outputs (text-to-speech) are wrapped in a `speech_span()`.
        *   Related audio spans may be parented under a `speech_group_span()`.
    *   By default, the trace is named "Agent trace". You can set this name if you use `trace`, or you can can configure the name and other properties with the `RunConfig`.
    *   In addition, you can set up custom trace processors to push traces to other destinations (as a replacement, or secondary destination).

*   **Higher level traces:**
    *   Sometimes, you might want multiple calls to `run()` to be part of a single trace. You can do this by wrapping the entire code in a `trace()`.
    ```python
    from agents import Agent, Runner, trace
    import asyncio

    async def main_higher_level_trace():
        agent = Agent(name="Joke generator", instructions="Tell funny jokes.", model="o3-mini")

        print("--- Testing Higher Level Trace ---")
        # Ensure API key is set
        # Wrap multiple run calls in a single trace context
        with trace("Joke Workflow"): # Name the overall workflow
            print("Running first joke generation...")
            first_result = await Runner.run(agent, "Tell me a joke")
            print(f"Joke: {first_result.final_output}")

            print("\nRunning joke rating...")
            # The second run will be part of the same "Joke Workflow" trace
            second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
            print(f"Rating: {second_result.final_output}")

    # if __name__ == "__main__":
    #     asyncio.run(main_higher_level_trace())
    ```

*   **Creating traces:**
    *   You can use the `trace()` function to create a trace. Traces need to be started and finished.
    *   You have two options to do so:
        1.  **Recommended:** Use the trace as a context manager, i.e. `with trace(...) as my_trace:`. This will automatically start and end the trace at the right time.
        2.  You can also manually call `trace.start()` and `trace.finish()`.
    *   The current trace is tracked via a Python `contextvar`. This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass `mark_as_current` and `reset_current` to `start()`/`finish()` to update the current trace.

*   **Creating spans:**
    *   You can use the various `*_span()` methods to create a span. In general, you don't need to manually create spans.
    *   A `custom_span()` function is available for tracking custom span information.
    *   Spans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python `contextvar`.

*   **Sensitive data:**
    *   Certain spans may capture potentially sensitive data.
    *   The `generation_span()` stores the inputs/outputs of the LLM generation, and `function_span()` stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via `RunConfig.trace_include_sensitive_data`.
    *   Similarly, Audio spans include base64-encoded PCM data for input and output audio by default. You can disable capturing this audio data by configuring `VoicePipelineConfig.trace_include_sensitive_audio_data`.

*   **Custom tracing processors:**
    *   The high level architecture for tracing is:
        *   At initialization, we create a global `TraceProvider`, which is responsible for creating traces.
        *   We configure the `TraceProvider` with a `BatchTraceProcessor` that sends traces/spans in batches to a `BackendSpanExporter`, which exports the spans and traces to the OpenAI backend in batches.
    *   To customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options:
        1.  `add_trace_processor()`: Lets you add an additional trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend.
        2.  `set_trace_processors()`: Lets you replace the default processors with your own trace processors. This means traces will *not* be sent to the OpenAI backend unless you include a `TracingProcessor` that does so.

*   **External tracing processors list:**
    *   Weights & Biases
    *   Arize-Phoenix
    *   MLflow (self-hosted/OSS)
    *   MLflow (Databricks hosted)
    *   Braintrust
    *   Pydantic Logfire
    *   AgentOps
    *   Scorecard
    *   Keywords AI
    *   LangSmith
    *   Maxim AI
    *   Comet Opik
    *   Langfuse
    *   Langtrace
    *   Okahu-Monocle

### Configuration

Customize SDK behavior globally.

*   **API keys and clients:**
    *   By default, the SDK looks for the `OPENAI_API_KEY` environment variable for LLM requests and tracing, as soon as it is imported.
    *   If you are unable to set that environment variable before your app starts, you can use the `set_default_openai_key()` function to set the key.
    ```python
    from agents import set_default_openai_key
    # Set the key programmatically if needed before other imports/usage
    # set_default_openai_key("sk-...")
    ```
    *   Alternatively, you can also configure an OpenAI client to be used. By default, the SDK creates an `AsyncOpenAI` instance, using the API key from the environment variable or the default key set above. You can change this by using the `set_default_openai_client()` function.
    ```python
    from openai import AsyncOpenAI
    from agents import set_default_openai_client

    # Example: Configure client with custom base URL
    # custom_client = AsyncOpenAI(base_url="https://your-openai-compatible-endpoint/v1", api_key="your-key")
    # set_default_openai_client(custom_client)
    ```
    *   Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the `set_default_openai_api()` function.
    ```python
    from agents import set_default_openai_api

    # Switch to using Chat Completions API globally
    # set_default_openai_api("chat_completions")
    ```

*   **Tracing:**
    *   Tracing is enabled by default. It uses the OpenAI API keys from the section above by default (i.e. the environment variable or the default key you set).
    *   You can specifically set the API key used for tracing by using the `set_tracing_export_api_key` function.
    ```python
    from agents import set_tracing_export_api_key

    # Use a specific key ONLY for uploading traces
    # set_tracing_export_api_key("sk-abc...")
    ```
    *   You can also disable tracing entirely by using the `set_tracing_disabled()` function.
    ```python
    from agents import set_tracing_disabled

    # Disable tracing globally
    # set_tracing_disabled(True)
    ```

*   **Debug logging:**
    *   The SDK has two Python loggers without any handlers set (`openai.agents`, `openai.agents.tracing`). By default, this means that warnings and errors are sent to stdout, but other logs are suppressed.
    *   To enable verbose logging, use the `enable_verbose_stdout_logging()` function.
    ```python
    from agents import enable_verbose_stdout_logging

    # Enable verbose logging to stdout (DEBUG level)
    # enable_verbose_stdout_logging()
    ```
    *   Alternatively, you can customize the logs by adding handlers, filters, formatters, etc. You can read more in the Python logging guide.
    ```python
    import logging

    # Get the main SDK logger
    logger_main = logging.getLogger("openai.agents")
    # Get the tracing logger
    logger_tracing = logging.getLogger("openai.agents.tracing")

    # Example: Set level for main logger
    logger_main.setLevel(logging.DEBUG) # Show all logs (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    # logger_main.setLevel(logging.INFO) # Show INFO and above
    # logger_main.setLevel(logging.WARNING) # Show WARNING and above (default-like behavior)

    # Example: Add a handler to output logs to console (stderr by default)
    console_handler = logging.StreamHandler()
    # Optional: Add a formatter for nicer output
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)

    # Add the handler to the logger(s) you want to configure
    logger_main.addHandler(console_handler)
    # logger_tracing.addHandler(console_handler) # Add handler to tracing logger too if needed
    # logger_tracing.setLevel(logging.DEBUG) # Set level for tracing logger if needed

    # Now logs at or above the configured level will appear via the handler
    # Example: logger_main.debug("This is a debug message.")
    ```

*   **Sensitive data in logs:**
    *   Certain logs may contain sensitive data (for example, user data). If you want to disable this data from being logged, set the following environment variables.
    *   To disable logging LLM inputs and outputs:
        ```bash
        export OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1
        ```
    *   To disable logging tool inputs and outputs:
        ```bash
        export OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1
        ```

### Agent Visualization

Agent visualization allows you to generate a structured graphical representation of agents and their relationships using Graphviz. This is useful for understanding how agents, tools, and handoffs interact within an application.

*   **Installation:** Install the optional `viz` dependency group:
    ```bash
    pip install "openai-agents[viz]"
    ```

*   **Generating a Graph:**
    *   You can generate an agent visualization using the `draw_graph` function from `agents.extensions.visualization`.
    *   This function creates a directed graph where:
        *   Agents are represented as yellow boxes.
        *   Tools are represented as green ellipses.
        *   Handoffs are directed edges from one agent to another.
*   **Example Usage:**
    ```python
    # Ensure 'openai-agents[viz]' and graphviz binaries are installed
    from agents import Agent, function_tool
    try:
        from agents.extensions.visualization import draw_graph
        viz_available = True
    except ImportError:
        print("Visualization extension not installed ('pip install openai-agents[viz]'). Skipping visualization.")
        viz_available = False

    # Define tool and agents (reuse from previous examples or define new)
    @function_tool
    def get_weather_viz(city: str) -> str:
        """Gets weather"""
        return f"The weather in {city} is sunny."

    spanish_agent_viz = Agent(name="Spanish agent", instructions="Speak Spanish.")
    english_agent_viz = Agent(name="English agent", instructions="Speak English.")

    triage_agent_viz = Agent(
        name="Triage agent",
        instructions="Handoff based on language.",
        handoffs=[spanish_agent_viz, english_agent_viz],
        tools=[get_weather_viz],
        model="o3-mini" # Specify model
    )

    if viz_available:
        print("--- Generating Agent Graph ---")
        # This generates a graph object. In environments like Jupyter, it might display automatically.
        graph = draw_graph(triage_agent_viz)
        print("Graph object created. To view or save:")
        print("  graph.view() # Opens in default viewer")
        print("  graph.render('agent_graph', format='png', view=False) # Saves as agent_graph.png")
        # Example saving:
        # try:
        #     graph.render('agent_graph', format='png', view=False)
        #     print("Graph saved as agent_graph.png")
        # except Exception as e:
        #     print(f"Could not save graph (ensure Graphviz is installed and in PATH): {e}")
    ```
    *   This generates a graph that visually represents the structure of the triage agent and its connections to sub-agents and tools.

*   **Understanding the Visualization:** The generated graph includes:
    *   A start node (`__start__`) indicating the entry point.
    *   Agents represented as rectangles with yellow fill.
    *   Tools represented as ellipses with green fill.
    *   Directed edges indicating interactions:
        *   Solid arrows for agent-to-agent handoffs.
        *   Dotted arrows for tool invocations.
    *   An end node (`__end__`) indicating where execution terminates.

*   **Customizing the Graph:**
    *   **Showing the Graph:** By default, `draw_graph` displays the graph inline (in supported environments like Jupyter). To show the graph in a separate window, write the following:
        ```python
        # graph = draw_graph(triage_agent_viz) # Assuming graph is generated
        # graph.view() # Requires Graphviz installation and viewer
        ```
    *   **Saving the Graph:** By default, `draw_graph` displays the graph inline. To save it as a file, specify a filename (implicitly via `render` or possibly a future `draw_graph` arg):
        ```python
        # graph = draw_graph(triage_agent_viz) # Assuming graph is generated
        # graph.render('agent_graph', format='png', view=False) # Saves file
        # Or potentially: draw_graph(triage_agent_viz, filename="agent_graph.png") # If API changes
        ```
        This will generate `agent_graph.png` in the working directory (using `render`).

### Voice Agents

Extend agents with voice capabilities using Text-to-Speech (TTS) and Speech-to-Text (STT).

*   **Prerequisites:**
    *   Follow the base quickstart for the Agents SDK.
    *   Install optional voice dependencies:
        ```bash
        pip install 'openai-agents[voice]'
        ```
*   **Concepts:**
    *   **`VoicePipeline`:** The main class orchestrating the voice interaction. It follows a 3-step process:
        1.  Run speech-to-text (STT) model on audio input.
        2.  Run your agentic workflow code (typically `Runner.run_streamed`).
        3.  Run text-to-speech (TTS) model on the workflow's text result.
    *   **Visual Flow:**
        ```
         Audio Input --> [Transcribe (STT)] --> [Your Agent Code] --> [Text-to-Speech (TTS)] -->  Audio Output
        ```

*   **Quickstart Example Steps:**
    1.  **Setup Agents:** Define agents and tools as usual. The example uses a weather tool and a handoff to a Spanish agent. Use `prompt_with_handoff_instructions` for better handoff behavior in voice contexts.
        ```python
        import asyncio
        import random
        from agents import ( Agent, function_tool )
        # Assuming extension is available
        try:
            from agents.extensions.handoff_prompt import prompt_with_handoff_instructions
        except ImportError:
            print("Warning: Handoff prompt extension not found.")
            def prompt_with_handoff_instructions(p): return p # Dummy function

        @function_tool
        def get_weather_voice(city: str) -> str:
            """Get the weather for a given city."""
            print(f"[debug] get_weather called with city: {city}")
            choices = ["sunny", "cloudy", "rainy", "snowy"]
            return f"The weather in {city} is {random.choice(choices)}."

        spanish_agent_voice = Agent(
            name="Spanish",
            handoff_description="A spanish speaking agent.",
            instructions=prompt_with_handoff_instructions(
                "You're speaking to a human, so be polite and concise. Speak in Spanish.",
            ),
            model="gpt-4o-mini", # Ensure appropriate model
        )

        agent_voice_main = Agent(
            name="Assistant",
            instructions=prompt_with_handoff_instructions(
                "You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.",
            ),
            model="gpt-4o-mini", # Ensure appropriate model
            handoffs=[spanish_agent_voice],
            tools=[get_weather_voice],
        )
        ```
    2.  **Setup Voice Pipeline:** Create a `VoicePipeline` instance, passing a workflow (e.g., `SingleAgentVoiceWorkflow`).
        ```python
        # Ensure voice dependencies are installed: pip install 'openai-agents[voice]'
        try:
            from agents.voice import SingleAgentVoiceWorkflow, VoicePipeline
            voice_deps_installed = True
        except ImportError:
            print("Voice dependencies not installed ('pip install openai-agents[voice]'). Skipping voice pipeline setup.")
            voice_deps_installed = False
            # Dummy classes if not installed
            class VoicePipeline: pass
            class SingleAgentVoiceWorkflow: pass

        if voice_deps_installed:
            pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent_voice_main))
            print("VoicePipeline created.")
        ```
    3.  **Run the Pipeline:** Provide audio input (`AudioInput` or `StreamedAudioInput`). The example uses a dummy silent buffer.
        ```python
        # Requires numpy and sounddevice: pip install numpy sounddevice
        import numpy as np
        import sounddevice as sd
        try:
            from agents.voice import AudioInput
            audio_input_available = True
        except ImportError:
             print("Voice dependencies not installed. Cannot create AudioInput.")
             audio_input_available = False
             class AudioInput: pass # Dummy

        async def run_voice_pipeline_example():
            if not voice_deps_installed or not audio_input_available:
                print("Cannot run voice pipeline example due to missing dependencies.")
                return

            print("--- Running Voice Pipeline (with silent input) ---")
            # For simplicity, create 3 seconds of silence (requires numpy)
            # In reality, you'd get microphone data
            sample_rate = 24000 # Default expected by SDK
            duration_sec = 3
            buffer = np.zeros(sample_rate * duration_sec, dtype=np.int16)
            audio_input_instance = AudioInput(buffer=buffer, frame_rate=sample_rate)
            print("Created silent audio input.")

            # Run the pipeline (requires API key)
            result = await pipeline.run(audio_input_instance)
            print("Pipeline run initiated. Streaming audio output...")

            # Create an audio player using `sounddevice`
            try:
                player = sd.OutputStream(samplerate=sample_rate, channels=1, dtype=np.int16)
                player.start()
                print("Audio output stream started.")

                # Play the audio stream as it comes in
                async for event in result.stream():
                    if event.type == "voice_stream_event_audio":
                        if event.data is not None:
                            # Ensure data is in the correct format (numpy array)
                            audio_chunk = np.frombuffer(event.data, dtype=np.int16) if isinstance(event.data, bytes) else event.data
                            if audio_chunk.size > 0:
                                player.write(audio_chunk)
                                print(".", end="", flush=True) # Indicate audio chunk received
                    elif event.type == "voice_stream_event_lifecycle":
                         print(f"\n[Lifecycle Event: {event.event}]")
                    elif event.type == "voice_stream_event_error":
                         print(f"\n[Error Event: {event.error}]")

                print("\nAudio stream finished.")
                player.stop()
                player.close()
                print("Audio output stream closed.")

            except sd.PortAudioError as e:
                print(f"\nError with sounddevice (audio output): {e}")
                print("Ensure you have a working audio output device and PortAudio installed.")
            except Exception as e:
                print(f"\nAn error occurred during audio playback: {e}")

        # if __name__ == "__main__":
        #     # Ensure API key is set, numpy, sounddevice installed
        #     # asyncio.run(run_voice_pipeline_example())
        ```
    4.  **Process Output Stream:** Iterate through `result.stream()` to get `VoiceStreamEvent`s and play `VoiceStreamEventAudio` data.

*   **Full Example Code:** (See the "Put it all together" block in the original source, which combines steps 1-4 into a single script). Note that running this requires `numpy`, `sounddevice`, and potentially microphone setup or using the dummy input as shown.

*   **Pipelines and Workflows:**
    *   `VoicePipeline`: Makes it easy to turn agentic workflows into voice apps. Takes care of STT, VAD, workflow execution, TTS.
    *   **Configuring a pipeline:** When creating `VoicePipeline`, you can set:
        1.  `workflow`: The `VoiceWorkflowBase` instance to run.
        2.  `stt_model`, `tts_model`: Specify STT/TTS models (string names or model instances). Defaults to OpenAI models.
        3.  `config`: A `VoicePipelineConfig` instance to configure:
            *   `model_provider`: Maps model names to models (default `OpenAIVoiceModelProvider`).
            *   Tracing (`tracing_disabled`, `trace_include_sensitive_data`, `trace_include_sensitive_audio_data`, `workflow_name`, `group_id`, `trace_metadata`).
            *   STT/TTS settings (`stt_settings`, `tts_settings` - e.g., prompt, language, data types).
    *   **Running a pipeline:** Call `pipeline.run(audio_input)`.
        *   `AudioInput`: For a complete audio buffer (e.g., pre-recorded, push-to-talk).
        *   `StreamedAudioInput`: For streaming audio chunks, allowing automatic activity detection.
    *   **Results (`StreamedAudioResult`):** Returned by `pipeline.run()`. Call `.stream()` to get an async iterator of `VoiceStreamEvent`s:
        *   `VoiceStreamEventAudio`: Contains an audio chunk (`.data`).
        *   `VoiceStreamEventLifecycle`: Informs about lifecycle events like `turn_started`, `turn_ended`.
        *   `VoiceStreamEventError`: Contains an error (`.error`).
        ```python
        # Example processing loop (inside async function)
        # result = await pipeline.run(input)
        # async for event in result.stream():
        #     if event.type == "voice_stream_event_audio":
        #         # play audio data (event.data)
        #         pass
        #     elif event.type == "voice_stream_event_lifecycle":
        #         # handle lifecycle event (event.event)
        #         pass
        #     elif event.type == "voice_stream_event_error":
        #         # handle error (event.error)
        #         pass
        ```

*   **Best Practices:**
    *   **Interruptions:** The Agents SDK currently does not support any built-in interruptions support for `StreamedAudioInput`. Instead for every detected turn it will trigger a separate run of your workflow. If you want to handle interruptions inside your application you can listen to the `VoiceStreamEventLifecycle` events. `turn_started` will indicate that a new turn was transcribed and processing is beginning. `turn_ended` will trigger after all the audio was dispatched for a respective turn. You could use these events to mute the microphone of the speaker when the model starts a turn and unmute it after you flushed all the related audio for a turn.
    *   **Tracing:** Voice pipelines are automatically traced, similar to agents. Configure via `VoicePipelineConfig`. Key fields:
        *   `tracing_disabled`: Controls if tracing is disabled (default `False`).
        *   `trace_include_sensitive_data`: Controls if traces include sensitive text like transcripts (default `True`).
        *   `trace_include_sensitive_audio_data`: Controls if traces include audio data bytes (default `True`).
        *   `workflow_name`: Name for the trace workflow (default `'Voice Agent'`).
        *   `group_id`: Links multiple traces (defaults to a random ID).
        *   `trace_metadata`: Additional custom metadata.

## API Reference (Detailed)

*(This section mirrors the API reference structure from the original text, providing detailed parameter and class descriptions.)*

### `agents` module (Top-Level Configuration)

*   **`set_default_openai_key(key: str, use_for_tracing: bool = True) -> None`**
    *   Sets the default OpenAI API key for LLM requests (and optionally tracing).
    *   Overrides `OPENAI_API_KEY` env var if provided.
    *   **Parameters:**
        *   `key` (str, required): The OpenAI key.
        *   `use_for_tracing` (bool, default=True): Whether to also use this key for sending traces. If False, tracing key must be set via env var or `set_tracing_export_api_key`.
*   **`set_default_openai_client(client: AsyncOpenAI, use_for_tracing: bool = True) -> None`**
    *   Sets the default `AsyncOpenAI` client instance.
    *   Overrides the default client created from the API key.
    *   **Parameters:**
        *   `client` (AsyncOpenAI, required): The client instance.
        *   `use_for_tracing` (bool, default=True): Whether to use the API key from this client for tracing uploads.
*   **`set_default_openai_api(api: Literal["chat_completions", "responses"]) -> None`**
    *   Sets the default API to use (default is `"responses"`).
*   **`enable_verbose_stdout_logging() -> None`**
    *   Enables verbose logging (DEBUG level) to stdout for `openai.agents` and `openai.agents.tracing`.

### `agents.tracing` module (Tracing Configuration)

*   **`set_tracing_export_api_key(api_key: str) -> None`**
    *   Sets the OpenAI API key specifically for the backend trace exporter.
*   **`set_tracing_disabled(disabled: bool) -> None`**
    *   Globally enables or disables tracing.
*   **`set_trace_processors(processors: list[TracingProcessor]) -> None`**
    *   Replaces the current list of trace processors with the provided list.
*   **`add_trace_processor(span_processor: TracingProcessor) -> None`**
    *   Adds an additional trace processor to the existing list.

### Agents (`agents.agent` module)

*   **`ToolsToFinalOutputFunction: TypeAlias = Callable[[RunContextWrapper[TContext], list[FunctionToolResult]], MaybeAwaitable[ToolsToFinalOutputResult]]`**
    *   Type hint for functions deciding if tool results are final output.
*   **`ToolsToFinalOutputResult`** (dataclass)
    *   `is_final_output` (bool): Whether this is the final output.
    *   `final_output` (Any | None): The final output value (if `is_final_output` is True).
*   **`StopAtTools`** (TypedDict)
    *   `stop_at_tool_names` (list[str]): List of tool names that halt execution.
*   **`MCPConfig`** (TypedDict)
    *   `convert_schemas_to_strict` (NotRequired[bool], default=False): Attempt to convert MCP schemas to strict mode.
*   **`Agent[TContext]`** (dataclass, Generic)
    *   Core agent class.
    *   **Attributes:**
        *   `name` (str): Agent name.
        *   `instructions` (str | Callable | None): System prompt (string or function).
        *   `handoff_description` (str | None): Description used in handoffs/tools.
        *   `handoffs` (list[Agent[Any] | Handoff[TContext]], default=[]): List of agents/handoff objects for delegation.
        *   `model` (str | Model | None): Model name or instance (defaults to `openai_provider.DEFAULT_MODEL`).
        *   `model_settings` (ModelSettings, default=ModelSettings()): Model tuning parameters.
        *   `tools` (list[Tool], default=[]): List of tools agent can use.
        *   `mcp_servers` (list[MCPServer], default=[]): MCP servers for tools (manage lifecycle externally).
        *   `mcp_config` (MCPConfig, default=MCPConfig()): MCP server config.
        *   `input_guardrails` (list[InputGuardrail[TContext]], default=[]): Guardrails for initial input.
        *   `output_guardrails` (list[OutputGuardrail[TContext]], default=[]): Guardrails for final output.
        *   `output_type` (type[Any] | None): Expected output type (default `str`).
        *   `hooks` (AgentHooks[TContext] | None): Agent-specific lifecycle callbacks.
        *   `tool_use_behavior` (Literal["run_llm_again", "stop_on_first_tool"] | StopAtTools | ToolsToFinalOutputFunction, default="run_llm_again"): How tool use is handled.
        *   `reset_tool_choice` (bool, default=True): Reset `tool_choice` to auto after tool call.
    *   **Methods:**
        *   `clone(**kwargs: Any) -> Agent[TContext]`: Creates a copy with modifications.
        *   `as_tool(tool_name: str | None, tool_description: str | None, custom_output_extractor: Callable[[RunResult], Awaitable[str]] | None = None) -> Tool`: Transforms agent into a callable tool.
        *   `get_system_prompt(run_context: RunContextWrapper[TContext]) -> MaybeAwaitable[str | None]`: Gets the system prompt (handles dynamic instructions).
        *   `get_mcp_tools() -> Awaitable[list[Tool]]`: Fetches tools from MCP servers.
        *   `get_all_tools() -> Awaitable[list[Tool]]`: Gets all tools (function + MCP).

### Runner (`agents.run` module)

*   **`Runner`** (class)
    *   Provides methods to run agent workflows.
    *   **Class Methods:**
        *   `run(starting_agent, input, *, context=None, max_turns=DEFAULT_MAX_TURNS, hooks=None, run_config=None, previous_response_id=None) -> RunResult`: Async run.
        *   `run_sync(...) -> RunResult`: Sync run (wraps `run`).
        *   `run_streamed(...) -> RunResultStreaming`: Async run with streaming events.
*   **`RunConfig`** (dataclass)
    *   Global settings for a run.
    *   **Attributes:**
        *   `model` (str | Model | None): Global model override.
        *   `model_provider` (ModelProvider, default=OpenAIProvider): Provider for resolving model names.
        *   `model_settings` (ModelSettings | None): Global model settings override.
        *   `handoff_input_filter` (HandoffInputFilter | None): Global filter for handoff inputs.
        *   `input_guardrails` (list[InputGuardrail[Any]] | None): Global input guardrails.
        *   `output_guardrails` (list[OutputGuardrail[Any]] | None): Global output guardrails.
        *   `tracing_disabled` (bool, default=False): Disable tracing for this run.
        *   `trace_include_sensitive_data` (bool, default=True): Include sensitive data in traces.
        *   `workflow_name` (str, default='Agent workflow'): Trace workflow name.
        *   `trace_id` (str | None): Custom trace ID.
        *   `group_id` (str | None): Trace group ID.
        *   `trace_metadata` (dict[str, Any] | None): Metadata for trace.

### Tools (`agents.tool` module)

*   **`Tool = Union[FunctionTool, FileSearchTool, WebSearchTool, ComputerTool]`**: Type alias for tools.
*   **`FunctionToolResult`** (dataclass)
    *   `tool` (FunctionTool): The tool that ran.
    *   `output` (Any): Tool output.
    *   `run_item` (RunItem): The corresponding run item.
*   **`FunctionTool`** (dataclass)
    *   Represents a function tool. Use `@function_tool` helper usually.
    *   **Attributes:**
        *   `name` (str): Tool name.
        *   `description` (str): Tool description.
        *   `params_json_schema` (dict[str, Any]): JSON schema for parameters.
        *   `on_invoke_tool` (Callable[[RunContextWrapper[Any], str], Awaitable[Any]]): Async function to invoke the tool. Receives context and JSON args string. Must return stringable output.
        *   `strict_json_schema` (bool, default=True): Whether schema is strict mode.
*   **`FileSearchTool`** (dataclass)
    *   Hosted tool for vector store search (requires Responses API).
    *   **Attributes:** `vector_store_ids` (list[str]), `max_num_results` (int | None), `include_search_results` (bool), `ranking_options` (RankingOptions | None), `filters` (Filters | None).
*   **`WebSearchTool`** (dataclass)
    *   Hosted tool for web search (requires Responses API).
    *   **Attributes:** `user_location` (UserLocation | None), `search_context_size` (Literal["low", "medium", "high"], default="medium").
*   **`ComputerTool`** (dataclass)
    *   Hosted tool for computer control.
    *   **Attributes:** `computer` (Computer | AsyncComputer): Implementation of computer actions.
*   **`default_tool_error_function(ctx: RunContextWrapper[Any], error: Exception) -> str`**
    *   Default error handler function for `@function_tool`.
*   **`function_tool(...)`** (decorator)
    *   Creates `FunctionTool` from a Python function.
    *   Parses signature and docstrings.
    *   **Parameters:** `func`, `name_override`, `description_override`, `docstring_style`, `use_docstring_info`, `failure_error_function`, `strict_mode`.

### Results (`agents.result` module)

*   **`RunResultBase`** (dataclass, ABC)
    *   Base class for run results.
    *   **Attributes:** `input`, `new_items`, `raw_responses`, `final_output`, `input_guardrail_results`, `output_guardrail_results`.
    *   **Properties:** `last_agent` (abstract), `last_response_id`.
    *   **Methods:** `final_output_as(cls: type[T], raise_if_incorrect_type: bool = False) -> T`, `to_input_list() -> list[TResponseInputItem]`.
*   **`RunResult(RunResultBase)`** (dataclass)
    *   Result from `Runner.run` / `run_sync`. Implements `last_agent`.
*   **`RunResultStreaming(RunResultBase)`** (dataclass)
    *   Result from `Runner.run_streamed`.
    *   **Additional Attributes:** `current_agent`, `current_turn`, `max_turns`, `is_complete` (bool, default=False).
    *   **Methods:** `stream_events() -> AsyncIterator[StreamEvent]`.

### Streaming Events (`agents.stream_events` module)

*   **`StreamEvent: TypeAlias = Union[RawResponsesStreamEvent, RunItemStreamEvent, AgentUpdatedStreamEvent]`**
*   **`RawResponsesStreamEvent`** (dataclass)
    *   `type`: `'raw_response_event'`
    *   `data` (TResponseStreamEvent): Raw event from LLM.
*   **`RunItemStreamEvent`** (dataclass)
    *   `type`: `'run_item_stream_event'`
    *   `name` (Literal): Type of item created (e.g., "message_output_created", "tool_called").
    *   `item` (RunItem): The fully generated item.
*   **`AgentUpdatedStreamEvent`** (dataclass)
    *   `type`: `'agent_updated_stream_event'`
    *   `new_agent` (Agent[Any]): The agent now running (due to handoff).

### Handoffs (`agents.handoffs` module)

*   **`HandoffInputFilter: TypeAlias = Callable[[HandoffInputData], HandoffInputData]`**
*   **`HandoffInputData`** (dataclass)
    *   `input_history` (str | tuple[TResponseInputItem, ...]): History before the run started.
    *   `pre_handoff_items` (tuple[RunItem, ...]): Items generated before the handoff turn.
    *   `new_items` (tuple[RunItem, ...]): Items generated during the handoff turn.
*   **`Handoff[TContext]`** (dataclass, Generic)
    *   Represents a configurable handoff.
    *   **Attributes:**
        *   `tool_name` (str): Name of the handoff tool.
        *   `tool_description` (str): Description of the handoff tool.
        *   `input_json_schema` (dict[str, Any]): Schema for handoff input data (if any).
        *   `on_invoke_handoff` (Callable[[RunContextWrapper[Any], str], Awaitable[Agent[TContext]]]): Function called when handoff invoked. Receives context, args JSON string. Must return target agent.
        *   `agent_name` (str): Name of the target agent.
        *   `input_filter` (HandoffInputFilter | None): Function to filter history for target agent.
        *   `strict_json_schema` (bool, default=True): Whether `input_json_schema` is strict.
*   **`handoff(...)`** (function)
    *   Factory to create `Handoff` objects.
    *   Overloads for different `on_handoff` signatures (with/without input) and `input_type`.
    *   **Parameters:** `agent`, `tool_name_override`, `tool_description_override`, `on_handoff`, `input_type`, `input_filter`.

### Lifecycle (`agents.lifecycle` module)

*   **`RunHooks[TContext]`** (class, Generic)
    *   Base class for run-level lifecycle callbacks. Subclass and override methods.
    *   **Methods:** `on_agent_start`, `on_agent_end`, `on_handoff`, `on_tool_start`, `on_tool_end`.
*   **`AgentHooks[TContext]`** (class, Generic)
    *   Base class for agent-specific lifecycle callbacks. Set via `Agent.hooks`.
    *   **Methods:** `on_start`, `on_end`, `on_handoff` (called when handed *to*), `on_tool_start`, `on_tool_end`.

### Items (`agents.items` module)

*   **Type Aliases:** `TResponse`, `TResponseInputItem`, `TResponseOutputItem`, `TResponseStreamEvent`, `ToolCallItemTypes`.
*   **`RunItem: TypeAlias = Union[...]`**: Union of all specific RunItem types.
*   **`RunItemBase[T]`** (dataclass, ABC, Generic)
    *   Base for run items.
    *   **Attributes:** `agent` (Agent[Any]), `raw_item` (T).
    *   **Methods:** `to_input_item() -> TResponseInputItem`.
*   **Specific Item Classes:**
    *   `MessageOutputItem(RunItemBase[ResponseOutputMessage])`
    *   `HandoffCallItem(RunItemBase[ResponseFunctionToolCall])`
    *   `HandoffOutputItem(RunItemBase[TResponseInputItem])`: Includes `source_agent`, `target_agent`.
    *   `ToolCallItem(RunItemBase[ToolCallItemTypes])`
    *   `ToolCallOutputItem(RunItemBase[Union[FunctionCallOutput, ComputerCallOutput]])`: Includes `output` (parsed tool result).
    *   `ReasoningItem(RunItemBase[ResponseReasoningItem])`
*   **`ModelResponse`** (dataclass)
    *   Wraps a single LLM response.
    *   **Attributes:** `output` (list[TResponseOutputItem]), `usage` (Usage), `response_id` (str | None).
    *   **Methods:** `to_input_items() -> list[TResponseInputItem]`.
*   **`ItemHelpers`** (class)
    *   Static helper methods: `extract_last_content`, `extract_last_text`, `input_to_new_input_list`, `text_message_outputs`, `text_message_output`, `tool_call_output_item`.

### Run Context (`agents.run_context`, `agents.usage` modules)

*   **`RunContextWrapper[TContext]`** (dataclass, Generic)
    *   Wraps the user-provided context object. Passed to tools, hooks, etc.
    *   **Attributes:** `context` (TContext), `usage` (Usage).
*   **`Usage`** (dataclass)
    *   Tracks token usage for a run.
    *   **Attributes:** `requests` (int), `input_tokens` (int), `output_tokens` (int), `total_tokens` (int).

### Exceptions (`agents.exceptions` module)

*   **`AgentsException(Exception)`**: Base SDK exception.
*   **`MaxTurnsExceeded(AgentsException)`**: Raised when `max_turns` exceeded.
*   **`ModelBehaviorError(AgentsException)`**: Raised for invalid model output.
*   **`UserError(AgentsException)`**: Raised for incorrect SDK usage.
*   **`InputGuardrailTripwireTriggered(AgentsException)`**: Raised by input guardrail tripwire. Includes `guardrail_result` (InputGuardrailResult).
*   **`OutputGuardrailTripwireTriggered(AgentsException)`**: Raised by output guardrail tripwire. Includes `guardrail_result` (OutputGuardrailResult).

### Guardrails (`agents.guardrail` module)

*   **`GuardrailFunctionOutput`** (dataclass)
    *   Output of a guardrail function.
    *   **Attributes:** `output_info` (Any), `tripwire_triggered` (bool).
*   **`InputGuardrailResult`** (dataclass)
    *   Result of an input guardrail run.
    *   **Attributes:** `guardrail` (InputGuardrail[Any]), `output` (GuardrailFunctionOutput).
*   **`OutputGuardrailResult`** (dataclass)
    *   Result of an output guardrail run.
    *   **Attributes:** `guardrail` (OutputGuardrail[Any]), `agent_output` (Any), `agent` (Agent[Any]), `output` (GuardrailFunctionOutput).
*   **`InputGuardrail[TContext]`** (dataclass, Generic)
    *   Configuration for an input guardrail.
    *   **Attributes:** `guardrail_function` (Callable), `name` (str | None).
*   **`OutputGuardrail[TContext]`** (dataclass, Generic)
    *   Configuration for an output guardrail.
    *   **Attributes:** `guardrail_function` (Callable), `name` (str | None).
*   **`input_guardrail(...)`** (decorator)
    *   Transforms a function into an `InputGuardrail`. Supports sync/async, optional `name`.
*   **`output_guardrail(...)`** (decorator)
    *   Transforms a function into an `OutputGuardrail`. Supports sync/async, optional `name`.

### Model Settings (`agents.model_settings` module)

*   **`ModelSettings`** (dataclass)
    *   Holds optional model configuration parameters.
    *   **Attributes:** `temperature`, `top_p`, `frequency_penalty`, `presence_penalty`, `tool_choice`, `parallel_tool_calls`, `truncation`, `max_tokens`, `reasoning`, `metadata`, `store`, `include_usage`, `extra_query`, `extra_body`.
    *   **Methods:** `resolve(override: ModelSettings | None) -> ModelSettings`.

### Agent Output (`agents.agent_output` module)

*   **`AgentOutputSchema`** (class)
    *   Captures JSON schema for agent output and validates/parses LLM JSON.
    *   **`__init__(output_type: type[Any], strict_json_schema: bool = True)`**
    *   **Attributes:** `output_type`, `strict_json_schema`.
    *   **Methods:** `is_plain_text()`, `json_schema()`, `validate_json(json_str: str, partial: bool = False) -> Any`, `output_type_name()`.

### Function Schema (`agents.function_schema` module)

*   **`FuncSchema`** (dataclass)
    *   Captures schema for a Python function tool.
    *   **Attributes:** `name`, `description`, `params_pydantic_model`, `params_json_schema`, `signature`, `takes_context`, `strict_json_schema`.
    *   **Methods:** `to_call_args(data: BaseModel) -> tuple[list[Any], dict[str, Any]]`.
*   **`FuncDocumentation`** (dataclass)
    *   Metadata extracted from function docstring.
    *   **Attributes:** `name`, `description`, `param_descriptions`.
*   **`generate_func_documentation(func: Callable[..., Any], style: DocstringStyle | None = None) -> FuncDocumentation`**
*   **`function_schema(...) -> FuncSchema`**: Generates `FuncSchema` from a function.

### Model Interface (`agents.models.interface` module)

*   **`ModelTracing`** (Enum): `DISABLED`, `ENABLED`, `ENABLED_WITHOUT_DATA`.
*   **`Model`** (ABC)
    *   Base interface for LLM models.
    *   **Abstract Methods:** `get_response(...) -> ModelResponse`, `stream_response(...) -> AsyncIterator[TResponseStreamEvent]`.
*   **`ModelProvider`** (ABC)
    *   Base interface for model providers.
    *   **Abstract Methods:** `get_model(model_name: str | None) -> Model`.

### OpenAI Models (`agents.models.openai_*` modules)

*   **`OpenAIChatCompletionsModel(Model)`**: Implements `Model` using Chat Completions API.
*   **`OpenAIResponsesModel(Model)`**: Implements `Model` using Responses API.

### MCP Servers (`agents.mcp.server`, `agents.mcp.util` modules)

*   **`MCPServer`** (ABC)
    *   Base class for MCP servers.
    *   **Abstract Properties:** `name`.
    *   **Abstract Methods:** `connect()`, `cleanup()`, `list_tools()`, `call_tool()`.
*   **`MCPServerStdioParams`** (TypedDict): Config for stdio server (`command`, `args`, `env`, `cwd`, `encoding`, etc.).
*   **`MCPServerStdio(MCPServer)`**: Stdio implementation. Includes `invalidate_tools_cache()`.
*   **`MCPServerSseParams`** (TypedDict): Config for SSE server (`url`, `headers`, `timeout`, etc.).
*   **`MCPServerSse(MCPServer)`**: SSE implementation. Includes `invalidate_tools_cache()`.
*   **`MCPUtil`**: Static class with helper methods (`get_all_function_tools`, `get_function_tools`, `to_function_tool`, `invoke_mcp_tool`).

### Tracing (`agents.tracing.*` modules - Detailed)

*   **Processor Interface (`processor_interface.py`)**
    *   `TracingProcessor` (ABC): Interface for processing spans (`on_trace_start`, `on_trace_end`, `on_span_start`, `on_span_end`, `shutdown`, `force_flush`).
    *   `TracingExporter` (ABC): Interface for exporting traces/spans (`export`).
*   **Span Data (`span_data.py`)**
    *   `SpanData` (ABC): Base class for span data (`type`, `export`).
    *   Concrete Classes: `AgentSpanData`, `CustomSpanData`, `FunctionSpanData`, `GenerationSpanData`, `GuardrailSpanData`, `HandoffSpanData`, `MCPListToolsSpanData`, `ResponseSpanData`, `SpeechGroupSpanData`, `SpeechSpanData`, `TranscriptionSpanData`.
*   **Spans (`spans.py`)**
    *   `Span[TSpanData]` (ABC, Generic): Base class for spans (`start`, `finish`).
    *   Implementations: `NoOpSpan`, `SpanImpl`.
*   **Traces (`traces.py`)**
    *   `Trace` (ABC): Base class for traces (`trace_id`, `name`, `start`, `finish`, `export`).
    *   Implementations: `NoOpTrace`, `TraceImpl`.
*   **Creation (`create.py`)**
    *   Functions to create specific spans: `agent_span`, `custom_span`, `function_span`, `generation_span`, `guardrail_span`, `handoff_span`, `mcp_tools_span`, `response_span`, `speech_group_span`, `speech_span`, `transcription_span`.
    *   `trace`: Creates a `Trace` object (use as context manager or manually).
    *   `get_current_span`, `get_current_trace`: Retrieve current context-aware span/trace.
*   **Processors (`processors.py`)**
    *   `ConsoleSpanExporter(TracingExporter)`: Prints traces/spans to console.
    *   `BackendSpanExporter(TracingExporter)`: Exports to OpenAI backend. `__init__` takes API key, org, project, endpoint, retry settings. `set_api_key`, `close` methods.
    *   `BatchTraceProcessor(TracingProcessor)`: Batches spans for export. `__init__` takes exporter, queue size, batch size, delay settings. `shutdown`, `force_flush` methods.
    *   `default_exporter()`, `default_processor()`: Get default instances.
*   **Scope (`scope.py`)**
    *   `Scope`: Manages current span/trace via `contextvars`.
*   **Setup (`setup.py`)**
    *   `SynchronousMultiTracingProcessor(TracingProcessor)`: Forwards calls to multiple processors. `add_tracing_processor`, `set_processors` methods.
    *   `TraceProvider`: Manages trace creation and processors. `register_processor`, `set_processors`, `get_current_trace`, `get_current_span`, `set_disabled`, `create_trace`, `create_span` methods.
*   **Util (`util.py`)**
    *   `time_iso()`, `gen_trace_id()`, `gen_span_id()`, `gen_group_id()`.

### Voice (`agents.voice.*` modules - Detailed)

*   **Pipeline (`pipeline.py`)**
    *   `VoicePipeline`: Orchestrates STT->Workflow->TTS.
        *   `__init__(*, workflow, stt_model=None, tts_model=None, config=None)`
        *   `run(audio_input: AudioInput | StreamedAudioInput) -> StreamedAudioResult` (async)
*   **Workflow (`workflow.py`)**
    *   `VoiceWorkflowBase` (ABC): Base for custom voice workflows.
        *   `run(transcription: str) -> AsyncIterator[str]` (abstractmethod)
    *   `VoiceWorkflowHelper`: Helper class.
        *   `stream_text_from(result: RunResultStreaming) -> AsyncIterator[str]` (classmethod, async)
    *   `SingleAgentWorkflowCallbacks`: Callback interface for `SingleAgentVoiceWorkflow`. `on_run`.
    *   `SingleAgentVoiceWorkflow(VoiceWorkflowBase)`: Simple workflow running one agent. `__init__(agent, callbacks=None)`.
*   **Input (`input.py`)**
    *   `AudioInput` (dataclass): Static audio buffer. Attributes: `buffer` (NDArray), `frame_rate`, `sample_width`, `channels`. Methods: `to_audio_file`, `to_base64`.
    *   `StreamedAudioInput`: Streamable audio input. Method: `add_audio(audio: NDArray)` (async).
*   **Result (`result.py`)**
    *   `StreamedAudioResult`: Output of `pipeline.run`. `__init__`, `stream() -> AsyncIterator[VoiceStreamEvent]` (async).
*   **Pipeline Config (`pipeline_config.py`)**
    *   `VoicePipelineConfig` (dataclass): Config for `VoicePipeline`. Attributes: `model_provider`, `tracing_disabled`, `trace_include_sensitive_data`, `trace_include_sensitive_audio_data`, `workflow_name`, `group_id`, `trace_metadata`, `stt_settings`, `tts_settings`.
*   **Events (`events.py`)**
    *   `VoiceStreamEvent`: TypeAlias for voice events.
    *   `VoiceStreamEventAudio` (dataclass): Contains audio chunk (`data`). `type='voice_stream_event_audio'`.
    *   `VoiceStreamEventLifecycle` (dataclass): Contains lifecycle event name (`event`: "turn_started", "turn_ended", "session_ended"). `type='voice_stream_event_lifecycle'`.
    *   `VoiceStreamEventError` (dataclass): Contains error (`error`). `type='voice_stream_event_error'`.
*   **Exceptions (`exceptions.py`)**
    *   `STTWebsocketConnectionError(AgentsException)`.
*   **Model (`model.py`)**
    *   `TTSModelSettings` (dataclass): Config for TTS (`voice`, `buffer_size`, `dtype`, `transform_data`, `instructions`, `text_splitter`, `speed`).
    *   `TTSModel` (ABC): Base for TTS models (`model_name` property, `run(text, settings)` abstractmethod).
    *   `StreamedTranscriptionSession` (ABC): Base for streaming STT (`transcribe_turns()`, `close()` abstractmethods).
    *   `STTModelSettings` (dataclass): Config for STT (`prompt`, `language`, `temperature`, `turn_detection`).
    *   `STTModel` (ABC): Base for STT models (`model_name` property, `transcribe(input, settings, ...)` abstractmethod, `create_session(input, settings, ...)` abstractmethod).
    *   `VoiceModelProvider` (ABC): Base for voice model providers (`get_stt_model`, `get_tts_model` abstractmethods).
*   **Utils (`utils.py`)**
    *   `get_sentence_based_splitter(min_sentence_length: int = 20) -> Callable[[str], tuple[str, str]]`.
*   **OpenAI Models (`models/openai_*`)**
    *   `OpenAIVoiceModelProvider(VoiceModelProvider)`: Implements provider for OpenAI STT/TTS.
    *   `OpenAISTTModel(STTModel)`: OpenAI STT implementation.
    *   `OpenAITTSModel(TTSModel)`: OpenAI TTS implementation.

### Extensions (`agents.extensions.*` modules - Detailed)

*   **Handoff Filters (`handoff_filters.py`)**
    *   `remove_all_tools(handoff_input_data: HandoffInputData) -> HandoffInputData`: Filters out tool call/output items.
*   **Handoff Prompt (`handoff_prompt.py`)**
    *   `RECOMMENDED_PROMPT_PREFIX` (str): Recommended text to add to prompts using handoffs.
    *   `prompt_with_handoff_instructions(prompt: str) -> str`: Helper function to prepend prefix to a prompt.
*   **Visualization (`visualization.py`)**
    *   `draw_graph(agent: Agent, filename: str | None = None)`: Generates Graphviz graph object. (Note: Original text implies it might display directly, but standard Graphviz library usually returns an object to be rendered/viewed).

---

## Related OpenAI Concepts and Guides (Preserved)

*(This section includes the broader context guides provided in the original text, maintaining their content as presented.)*

### General Agent Concepts

*   **Overview:** Agents represent systems that intelligently accomplish tasks, ranging from executing simple workflows to pursuing complex, open-ended objectives. OpenAI provides a rich set of composable primitives that enable you to build agents. This guide walks through those primitives, and how they come together to form a robust agentic platform. Building agents involves assembling components across several domainssuch as models, tools, knowledge and memory, audio and speech, guardrails, and orchestrationand OpenAI provides composable primitives for each.
    *   **Domain** | **Description** | **OpenAI Primitives**
        ---|---|---
        Models | Core intelligence capable of reasoning, making decisions, and processing different modalities. | o1, o3-mini, GPT-4.5, GPT-4o, GPT-4o-mini
        Tools | Interface to the world, interact with environment, function calling, built-in tools, etc. | Function calling, Web search, File search, Computer use
        Knowledge and memory | Augment agents with external and persistent knowledge. | Vector stores, File search, Embeddings
        Audio and speech | Create agents that can understand audio and respond back in natural language. | Audio generation, realtime, Audio agents
        Guardrails | Prevent irrelevant, harmful, or undesirable behavior. | Moderation, Instruction hierarchy
        Orchestration | Develop, deploy, monitor, and improve agents. | Agents SDK, Tracing, Evaluations, Fine-tuning
        Voice agents | Create agents that can understand audio and respond back in natural language. | Realtime API, Voice support in the Agents SDK
*   **Models for Agents:**
    *   **Model** | **Agentic Strengths**
        ---|---
        o1 and o3-mini | Best for long-term planning, hard tasks, and reasoning.
        GPT-4.5 | Best for agentic execution.
        GPT-4o | Good balance of agentic capability and latency.
        GPT-4o-mini | Best for low-latency.
    *   Large language models (LLMs) are at the core of many agentic systems, responsible for making decisions and interacting with the world. OpenAIs models support a wide range of capabilities: High intelligence, Tools, Multimodality, Low-latency. For detailed model comparisons, visit the [models page](https://platform.openai.com/docs/models).
*   **Tools:** Tools enable agents to interact with the world. OpenAI supports function calling to connect with your code, and built-in tools for common tasks like web searches and data retrieval.
    *   **Tool** | **Description**
        ---|---
        Function calling | Interact with developer-defined code.
        Web search | Fetch up-to-date information from the web.
        File search | Perform semantic search across your documents.
        Computer use | Understand and control a computer or browser.
*   **Knowledge and Memory:** Knowledge and memory help agents store, retrieve, and utilize information beyond their initial training data. Vector stores enable agents to search your documents semantically and retrieve relevant information at runtime. Meanwhile, embeddings represent data efficiently for quick retrieval, powering dynamic knowledge solutions and long-term agent memory. You can integrate your data using OpenAIs vector stores and Embeddings API.
*   **Guardrails:** Guardrails ensure your agents behave safely, consistently, and within your intended boundariescritical for production deployments. Use OpenAIs free Moderation API to automatically filter unsafe content. Further control your agents behavior by leveraging the instruction hierarchy, which prioritizes developer-defined prompts and mitigates unwanted agent behaviors.
*   **Orchestration:** Building agents is a process. OpenAI provides tools to effectively build, deploy, monitor, evaluate, and improve agentic systems.
    *   **Phase** | **Description** | **OpenAI Primitives**
        ---|---|---
        Build and deploy | Rapidly build agents, enforce guardrails, and handle conversational flows using the Agents SDK. | Agents SDK
        Monitor | Observe agent behavior in real-time, debug issues, and gain insights through tracing. | Tracing
        Evaluate and improve | Measure agent performance, identify areas for improvement, and refine your agents. | Evaluations, Fine-tuning
*   **Get started:** Get started by installing the OpenAI Agents SDK for Python via: `pip install openai-agents`. Explore the repository and documentation for more details.

### Voice Agents (General Guide)

*   Use the OpenAI API and Agents SDK to create powerful, context-aware voice agents for applications like customer support and language tutoring. This guide helps you design and build a voice agent.
*   **Choose the right architecture:** OpenAI provides two primary architectures for building voice agents:
    1.  **Speech-to-speech (multimodal):** The multimodal speech-to-speech (S2S) architecture directly processes audio inputs and outputs, handling speech in real time in a single multimodal model, `gpt-4o-realtime-preview`. The model thinks and responds in speech. It doesn't rely on a transcript of the user's inputit hears emotion and intent, filters out noise, and responds directly in speech. Use this approach for highly interactive, low-latency, conversational use cases.
        *   **Strengths:** Low latency interactions, Rich multimodal understanding, Natural conversational flow, Enhanced user experience.
        *   **Best for:** Interactive/unstructured conversations, Language tutoring, Conversational search, Interactive customer service.
    2.  **Chained (speech-to-text  LLM  text-to-speech):** A chained architecture processes audio sequentially, converting audio to text, generating intelligent responses using large language models (LLMs), and synthesizing audio from text. We recommend this predictable architecture if you're new to building voice agents. Both the user input and model's response are in text, so you have a transcript and can control what happens in your application. It's also a reliable way to convert an existing LLM-based application into a voice agent. (Example chain: `gpt-4o-transcribe`  `gpt-4o`  `gpt-4o-mini-tts`)
        *   **Strengths:** High control/transparency, Robust function calling, Reliable/predictable responses, Support for extended context/transcripts.
        *   **Best for:** Structured workflows, Customer support, Sales/inbound triage, Scenarios involving transcripts/scripted responses.
*   **Build a voice agent:**
    *   **Use a speech-to-speech architecture for realtime processing:** Requires establishing a connection for realtime data transfer, creating a realtime session with the Realtime API, using an OpenAI model with realtime audio I/O capabilities (e.g., `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`). Read the [Realtime API guide](/docs/guides/realtime) and [reference](/docs/api-reference/realtime-sessions).
    *   **Chain together audio input  text processing  audio output:** The Agents SDK supports extending existing agents with voice. Install with `pip install openai-agents[voice]`. See the [Agents SDK voice agents quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/) in GitHub. In the example, you'll: Run STT, run agentic workflow, run TTS.

### Developer Quickstart (General OpenAI API)

*   Overview of using the API for text generation, image analysis, tools, streaming, and agents. Includes code snippets (JavaScript focus in original).
*   **Generate text:** Uses `client.responses.create` with `model` and `input`.
*   **Analyze images:** Provide image inputs (`input_image` type with `image_url`).
*   **Extend with tools:** Use built-in tools like `web_search_preview` via the `tools` parameter.
*   **Deliver fast experiences:** Streaming (`stream: true`) or Realtime API.
*   **Build agents:** References Agents SDK example.
*   **Explore further:** Links to guides on prompting, vision, JSON output, function calling, web/file search, etc.

### Text Generation and Prompting (General Guide)

*   Using the API (e.g., Responses API) to generate text from prompts.
*   Output format (`output` array, `output_text` helper).
*   Choosing models (Reasoning vs. GPT, Large vs. Small). `gpt-4.1` recommended as a starting point.
*   **Prompt engineering:** Techniques for writing effective instructions. Importance of pinning model snapshots and using evals.
*   **Message roles:** `developer`, `user`, `assistant` have different priorities. `instructions` parameter acts like a high-priority developer message.
*   **Message formatting:** Using Markdown and XML tags for structure (Identity, Instructions, Examples, Context sections recommended).
*   **Prompt caching:** Keep static content early in the prompt/request body for cost/latency savings.
*   **Few-shot learning:** Providing input/output examples in the prompt.
*   **Including context (RAG):** Adding relevant information (proprietary data, specific resources) to the prompt. Use vector databases or built-in tools like File Search.
*   **Context window:** Limit on total tokens (input + output + reasoning). Check model docs for sizes.
*   **Prompting GPT-4.1:** Benefits from precise instructions. Refer to [GPT-4.1 prompting guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide) cookbook.
    *   Best practices: System prompt reminders (Persistence, Tool Calling, Planning), use `tools` field for API requests, specific diff format, optimal context placement (critical instructions top & bottom), basic chain-of-thought prompting, explicit instruction following.
    *   Common failure modes: Overly strict instructions (e.g., must call tool), repetitive phrases from examples, excessive prose/formatting.
*   **Prompting reasoning models:** Benefit more from high-level guidance than precise instructions.

### Images and Vision (General Guide)

*   Using models with vision capabilities (e.g., `gpt-4.1-mini`) to understand images.
*   Providing images via URL or Base64 data URL within the `input` array (`type: "input_image"`).
*   **Input requirements:** PNG, JPEG, WEBP, non-animated GIF; up to 20MB/image, 500 images/request, 50MB total bytes/request; low-res 512x512, high-res 768px short side / 2000px long side; no watermarks/logos/text/NSFW, clear content.
*   **`detail` parameter:** `low` (512x512, 85 tokens - except gpt-4o mini), `high` (low-res + detailed crops, more tokens), `auto` (default).
*   **Multiple image inputs:** Pass multiple `input_image` objects in the content array.
*   **Limitations:** Not for medical images/advice; struggles with non-Latin text, small text, rotation, complex visual elements (graphs), precise spatial reasoning (chess), image shape (panoramic/fisheye), metadata/resizing, exact counting, CAPTCHAs. May generate incorrect descriptions.
*   **Calculating costs:** Metered in tokens based on dimensions and detail level. Different calculation methods for GPT-4.1 series vs. GPT-4o/o-series. Consult [pricing page](https://openai.com/api/pricing/).

### Audio and Speech (General Guide)

*   Overview of audio capabilities: Voice agents, transcription, text-to-speech.
*   **Use cases:**
    *   **Voice agents:** Build interactive voice apps using speech-to-speech (Realtime API) or chained STT->LLM->TTS (Agents SDK).
    *   **Streaming audio:** Realtime processing via Realtime API.
    *   **Text to speech (TTS):** Use `/v1/audio/speech` endpoint with `gpt-4o-mini-tts`, `tts-1`, `tts-1-hd`. `gpt-4o-mini-tts` supports instructions for tone/style.
    *   **Speech to text (STT):** Use `/v1/audio/transcriptions` endpoint with `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, `whisper-1`. Streaming available.
*   **Choosing the right API:** Realtime API (realtime), Chat Completions API (multimodal features like function calling), specialized Audio APIs (Transcription, Speech). Consider control needed (conversational vs. scripted).
*   **Adding audio to existing apps:** Use multimodal models (e.g., `gpt-4o-audio-preview`) in Chat Completions API by including `audio` in `modalities`. (Note: Audio not yet supported in Responses API). Includes code examples for audio output and input with Chat Completions.

### Structured Outputs (General Guide)

*   Ensuring model responses adhere to a provided JSON Schema.
*   Benefits: Reliable type-safety, detectable refusals, simpler prompting.
*   Usage in Responses API: `text: { format: { type: "json_schema", name: "...", schema: {...}, strict: true } }`.
*   Comparison: Use function calling for structuring tool interactions, use `text.format` for structuring the final user-facing response.
*   Comparison with JSON Mode: Structured Outputs enforces schema adherence, JSON mode (`text: { format: { type: "json_object" } }`) only ensures valid JSON. Structured Outputs requires newer models (`gpt-4o-mini-2024-07-18`, `gpt-4o-2024-08-06`+).
*   **Examples:** Chain of thought (math tutoring), Structured data extraction (research papers), UI Generation (HTML structure), Moderation (classification).
*   **How-to steps:** Define schema, supply in API call (`text.format`), handle edge cases (refusals via `refusal` property, incomplete responses via `status` and `incomplete_details`).
*   **Streaming:** Supported, recommend using SDKs. Example shows iterating through stream events.
*   **Supported schemas:** Subset of JSON Schema. Supports: String, Number, Boolean, Integer, Object, Array, Enum, `anyOf`. Root object cannot be `anyOf`. All fields must be `required` (use `["type", "null"]` for optionality). Limits on nesting depth (5), total properties (100), string lengths (~15k chars), enum values (~500 total, length limits apply). `additionalProperties: false` required for objects. Definitions (`$defs`) and recursion (`$ref: "#"`) supported. Some keywords (`minLength`, `pattern`, `minimum`, `contains`, etc.) not supported in strict mode.

### Function Calling (General Guide)

*   Enabling models to call custom code or external services.
*   **Steps:**
    1.  Define functions (schema) in `tools` parameter.
    2.  Call model; model may return `function_call` output(s).
    3.  Execute function code based on `name` and `arguments` from model.
    4.  Supply function result back to model using `function_call_output` type in `input` array, matching `call_id`.
    5.  Call model again; model incorporates result into final response.
*   **Function definition schema:** `type: "function"`, `name`, `description`, `parameters` (JSON schema), `strict` (optional bool).
*   **Best practices:** Clear descriptions, system prompt guidance, handle edge cases, make functions intuitive, combine sequential calls, limit function count (<20 suggested), use Playground for schema generation, consider fine-tuning.
*   **Token usage:** Function definitions count against input tokens.
*   **Handling function calls:** Iterate through `output` array, check `type == "function_call"`, parse `arguments` (JSON string), execute code, format result as string, append `function_call` and `function_call_output` items to message history for next API call.
*   **Additional configurations:**
    *   `tool_choice`: Control tool usage (`auto`, `required`, `none`, specific function name).
    *   `parallel_tool_calls`: Allow model to call multiple functions in one turn (default `True`). Disables strict mode for that turn if multiple calls occur. Note on `gpt-4.1-nano` potentially calling same tool multiple times.
    *   `strict` mode: Set `strict: true` in function definition for reliable schema adherence. Requires `additionalProperties: false` and all fields `required` in parameter schema. Use `["type", "null"]` for optionality. Some JSON schema features not supported. Caching implications.
*   **Streaming:** Stream function calls. Events include `response.output_item.added` (for the call itself), `response.function_call_arguments.delta` (for argument chunks), `response.function_call_arguments.done` (full arguments), `response.output_item.done` (full call object). Requires accumulating deltas.

### Conversation State (General Guide)

*   Managing context across multiple turns.
*   **Manual management:** Include previous user and assistant messages in the `input` array for subsequent calls.
*   **Responses API state management:** Use `previous_response_id` parameter to link responses automatically. (Note: Still bills for previous input tokens).
*   **Managing context window:** Total tokens (input + output + reasoning) must be below model limit. Use tokenizer tool (`tiktoken`) to estimate. Truncate or summarize history if needed. Long conversations risk incomplete replies if output hits limit.

### Streaming API Responses (General Guide)

*   Use `stream=True` parameter in API request (Responses or Chat Completions).
*   Responses API uses semantic events (typed objects, e.g., `response.output_text.delta`).
*   Chat Completions API appends to `content` field in deltas.
*   Key lifecycle events: `response.created`, delta events (e.g., `response.output_text.delta`), `response.completed`, `error`.
*   Links to guides for streaming function calls and structured outputs.
*   Moderation risk: Harder to moderate partial completions.

### File Inputs (General Guide)

*   Using PDF files as input to vision-capable models (e.g., gpt-4o).
*   How it works: Model gets extracted text + image of each page.
*   **Uploading:** Use Files API (`purpose="user_data"` recommended), then reference `file_id` in `input` array (`type: "input_file"`).
*   **Base64:** Provide file directly using `file_data: "data:application/pdf;base64,..."` and `filename` within `input` array (`type: "input_file"`).
*   **Usage considerations:** Tokens include text + images; limits: 100 pages/32MB total per request; supported models only; file upload purpose.

### Reasoning Models (General Guide)

*   Models like `o1`, `o3`, `o3-mini`, `o4-mini` perform internal chain-of-thought reasoning.
*   Use via Responses API with `reasoning: { effort: "low" | "medium" | "high" }`.
*   **Reasoning tokens:** Used internally, billed as output tokens, count towards context window, discarded after generation. View count in `usage.output_tokens_details.reasoning_tokens`.
*   **Context window management:** Reserve sufficient space (recommend ~25k initially). Handle `status: "incomplete", reason: "max_output_tokens"`.
*   **Keeping reasoning items in context:** Recommended when doing function calling; pass back reasoning items between last user message and function call output.
*   **Reasoning summaries:** Request with `reasoning: { summary: "auto" | "concise" | "detailed" }`. Summary appears in `reasoning` output item.
*   **Prompting advice:** Prefer high-level guidance over explicit steps compared to GPT models. Refer to [reasoning best practices guide](/docs/guides/reasoning-best-practices).
*   Prompt examples: Coding (refactoring, planning), STEM research.
*   Cookbook examples: Data validation, routine generation.

### Evaluating Model Performance (General Guide)

*   Testing model outputs against criteria using Evals API or dashboard.
*   **Steps:** Define eval (task schema + criteria), run eval (prompt + test data), analyze results.
*   **Creating an eval:** POST to `/v1/evals` with `name`, `data_source_config` (item schema, `include_sample_schema`), `testing_criteria` (e.g., `string_check` with templated `input` and `reference`).
*   **Testing a prompt:** Upload test data (JSONL recommended) via Files API (`purpose="evals"`). Create eval run (POST `/v1/evals/{eval_id}/runs`) specifying `name`, `data_source` (including `model`, `input` template, `source` file ID).
*   **Analyzing results:** Fetch run status/results (GET `/v1/evals/{eval_id}/runs/{run_id}`). Check `status`, `result_counts`, `per_testing_criteria_results`, `report_url`.

### Built-in Tools (General Guide)

*   Extend model capabilities (Web search, File search, Computer use, Function calling).
*   Enabled via `tools` parameter in Responses API.
*   Model decides usage based on prompt; `tool_choice` can guide this.

### Web Search (Tool Guide)

*   Enable with `tools: [ { type: "web_search_preview" } ]` (or specific dated version).
*   Model chooses when to use it. Forcing via `tool_choice` possible.
*   Output includes `web_search_call` item and `message` with `url_citation` annotations (URL, title). Display citations clearly.
*   `user_location` parameter (`country`, `city`, `region`, `timezone`) refines results.
*   `search_context_size` parameter (`low`, `medium`, `high`) controls context retrieved, affecting cost, quality, latency. Tokens used by search tool are separate from main model context/billing.
*   Limitations: Not supported in `gpt-4.1-nano`. Specific preview models (`gpt-4o-search-preview`) have limited params. Tiered rate limits apply. Data handling per policies.

### File Search (Tool Guide)

*   Allows models to search uploaded files in Vector Stores via Responses API.
*   Requires pre-creating Vector Store and uploading files (using Files API).
*   Enable with `tools: [ { type: "file_search", vector_store_ids: ["vs_..."] } ]`. (Currently only one VS ID allowed per call).
*   Output includes `file_search_call` item and `message` with `file_citation` annotations.
*   **Retrieval customization:**
    *   `max_num_results`: Limit number of results (default 10, max 50).
    *   `include=["file_search_call.results"]`: Include actual search result text chunks in the response.
    *   `filters`: Filter results based on file metadata attributes (see [Retrieval guide](/docs/guides/retrieval)).
*   Supported file formats listed.
*   Limitations: Project storage (100GB), vector store file count (10k), individual file size (512MB / ~5M tokens).

### Computer Use (Tool Guide - Beta)

*   Uses `computer-use-preview` model (Responses API only) to simulate computer interaction.
*   Requires an environment (browser automation like Playwright, VM like Docker) to execute actions and capture screenshots.
*   **Loop:**
    1.  Send request with `computer_use_preview` tool config (display size, environment), optional initial screenshot. Set `truncation: "auto"`. Optionally request `reasoning.summary`.
    2.  Model responds with `computer_call` item (action: click, scroll, keypress, type, wait, etc.) and possibly `reasoning` item. May include `pending_safety_checks`.
    3.  Execute the action in your environment.
    4.  Capture screenshot of the updated state.
    5.  Send screenshot back as `computer_call_output` (incl. `call_id`, `output` with image, `acknowledged_safety_checks` if any, optional `current_url`). Repeat from step 2.
*   **Safety Checks:** `malicious_instructions`, `irrelevant_domain`, `sensitive_domain`. Require user confirmation/awareness. Pass back acknowledged checks. Providing `current_url` helps accuracy.
*   **Limitations:** Beta, best for browser tasks, OS automation less reliable (OSWorld 38.1%), specific rate limits/features.
*   **Risks & Safety:** Human-in-the-loop for high stakes, beware prompt injection (use sandboxed env), use allow/blocklists, send user IDs, use/acknowledge safety checks, implement own guardrails, comply with Usage Policy.

### Realtime API (Beta Guide)

*   For low-latency, multimodal (speech-to-speech, real-time transcription).
*   Uses models like `gpt-4o-realtime-preview`.
*   **Connection:** WebRTC (client-side, needs ephemeral key) or WebSockets (server-side, can use standard key).
*   **WebRTC:** Requires server endpoint to mint ephemeral keys. Client uses key to establish peer connection. Handles audio via MediaStreams.
*   **WebSockets:** Connect directly using standard/ephemeral key. Requires manual handling of audio chunking/encoding (Base64).
*   **Use Cases:** Realtime Conversations, Realtime Transcription (set `intent=transcription`).
*   Event-driven architecture.

### Batch API (Guide)

*   Asynchronous processing for large jobs (evals, classification, embeddings).
*   Benefits: 50% lower cost, higher separate rate limits, 24h turnaround.
*   **Steps:**
    1.  Prepare input `.jsonl` file (each line a request: `custom_id`, `method`, `url`, `body`). Single model per file.
    2.  Upload file (`purpose="batch"`). Get `file_id`.
    3.  Create batch (POST `/v1/batches`) with `input_file_id`, `endpoint`, `completion_window` (`"24h"`). Get `batch_id`.
    4.  Check status (GET `/v1/batches/{batch_id}`). Statuses: validating, failed, in_progress, finalizing, completed, expired, cancelling, cancelled.
    5.  Retrieve results (GET `/v1/files/{output_file_id}/content`). Output `.jsonl` matches requests by `custom_id`. Errors in `error_file_id`. Output file deleted after 30 days.
    6.  Cancel batch (POST `/v1/batches/{batch_id}/cancel`).
    7.  List batches (GET `/v1/batches`).
*   **Model Availability:** Most, but not all, models supported. Check model docs.
*   **Rate Limits:** Per-batch (50k requests, 200MB file; embeddings limit 50k inputs total). Enqueued prompt tokens per model (see limits page). Does not consume standard rate limits.
*   **Expiration:** Batches expire after 24h. Completed requests charged, unfinished cancelled/logged in error file.
*   Cookbook examples available.

### Prompt Generation (Playground Feature)

*   "Generate" button helps create prompts, functions, schemas from descriptions.
*   Uses meta-prompts (incorporating best practices) for prompt generation.
*   Uses meta-schemas (self-describing schemas) for function/structured output schema generation.
*   Handles schema generation constraints (strict mode limitations) and performs output cleaning.

### Production Best Practices (Guide)

*   **Organization setup:** Org name/ID, members (owner/reader roles), billing limits (usage limit, notification threshold, monthly budget), API key management & safety.
*   **Staging projects:** Isolate dev/test from production, separate limits/access.
*   **Scaling architecture:** Horizontal scaling (more nodes), vertical scaling (bigger nodes), caching (API responses, common data), load balancing.
*   **Managing rate limits:** Understand TPM/RPM limits, batching requests.
*   **Improving latencies:** (See Latency Optimization guide).
*   **Managing costs:** Pay-as-you-go (per token), estimate usage, reduce tokens (smaller models, shorter prompts, fine-tuning, caching), use tokenizer tool.
*   **MLOps strategy:** Data/model management, monitoring, retraining, deployment automation.
*   **Security and compliance:** Data handling (storage, transmission, retention), privacy, secure coding. Check OpenAI security/trust docs, Privacy Policy, Terms of Use.
*   **Safety best practices:** (See separate guide).
*   **Business considerations:** Define success/failure costs, measure impact (CSAT, accuracy, time-to-resolution), align technical solutions with business risk tolerance.

### Safety Best Practices (Guide)

*   **Use Moderation API:** Free tool to filter unsafe content.
*   **Adversarial testing:** Red-team your application against prompt injections, off-topic drift.
*   **Human in the loop (HITL):** Especially critical for high-stakes domains, code generation. Provide verification tools.
*   **Prompt engineering:** Constrain topic and tone, provide examples.
*   **"Know your customer" (KYC):** User registration/login, potentially link accounts or require ID/payment.
*   **Constrain inputs/outputs:** Limit input length, output tokens. Use validated inputs (dropdowns) vs. open text. Return results from trusted sources where possible.
*   **Allow user reporting:** Provide mechanism for users to report issues.
*   **Understand/communicate limitations:** Be aware of hallucinations, bias, etc. Calibrate user expectations.
*   **Report security issues:** Use Coordinated Vulnerability Disclosure Program.
*   **End-user IDs:** Send unique (hashed recommended) user IDs via `user` parameter for abuse monitoring.

### Prompt Caching (Guide)

*   Reduces latency (up to 80%) and cost (up to 50%) for requests with identical prompt prefixes.
*   Automatic for prompts >= 1024 tokens (hits in 128-token increments).
*   Requires exact prefix match; structure prompts with static content first.
*   Cache lookup happens automatically.
*   `usage.prompt_tokens_details.cached_tokens` shows cached amount.
*   Caches messages, images, tools, structured output schemas.
*   Cache lifetime ~5-10 mins inactivity (up to 1 hour off-peak).
*   Does not affect output content, rate limits. Compliant with ZDR. Available on Scale Tier. Manual clearing not available.

### Predicted Outputs (Guide)

*   Speeds up Chat Completions responses when most output is known (e.g., code refactoring).
*   Provide prediction via `prediction: { type: "content", content: "..." }` parameter.
*   Supported by `gpt-4o`, `gpt-4o-mini` models.
*   Reduces latency, especially with streaming.
*   `usage.completion_tokens_details` shows `accepted_prediction_tokens` and `rejected_prediction_tokens`.
*   **Cost:** Rejected prediction tokens are still billed as completion tokens.
*   Prediction text can appear anywhere in the final generated output.
*   **Limitations:** Model support; cost implication; incompatible with n>1, logprobs, presence/frequency penalties > 0, audio, non-text modalities, max_completion_tokens, tools.

### Latency Optimization (Guide)

*   **7 Principles:**
    1.  **Process tokens faster:** Smaller models, fine-tuning, Predicted Outputs.
    2.  **Generate fewer tokens:** Ask for conciseness, minimize syntax, use stop sequences/max_tokens.
    3.  **Use fewer input tokens:** Fine-tuning, filter context (RAG), maximize shared prefix (KV cache). (Less impact than output tokens).
    4.  **Make fewer requests:** Combine sequential steps into single prompts.
    5.  **Parallelize:** Run independent steps concurrently; use speculative execution for sequential steps.
    6.  **Make users wait less:** Streaming, chunking (backend processing -> frontend), show intermediate steps, loading states.
    7.  **Don't default to an LLM:** Use hard-coding, pre-computing, UI elements, traditional algorithms where appropriate.
*   Includes detailed customer service bot optimization example applying several principles.

### Optimizing LLM Accuracy (Guide)

*   **Mental Model:** Optimization involves **Context Optimization** (for knowledge gaps -> RAG) and **LLM Optimization** (for consistency/behavior -> Prompt Eng/Fine-tuning). It's iterative: Evaluate -> Hypothesize -> Apply -> Evaluate.
*   **Prompt Engineering:** Best starting point. Define task, test, iterate by adding context/instructions/examples. Evaluate using metrics (Bleu, ROUGE, BERTScore) or LLM-as-judge (G-Eval).
*   **Retrieval-Augmented Generation (RAG):** Addresses context/knowledge gaps. Retrieve relevant info, augment prompt. Evaluate both retrieval quality and generation quality. Tune retrieval (search params, chunking) and generation (prompting, fine-tuning).
*   **Fine-tuning:** Addresses learned memory/behavior issues (consistency, style, format, reasoning). Improves accuracy or efficiency. Requires high-quality, representative training data (start ~50 examples). Use prompt baking from logs. Evaluate against hold-out set.
*   **Combining RAG + Fine-tuning:** Addresses both context and behavior issues. Fine-tuning can minimize prompt tokens, teach complex behavior; RAG injects context.
*   **Example (Icelandic Correction):** Demonstrated fine-tuning significantly improved behavior-based task; RAG slightly decreased accuracy when model already learned task well via fine-tuning.
*   **When is accuracy "good enough"?**
    *   **Business Context:** Quantify cost/value of success/failure cases. Measure impact on business metrics (CSAT, resolution time). Determine break-even accuracy based on value. Hand off high-risk tasks to humans.
    *   **Technical Context:** Implement graceful failure handling (ask for clarification, allow human handoff, design resilient UX). Trade off latency/cost vs. accuracy/safety based on business needs.

### Advanced Usage (Guide)

*   **Reproducible Outputs:** Use `seed` parameter (integer) and check `system_fingerprint`. Requires identical parameters. Determinism not guaranteed across OpenAI system changes (indicated by `system_fingerprint`).
*   **Managing Tokens:** Understand tokenization (1 token  4 chars / 0.75 words English). Use `tiktoken` library to count tokens. Total tokens (input+output) affect cost, latency, validity (must be < model max limit). Check `usage` field in response. Chat messages include overhead. Truncate/omit messages if exceeding limit.
*   **Frequency/Presence Penalties:** `frequency_penalty`, `presence_penalty` reduce repetition (values ~0.1-1, up to 2 for strong suppression, negative to increase repetition).
*   **Token Log Probabilities:** `logprobs` parameter returns log probabilities for output tokens and top alternative tokens at each step. Useful for confidence assessment.


```markdown
# OpenAI Agents SDK

*(... Content from the previous response up to here ...)*

---

## API Reference (Detailed)

*(This section mirrors the API reference structure from the original text, providing detailed parameter and class descriptions.)*

### `agents` module (Top-Level Configuration)

*   **`set_default_openai_key`**
    ```python
    set_default_openai_key(
        key: str, use_for_tracing: bool = True
    ) -> None
    ```
    Set the default OpenAI API key to use for LLM requests (and optionally tracing). This is only necessary if the `OPENAI_API_KEY` environment variable is not already set. If provided, this key will be used instead of the `OPENAI_API_KEY` environment variable.
    *   **Parameters:**
        *   `key` (str, required): The OpenAI key to use.
        *   `use_for_tracing` (bool, default=True): Whether to also use this key to send traces to OpenAI. If False, you'll either need to set the `OPENAI_API_KEY` environment variable or call `set_tracing_export_api_key()` with the API key you want to use for tracing.
    *   *Source code in `src/agents/__init__.py`*

*   **`set_default_openai_client`**
    ```python
    set_default_openai_client(
        client: AsyncOpenAI, use_for_tracing: bool = True
    ) -> None
    ```
    Set the default OpenAI client to use for LLM requests and/or tracing. If provided, this client will be used instead of the default OpenAI client.
    *   **Parameters:**
        *   `client` (AsyncOpenAI, required): The OpenAI client to use.
        *   `use_for_tracing` (bool, default=True): Whether to use the API key from this client for uploading traces. If False, you'll either need to set the `OPENAI_API_KEY` environment variable or call `set_tracing_export_api_key()` with the API key you want to use for tracing.
    *   *Source code in `src/agents/__init__.py`*

*   **`set_default_openai_api`**
    ```python
    set_default_openai_api(
        api: Literal["chat_completions", "responses"],
    ) -> None
    ```
    Set the default API to use for OpenAI LLM requests. By default, we will use the responses API but you can set this to use the chat completions API instead.
    *   *Source code in `src/agents/__init__.py`*

*   **`enable_verbose_stdout_logging`**
    ```python
    enable_verbose_stdout_logging()
    ```
    Enables verbose logging to stdout. This is useful for debugging.
    *   *Source code in `src/agents/__init__.py`*

### `agents.tracing` module (Tracing Configuration)

*   **`set_tracing_export_api_key`**
    ```python
    set_tracing_export_api_key(api_key: str) -> None
    ```
    Set the OpenAI API key for the backend exporter.
    *   *Source code in `src/agents/tracing/__init__.py`*

*   **`set_tracing_disabled`**
    ```python
    set_tracing_disabled(disabled: bool) -> None
    ```
    Set whether tracing is globally disabled.
    *   *Source code in `src/agents/tracing/__init__.py`*

*   **`set_trace_processors`**
    ```python
    set_trace_processors(
        processors: list[TracingProcessor],
    ) -> None
    ```
    Set the list of trace processors. This will replace the current list of processors.
    *   *Source code in `src/agents/tracing/__init__.py`*

*   **`add_trace_processor`**
    ```python
    add_trace_processor(
        span_processor: TracingProcessor,
    ) -> None
    ```
    Adds a new trace processor. This processor will receive all traces/spans.
    *   *Source code in `src/agents/tracing/__init__.py`*

### Agents (`agents.agent` module)

*   **`ToolsToFinalOutputFunction`** (module-attribute)
    ```python
    ToolsToFinalOutputFunction: TypeAlias = Callable[
        [RunContextWrapper[TContext], list[FunctionToolResult]],
        MaybeAwaitable[ToolsToFinalOutputResult],
    ]
    ```
    A function that takes a run context and a list of tool results, and returns a `ToolsToFinalOutputResult`.

*   **`ToolsToFinalOutputResult`** (dataclass)
    *Source code in `src/agents/agent.py`*
    *   `is_final_output` (instance-attribute, bool): Whether this is the final output. If False, the LLM will run again and receive the tool call output.
    *   `final_output` (class-attribute, instance-attribute, Any | None = None): The final output. Can be None if `is_final_output` is False, otherwise must match the `output_type` of the agent.

*   **`StopAtTools`** (TypedDict)
    *Bases: `TypedDict`*
    *Source code in `src/agents/agent.py`*
    *   `stop_at_tool_names` (instance-attribute, list[str]): A list of tool names, any of which will stop the agent from running further.

*   **`MCPConfig`** (TypedDict)
    *Bases: `TypedDict`*
    Configuration for MCP servers.
    *Source code in `src/agents/agent.py`*
    *   `convert_schemas_to_strict` (instance-attribute, NotRequired[bool]): If True, we will attempt to convert the MCP schemas to strict-mode schemas. This is a best-effort conversion, so some schemas may not be convertible. Defaults to False.

*   **`Agent`** (dataclass)
    *Bases: `Generic[TContext]`*
    An agent is an AI model configured with instructions, tools, guardrails, handoffs and more. We strongly recommend passing instructions, which is the "system prompt" for the agent. In addition, you can pass `handoff_description`, which is a human-readable description of the agent, used when the agent is used inside tools/handoffs. Agents are generic on the context type. The context is a (mutable) object you create. It is passed to tool functions, handoffs, guardrails, etc.
    *Source code in `src/agents/agent.py`*
    *   **Attributes:**
        *   `name` (instance-attribute, str): The name of the agent.
        *   `instructions` (class-attribute, instance-attribute, `str | Callable[[RunContextWrapper[TContext], Agent[TContext]], MaybeAwaitable[str]] | None` = None): The instructions for the agent. Will be used as the "system prompt". Can either be a string, or a function that dynamically generates instructions.
        *   `handoff_description` (class-attribute, instance-attribute, `str | None` = None): A description used when the agent is a handoff target.
        *   `handoffs` (class-attribute, instance-attribute, `list[Agent[Any] | Handoff[TContext]]` = `field(default_factory=list)`): Sub-agents that the agent can delegate to.
        *   `model` (class-attribute, instance-attribute, `str | Model | None` = None): Model implementation to use. Defaults to `openai_provider.DEFAULT_MODEL`.
        *   `model_settings` (class-attribute, instance-attribute, `ModelSettings` = `field(default_factory=ModelSettings)`): Model-specific tuning parameters.
        *   `tools` (class-attribute, instance-attribute, `list[Tool]` = `field(default_factory=list)`): Tools the agent can use.
        *   `mcp_servers` (class-attribute, instance-attribute, `list[MCPServer]` = `field(default_factory=list)`): Model Context Protocol servers. NOTE: You must manage their lifecycle (`connect()`, `cleanup()`).
        *   `mcp_config` (class-attribute, instance-attribute, `MCPConfig` = `field(default_factory=lambda: MCPConfig())`): Configuration for MCP servers.
        *   `input_guardrails` (class-attribute, instance-attribute, `list[InputGuardrail[TContext]]` = `field(default_factory=list)`): Checks run on initial input if this is the first agent.
        *   `output_guardrails` (class-attribute, instance-attribute, `list[OutputGuardrail[TContext]]` = `field(default_factory=list)`): Checks run on final output if this agent produces it.
        *   `output_type` (class-attribute, instance-attribute, `type[Any] | None` = None): Type of the output object (default `str`).
        *   `hooks` (class-attribute, instance-attribute, `AgentHooks[TContext] | None` = None): Class receiving agent-specific lifecycle callbacks.
        *   `tool_use_behavior` (class-attribute, instance-attribute, `Literal["run_llm_again", "stop_on_first_tool"] | StopAtTools | ToolsToFinalOutputFunction` = `"run_llm_again"`): How tool use is handled. Options: run LLM again (default), stop on first tool output, stop if specific tools called, use function to decide. NOTE: Specific to `FunctionTools`; hosted tools always processed by LLM.
        *   `reset_tool_choice` (class-attribute, instance-attribute, `bool` = `True`): Reset `tool_choice` to default after a tool call (prevents infinite loops).
    *   **Methods:**
        *   **`clone(**kwargs: Any) -> Agent[TContext]`**
            Make a copy of the agent, with the given arguments changed. Example: `new_agent = agent.clone(instructions="New instructions")`.
            *Source code in `src/agents/agent.py`*
        *   **`as_tool(tool_name: str | None, tool_description: str | None, custom_output_extractor: Callable[[RunResult], Awaitable[str]] | None = None) -> Tool`**
            Transform this agent into a tool, callable by other agents. Different from handoffs: new agent gets generated input (not history), original agent continues conversation after tool call.
            *   **Parameters:**
                *   `tool_name` (str | None, required): Tool name (defaults to agent name).
                *   `tool_description` (str | None, required): Tool description.
                *   `custom_output_extractor` (Callable[[RunResult], Awaitable[str]] | None, default=None): Optional function to extract output string from the tool agent's `RunResult`. Defaults to last message.
            *Source code in `src/agents/agent.py`*
        *   **`get_system_prompt(run_context: RunContextWrapper[TContext]) -> MaybeAwaitable[str | None]`** (async capable)
            Get the system prompt for the agent, handling dynamic instruction functions.
            *Source code in `src/agents/agent.py`*
        *   **`get_mcp_tools() -> Awaitable[list[Tool]]`** (async)
            Fetches the available tools from the MCP servers.
            *Source code in `src/agents/agent.py`*
        *   **`get_all_tools() -> Awaitable[list[Tool]]`** (async)
            All agent tools, including MCP tools and function tools.
            *Source code in `src/agents/agent.py`*

### Runner (`agents.run` module)

*   **`Runner`** (class)
    *Source code in `src/agents/run.py`*
    *   **Class Methods:**
        *   **`run(starting_agent: Agent[TContext], input: str | list[TResponseInputItem], *, context: TContext | None = None, max_turns: int = DEFAULT_MAX_TURNS, hooks: RunHooks[TContext] | None = None, run_config: RunConfig | None = None, previous_response_id: str | None = None) -> Awaitable[RunResult]`** (async)
            Run a workflow starting at the given agent. Runs in a loop (invoke LLM -> check output/handoff/tool -> run tools/update agent -> repeat) until final output or `max_turns`. Raises `MaxTurnsExceeded` or `GuardrailTripwireTriggered`. Only first agent's input guardrails run.
            *   **Parameters:** `starting_agent`, `input`, `context`, `max_turns`, `hooks`, `run_config`, `previous_response_id`.
            *   **Returns:** `RunResult`.
            *Source code in `src/agents/run.py`*
        *   **`run_sync(...) -> RunResult`**
            Synchronous wrapper for `run()`. **Warning:** Will not work if an event loop is already running (e.g., async function, Jupyter, FastAPI).
            *   **Parameters:** Same as `run`.
            *   **Returns:** `RunResult`.
            *Source code in `src/agents/run.py`*
        *   **`run_streamed(...) -> RunResultStreaming`** (async)
            Run a workflow in streaming mode. Returns `RunResultStreaming` containing run data and `.stream_events()` method. Raises exceptions like `run`.
            *   **Parameters:** Same as `run`.
            *   **Returns:** `RunResultStreaming`.
            *Source code in `src/agents/run.py`*

*   **`RunConfig`** (dataclass)
    Configures settings for the entire agent run.
    *Source code in `src/agents/run.py`*
    *   **Attributes:**
        *   `model` (`str | Model | None`, default=None): Override model for all agents in run.
        *   `model_provider` (`ModelProvider`, default=`OpenAIProvider`): Provider for resolving model names.
        *   `model_settings` (`ModelSettings | None`, default=None): Global override for model settings.
        *   `handoff_input_filter` (`HandoffInputFilter | None`, default=None): Global input filter for handoffs (unless `Handoff.input_filter` is set).
        *   `input_guardrails` (`list[InputGuardrail[Any]] | None`, default=None): Global input guardrails for initial input.
        *   `output_guardrails` (`list[OutputGuardrail[Any]] | None`, default=None): Global output guardrails for final output.
        *   `tracing_disabled` (bool, default=False): Disable tracing for this run.
        *   `trace_include_sensitive_data` (bool, default=True): Include sensitive data (LLM/tool I/O) in traces.
        *   `workflow_name` (str, default='Agent workflow'): Name for tracing.
        *   `trace_id` (`str | None`, default=None): Custom trace ID.
        *   `group_id` (`str | None`, default=None): Grouping ID for traces (e.g., conversation ID).
        *   `trace_metadata` (`dict[str, Any] | None`, default=None): Additional metadata for trace.

### Tools (`agents.tool` module)

*   **`Tool`** (module-attribute)
    ```python
    Tool = Union[
        FunctionTool,
        FileSearchTool,
        WebSearchTool,
        ComputerTool,
    ]
    ```
    A tool that can be used in an agent.

*   **`FunctionToolResult`** (dataclass)
    *Source code in `src/agents/tool.py`*
    *   `tool` (instance-attribute, FunctionTool): The tool that was run.
    *   `output` (instance-attribute, Any): The output of the tool.
    *   `run_item` (instance-attribute, RunItem): The run item produced by the tool call.

*   **`FunctionTool`** (dataclass)
    A tool that wraps a function. Use `@function_tool` helper usually.
    *Source code in `src/agents/tool.py`*
    *   `name` (instance-attribute, str): Tool name shown to LLM.
    *   `description` (instance-attribute, str): Tool description shown to LLM.
    *   `params_json_schema` (instance-attribute, dict[str, Any]): JSON schema for parameters.
    *   `on_invoke_tool` (instance-attribute, Callable[[RunContextWrapper[Any], str], Awaitable[Any]]): Async function invoking the tool. Receives context, JSON args string. Must return stringable output. Handle errors internally or return error string.
    *   `strict_json_schema` (class-attribute, instance-attribute, bool = True): Whether JSON schema is strict mode (recommended).

*   **`FileSearchTool`** (dataclass)
    Hosted tool for vector store search (OpenAI models, Responses API only).
    *Source code in `src/agents/tool.py`*
    *   `vector_store_ids` (instance-attribute, list[str]): IDs of vector stores.
    *   `max_num_results` (class-attribute, instance-attribute, int | None = None): Max results to return.
    *   `include_search_results` (class-attribute, instance-attribute, bool = False): Include search result chunks in LLM output.
    *   `ranking_options` (class-attribute, instance-attribute, RankingOptions | None = None): Search ranking options.
    *   `filters` (class-attribute, instance-attribute, Filters | None = None): Filter based on file attributes.

*   **`WebSearchTool`** (dataclass)
    Hosted tool for web search (OpenAI models, Responses API only).
    *Source code in `src/agents/tool.py`*
    *   `user_location` (class-attribute, instance-attribute, UserLocation | None = None): Optional location for relevant results.
    *   `search_context_size` (class-attribute, instance-attribute, Literal["low", "medium", "high"] = "medium"): Amount of context used for search.

*   **`ComputerTool`** (dataclass)
    Hosted tool for computer control.
    *Source code in `src/agents/tool.py`*
    *   `computer` (instance-attribute, Computer | AsyncComputer): Implementation providing environment details and actions.

*   **`default_tool_error_function`**
    ```python
    default_tool_error_function(
        ctx: RunContextWrapper[Any], error: Exception
    ) -> str
    ```
    Default error handler for `@function_tool`, returns generic error message.
    *Source code in `src/agents/tool.py`*

*   **`function_tool`** (decorator)
    ```python
    # Multiple overloads for usage as decorator directly or with arguments
    function_tool(...) -> FunctionTool | Callable[[ToolFunction], FunctionTool]
    ```
    Decorator to create `FunctionTool` from a Python function. Parses signature, docstrings.
    *   **Parameters:**
        *   `func` (ToolFunction | None): Function to wrap.
        *   `name_override` (str | None): Override function name for tool.
        *   `description_override` (str | None): Override docstring description.
        *   `docstring_style` (DocstringStyle | None): Explicit docstring style (google, sphinx, numpy). Auto-detects if None.
        *   `use_docstring_info` (bool, default=True): Use docstring for tool/arg descriptions.
        *   `failure_error_function` (ToolErrorFunction | None, default=`default_tool_error_function`): Function to generate error message on failure. Pass `None` to re-raise exceptions.
        *   `strict_mode` (bool, default=True): Enable strict JSON schema mode (recommended).
    *Source code in `src/agents/tool.py`*

### Results (`agents.result` module)

*   **`RunResultBase`** (dataclass, ABC)
    *Source code in `src/agents/result.py`*
    *   **Attributes:**
        *   `input` (instance-attribute, `str | list[TResponseInputItem]`): Original input items (potentially mutated by filters).
        *   `new_items` (instance-attribute, `list[RunItem]`): New items generated during the run.
        *   `raw_responses` (instance-attribute, `list[ModelResponse]`): Raw LLM responses.
        *   `final_output` (instance-attribute, `Any`): Output of the last agent.
        *   `input_guardrail_results` (instance-attribute, `list[InputGuardrailResult]`): Input guardrail results.
        *   `output_guardrail_results` (instance-attribute, `list[OutputGuardrailResult]`): Output guardrail results.
    *   **Properties:**
        *   `last_agent` (abstractmethod property): -> `Agent[Any]`
        *   `last_response_id` (property): -> `str | None` (convenience method)
    *   **Methods:**
        *   **`final_output_as(cls: type[T], raise_if_incorrect_type: bool = False) -> T`**
            Convenience method to cast `final_output`. Raises `TypeError` if `raise_if_incorrect_type` is True and types mismatch.
            *Source code in `src/agents/result.py`*
        *   **`to_input_list() -> list[TResponseInputItem]`**
            Merges original `input` with `new_items` into a list suitable for the next run's input.
            *Source code in `src/agents/result.py`*

*   **`RunResult(RunResultBase)`** (dataclass)
    Result from `Runner.run` / `run_sync`. Implements `last_agent`.
    *Source code in `src/agents/result.py`*
    *(Inherits attributes, properties, methods from RunResultBase)*

*   **`RunResultStreaming(RunResultBase)`** (dataclass)
    Result from `Runner.run_streamed`. Use `.stream_events()` for events. Raises exceptions during streaming if limits/guardrails hit.
    *Source code in `src/agents/result.py`*
    *(Inherits attributes, properties, methods from RunResultBase)*
    *   **Additional Attributes:**
        *   `current_agent` (instance-attribute, `Agent[Any]`): Current agent running.
        *   `current_turn` (instance-attribute, int): Current turn number.
        *   `max_turns` (instance-attribute, int): Max turns allowed for the run.
        *   `final_output` (instance-attribute, `Any`): Populated when run completes (None during run).
        *   `is_complete` (class-attribute, instance-attribute, bool = False): Whether the agent run finished.
    *   **Properties:**
        *   `last_agent` (property): -> `Agent[Any]` (Updates during run, final value after completion).
    *   **Methods:**
        *   **`stream_events() -> AsyncIterator[StreamEvent]`** (async)
            Stream semantic events (`RawResponsesStreamEvent`, `RunItemStreamEvent`, `AgentUpdatedStreamEvent`) as they are generated. Raises `MaxTurnsExceeded` or `GuardrailTripwireTriggered`.
            *Source code in `src/agents/result.py`*

### Streaming Events (`agents.stream_events` module)

*   **`StreamEvent`** (module-attribute, TypeAlias)
    Union of `RawResponsesStreamEvent`, `RunItemStreamEvent`, `AgentUpdatedStreamEvent`.

*   **`RawResponsesStreamEvent`** (dataclass)
    Streaming event from the LLM (raw OpenAI Responses API format).
    *Source code in `src/agents/stream_events.py`*
    *   `type` (class-attribute, instance-attribute, `Literal['raw_response_event']` = `'raw_response_event'`): Event type.
    *   `data` (instance-attribute, `TResponseStreamEvent`): Raw streaming event data.

*   **`RunItemStreamEvent`** (dataclass)
    Higher-level event when a `RunItem` is fully generated.
    *Source code in `src/agents/stream_events.py`*
    *   `type` (class-attribute, instance-attribute, `Literal['run_item_stream_event']` = `'run_item_stream_event'`): Event type.
    *   `name` (instance-attribute, Literal[...] ): Name describing the item type (e.g., `"message_output_created"`, `"tool_called"`).
    *   `item` (instance-attribute, `RunItem`): The fully generated item.

*   **`AgentUpdatedStreamEvent`** (dataclass)
    Event notifying that the currently running agent has changed (due to handoff).
    *Source code in `src/agents/stream_events.py`*
    *   `type` (class-attribute, instance-attribute, `Literal['agent_updated_stream_event']` = `'agent_updated_stream_event'`): Event type.
    *   `new_agent` (instance-attribute, `Agent[Any]`): The new agent instance.

### Handoffs (`agents.handoffs` module)

*   **`HandoffInputFilter`** (module-attribute, TypeAlias)
    `Callable[[HandoffInputData], HandoffInputData]` - Function type for filtering handoff history.

*   **`HandoffInputData`** (dataclass)
    Data passed to the `HandoffInputFilter`.
    *Source code in `src/agents/handoffs.py`*
    *   `input_history` (instance-attribute, `str | tuple[TResponseInputItem, ...]`): Original input history before run.
    *   `pre_handoff_items` (instance-attribute, `tuple[RunItem, ...]`): Items generated before the handoff turn.
    *   `new_items` (instance-attribute, `tuple[RunItem, ...]`): Items generated during handoff turn (incl. handoff call/output).

*   **`Handoff[TContext]`** (dataclass, Generic)
    Represents a configurable handoff.
    *Source code in `src/agents/handoffs.py`*
    *   **Attributes:**
        *   `tool_name` (str): Name of the handoff tool.
        *   `tool_description` (str): Description of the handoff tool.
        *   `input_json_schema` (dict[str, Any]): Schema for handoff input data (if any).
        *   `on_invoke_handoff` (Callable[[RunContextWrapper[Any], str], Awaitable[Agent[TContext]]]): Async function called on invocation. Receives context, args JSON string. Must return target agent.
        *   `agent_name` (str): Name of the target agent being handed off to.
        *   `input_filter` (HandoffInputFilter | None = None): Optional function to filter history seen by target agent. IMPORTANT: Filter modifications won't be streamed; prior items already streamed.
        *   `strict_json_schema` (bool = True): Whether `input_json_schema` is strict mode.

*   **`handoff(...)`** (function)
    ```python
    # Multiple overloads based on on_handoff and input_type presence
    handoff(...) -> Handoff[TContext]
    ```
    Factory function to create `Handoff` objects.
    *   **Parameters:**
        *   `agent` (Agent[TContext], required): Target agent.
        *   `tool_name_override` (str | None): Override default tool name.
        *   `tool_description_override` (str | None): Override default tool description.
        *   `on_handoff` (OnHandoffWithInput | OnHandoffWithoutInput | None): Callback function run on invocation.
        *   `input_type` (type[THandoffInput] | None): Pydantic type for input data (if `on_handoff` takes input).
        *   `input_filter` (Callable[[HandoffInputData], HandoffInputData] | None): Function to filter inputs for target agent.
    *Source code in `src/agents/handoffs.py`*

### Lifecycle (`agents.lifecycle` module)

*   **`RunHooks[TContext]`** (class, Generic)
    Base class for run-level lifecycle callbacks. Subclass and override methods.
    *   **Methods (async):**
        *   `on_agent_start(context, agent)`: Called before agent invoked (each time current agent changes).
        *   `on_agent_end(context, agent, output)`: Called when agent produces final output.
        *   `on_handoff(context, from_agent, to_agent)`: Called when handoff occurs.
        *   `on_tool_start(context, agent, tool)`: Called before tool invoked.
        *   `on_tool_end(context, agent, tool, result)`: Called after tool invoked.

*   **`AgentHooks[TContext]`** (class, Generic)
    Base class for agent-specific lifecycle callbacks (`Agent.hooks`). Subclass and override.
    *   **Methods (async):**
        *   `on_start(context, agent)`: Called before this specific agent is invoked.
        *   `on_end(context, agent, output)`: Called when this specific agent produces final output.
        *   `on_handoff(context, agent, source)`: Called when handed *off to* this agent (source is the originating agent).
        *   `on_tool_start(context, agent, tool)`: Called before tool invoked by this agent.
        *   `on_tool_end(context, agent, tool, result)`: Called after tool invoked by this agent.

### Items (`agents.items` module)

*   **Type Aliases:**
    *   `TResponse = Response`: OpenAI SDK Response type.
    *   `TResponseInputItem = ResponseInputItemParam`: OpenAI SDK Response Input Item type.
    *   `TResponseOutputItem = ResponseOutputItem`: OpenAI SDK Response Output Item type.
    *   `TResponseStreamEvent = ResponseStreamEvent`: OpenAI SDK Response Stream Event type.
    *   `ToolCallItemTypes: TypeAlias = Union[...]`: Union of raw tool call types (`ResponseFunctionToolCall`, `ResponseComputerToolCall`, etc.).
    *   `RunItem: TypeAlias = Union[...]`: Union of all `RunItemBase` subclasses.

*   **`RunItemBase[T]`** (dataclass, ABC, Generic)
    Base class for items generated during a run.
    *Source code in `src/agents/items.py`*
    *   `agent` (instance-attribute, `Agent[Any]`): Agent whose run generated this item.
    *   `raw_item` (instance-attribute, `T`): Raw OpenAI SDK item (`ResponseOutputItem` or `ResponseInputItemParam`).
    *   `to_input_item() -> TResponseInputItem`: Converts item to format suitable for model input.

*   **Specific Item Classes (inherit from `RunItemBase`):**
    *   `MessageOutputItem(RunItemBase[ResponseOutputMessage])`
    *   `HandoffCallItem(RunItemBase[ResponseFunctionToolCall])`
    *   `HandoffOutputItem(RunItemBase[TResponseInputItem])`: Has `source_agent`, `target_agent`.
    *   `ToolCallItem(RunItemBase[ToolCallItemTypes])`
    *   `ToolCallOutputItem(RunItemBase[Union[FunctionCallOutput, ComputerCallOutput]])`: Has `output` (parsed tool result).
    *   `ReasoningItem(RunItemBase[ResponseReasoningItem])`

*   **`ModelResponse`** (dataclass)
    Wraps a single LLM response object.
    *Source code in `src/agents/items.py`*
    *   `output` (instance-attribute, `list[TResponseOutputItem]`): List of outputs from model.
    *   `usage` (instance-attribute, `Usage`): Token usage info.
    *   `response_id` (instance-attribute, `str | None`): Response ID (if supported, e.g., Responses API).
    *   `to_input_items() -> list[TResponseInputItem]`: Converts outputs to list suitable for model input.

*   **`ItemHelpers`** (class)
    Contains static utility methods.
    *Source code in `src/agents/items.py`*
    *   `extract_last_content(message: TResponseOutputItem) -> str`: Extracts last text or refusal.
    *   `extract_last_text(message: TResponseOutputItem) -> str | None`: Extracts last text content (ignores refusals).
    *   `input_to_new_input_list(input: str | list[TResponseInputItem]) -> list[TResponseInputItem]`: Converts input to list format.
    *   `text_message_outputs(items: list[RunItem]) -> str`: Concatenates text from list of `MessageOutputItem`.
    *   `text_message_output(message: MessageOutputItem) -> str`: Extracts all text content from single `MessageOutputItem`.
    *   `tool_call_output_item(tool_call: ResponseFunctionToolCall, output: str) -> FunctionCallOutput`: Creates tool output item.

### Run Context (`agents.run_context`, `agents.usage` modules)

*   **`RunContextWrapper[TContext]`** (dataclass, Generic)
    Wraps the context object passed to `Runner.run()`. Contains usage info. NOTE: Context is local, not passed to LLM.
    *Source code in `src/agents/run_context.py`*
    *   `context` (instance-attribute, `TContext`): The user-provided context object.
    *   `usage` (class-attribute, instance-attribute, `Usage` = `field(default_factory=Usage)`): Usage for the run so far (may be stale in streams until end).

*   **`Usage`** (dataclass)
    Tracks token usage for a run.
    *Source code in `src/agents/usage.py`*
    *   `requests` (class-attribute, instance-attribute, int = 0): Total LLM requests.
    *   `input_tokens` (class-attribute, instance-attribute, int = 0): Total input tokens.
    *   `output_tokens` (class-attribute, instance-attribute, int = 0): Total output tokens.
    *   `total_tokens` (class-attribute, instance-attribute, int = 0): Total tokens.

### Exceptions (`agents.exceptions` module)

*   **`AgentsException(Exception)`**: Base SDK exception. *Source code in `src/agents/exceptions.py`*
*   **`MaxTurnsExceeded(AgentsException)`**: Run exceeded `max_turns`. *Source code in `src/agents/exceptions.py`*
*   **`ModelBehaviorError(AgentsException)`**: Invalid model output (bad JSON, non-existent tool). *Source code in `src/agents/exceptions.py`*
*   **`UserError(AgentsException)`**: Incorrect SDK usage by developer. *Source code in `src/agents/exceptions.py`*
*   **`InputGuardrailTripwireTriggered(AgentsException)`**: Input guardrail tripped. Contains `guardrail_result` (InputGuardrailResult). *Source code in `src/agents/exceptions.py`*
*   **`OutputGuardrailTripwireTriggered(AgentsException)`**: Output guardrail tripped. Contains `guardrail_result` (OutputGuardrailResult). *Source code in `src/agents/exceptions.py`*

### Guardrails (`agents.guardrail` module)

*   **`GuardrailFunctionOutput`** (dataclass)
    Output of a guardrail function.
    *Source code in `src/agents/guardrail.py`*
    *   `output_info` (instance-attribute, Any): Optional info about guardrail output.
    *   `tripwire_triggered` (instance-attribute, bool): Whether to halt agent execution.

*   **`InputGuardrailResult`** (dataclass)
    Result of an input guardrail run.
    *Source code in `src/agents/guardrail.py`*
    *   `guardrail` (instance-attribute, `InputGuardrail[Any]`): The guardrail that ran.
    *   `output` (instance-attribute, `GuardrailFunctionOutput`): Output from the guardrail function.

*   **`OutputGuardrailResult`** (dataclass)
    Result of an output guardrail run.
    *Source code in `src/agents/guardrail.py`*
    *   `guardrail` (instance-attribute, `OutputGuardrail[Any]`): The guardrail that ran.
    *   `agent_output` (instance-attribute, Any): The agent output checked.
    *   `agent` (instance-attribute, `Agent[Any]`): The agent checked.
    *   `output` (instance-attribute, `GuardrailFunctionOutput`): Output from the guardrail function.

*   **`InputGuardrail[TContext]`** (dataclass, Generic)
    Configuration for an input guardrail. Use `@input_guardrail` decorator usually.
    *Source code in `src/agents/guardrail.py`*
    *   `guardrail_function` (instance-attribute, Callable): Function receiving context, agent, input. Returns `GuardrailFunctionOutput`.
    *   `name` (class-attribute, instance-attribute, `str | None` = None): Guardrail name for tracing (defaults to function name).

*   **`OutputGuardrail[TContext]`** (dataclass, Generic)
    Configuration for an output guardrail. Use `@output_guardrail` decorator usually.
    *Source code in `src/agents/guardrail.py`*
    *   `guardrail_function` (instance-attribute, Callable): Function receiving context, agent, output. Returns `GuardrailFunctionOutput`.
    *   `name` (class-attribute, instance-attribute, `str | None` = None): Guardrail name for tracing (defaults to function name).

*   **`input_guardrail(...)`** (decorator)
    Transforms sync/async function into `InputGuardrail`. Use directly or with `name` kwarg.
    *Source code in `src/agents/guardrail.py`*

*   **`output_guardrail(...)`** (decorator)
    Transforms sync/async function into `OutputGuardrail`. Use directly or with `name` kwarg.
    *Source code in `src/agents/guardrail.py`*

### Model Settings (`agents.model_settings` module)

*   **`ModelSettings`** (dataclass)
    Settings for LLM calls. Holds optional config parameters (check provider docs for support).
    *Source code in `src/agents/model_settings.py`*
    *   **Attributes (all optional):**
        *   `temperature` (float | None): Sampling temperature.
        *   `top_p` (float | None): Nucleus sampling parameter.
        *   `frequency_penalty` (float | None): Penalizes new tokens based on frequency.
        *   `presence_penalty` (float | None): Penalizes new tokens based on presence.
        *   `tool_choice` (Literal["auto", "required", "none"] | str | None): Tool choice mode.
        *   `parallel_tool_calls` (bool | None): Allow parallel tool calls (default False).
        *   `truncation` (Literal['auto', 'disabled'] | None): Truncation strategy.
        *   `max_tokens` (int | None): Max output tokens.
        *   `reasoning` (Reasoning | None): Config for reasoning models.
        *   `metadata` (dict[str, str] | None): Metadata for model call.
        *   `store` (bool | None): Store response for retrieval (default True).
        *   `include_usage` (bool | None): Include usage chunk (default True).
        *   `extra_query` (Query | None): Additional query fields.
        *   `extra_body` (Body | None): Additional body fields.
    *   **Methods:**
        *   `resolve(override: ModelSettings | None) -> ModelSettings`: Merges settings, applying non-None values from `override`.

### Agent Output (`agents.agent_output` module)

*   **`AgentOutputSchema`** (class)
    Captures JSON schema for output type and validates/parses LLM JSON.
    *Source code in `src/agents/agent_output.py`*
    *   **`__init__(output_type: type[Any], strict_json_schema: bool = True)`**
    *   **Attributes:** `output_type`, `strict_json_schema` (bool, default=True).
    *   **Methods:**
        *   `is_plain_text() -> bool`: True if `output_type` is plain text (e.g., `str`).
        *   `json_schema() -> dict[str, Any]`: Returns JSON schema for `output_type`.
        *   `validate_json(json_str: str, partial: bool = False) -> Any`: Validates JSON string, returns parsed object or raises `ModelBehaviorError`.
        *   `output_type_name() -> str`: Name of the output type.

### Function Schema (`agents.function_schema` module)

*   **`FuncSchema`** (dataclass)
    Captures schema for a Python function tool.
    *Source code in `src/agents/function_schema.py`*
    *   **Attributes:**
        *   `name` (str): Function name.
        *   `description` (str | None): Function description.
        *   `params_pydantic_model` (type[BaseModel]): Pydantic model for parameters.
        *   `params_json_schema` (dict[str, Any]): JSON schema for parameters.
        *   `signature` (Signature): Function signature object.
        *   `takes_context` (bool = False): If function takes `RunContextWrapper` as first arg.
        *   `strict_json_schema` (bool = True): If schema is strict mode.
    *   **Methods:**
        *   `to_call_args(data: BaseModel) -> tuple[list[Any], dict[str, Any]]`: Converts validated Pydantic data to `(args, kwargs)` for calling original function.

*   **`FuncDocumentation`** (dataclass)
    Metadata extracted from function docstring.
    *Source code in `src/agents/function_schema.py`*
    *   `name` (str): Function `__name__`.
    *   `description` (str | None): Description from docstring.
    *   `param_descriptions` (dict[str, str] | None): Parameter descriptions from docstring.

*   **`generate_func_documentation(...) -> FuncDocumentation`**
    Extracts `FuncDocumentation` from a function using `griffe`. Auto-detects docstring style or uses provided `style`.

*   **`function_schema(...) -> FuncSchema`**
    Generates `FuncSchema` from a Python function. Uses `inspect`, `griffe`, `pydantic`.
    *   **Parameters:** `func`, `docstring_style`, `name_override`, `description_override`, `use_docstring_info`, `strict_json_schema`.

### Model Interface (`agents.models.interface` module)

*   **`ModelTracing`** (Enum)
    Enum for tracing level passed to model.
    *Source code in `src/agents/models/interface.py`*
    *   `DISABLED = 0`
    *   `ENABLED = 1`
    *   `ENABLED_WITHOUT_DATA = 2`

*   **`Model`** (ABC)
    Base interface for calling an LLM.
    *Source code in `src/agents/models/interface.py`*
    *   **Abstract Methods (async):**
        *   `get_response(system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, *, previous_response_id) -> ModelResponse`
        *   `stream_response(...) -> AsyncIterator[TResponseStreamEvent]` (Parameters same as `get_response`)

*   **`ModelProvider`** (ABC)
    Base interface for model providers (lookup Models by name).
    *Source code in `src/agents/models/interface.py`*
    *   **Abstract Methods:**
        *   `get_model(model_name: str | None) -> Model`

### OpenAI Models (`agents.models.openai_*` modules)

*   **`OpenAIChatCompletionsModel(Model)`**: Implements `Model` using Chat Completions API. *Source code in `src/agents/models/openai_chatcompletions.py`*
*   **`OpenAIResponsesModel(Model)`**: Implements `Model` using Responses API. *Source code in `src/agents/models/openai_responses.py`*

### MCP Servers (`agents.mcp.server`, `agents.mcp.util` modules)

*   **`MCPServer`** (ABC)
    Base class for MCP servers.
    *Source code in `src/agents/mcp/server.py`*
    *   **Abstract Properties:** `name` (str).
    *   **Abstract Methods (async):** `connect()`, `cleanup()`, `list_tools() -> list[Tool]`, `call_tool(tool_name: str, arguments: dict[str, Any] | None) -> CallToolResult`.

*   **`MCPServerStdioParams`** (TypedDict)
    Mirrors `mcp.client.stdio.StdioServerParameters`.
    *Source code in `src/agents/mcp/server.py`*
    *   **Keys:** `command` (str, required), `args` (NotRequired[list[str]]), `env` (NotRequired[dict[str, str]]), `cwd` (NotRequired[str | Path]), `encoding` (NotRequired[str]), `encoding_error_handler` (NotRequired[Literal["strict", "ignore", "replace"]]).

*   **`MCPServerStdio(MCPServer)`**
    MCP server via stdio transport.
    *Source code in `src/agents/mcp/server.py`*
    *   **`__init__(params: MCPServerStdioParams, cache_tools_list: bool = False, name: str | None = None)`**
    *   Implements `connect`, `cleanup`, `list_tools`, `call_tool`.
    *   `invalidate_tools_cache()`: Clears cached tools list.
    *   `create_streams()`: Internal method for creating communication streams.

*   **`MCPServerSseParams`** (TypedDict)
    Mirrors params in `mcp.client.sse.sse_client`.
    *Source code in `src/agents/mcp/server.py`*
    *   **Keys:** `url` (str, required), `headers` (NotRequired[dict[str, str]]), `timeout` (NotRequired[float]), `sse_read_timeout` (NotRequired[float]).

*   **`MCPServerSse(MCPServer)`**
    MCP server via HTTP with SSE transport.
    *Source code in `src/agents/mcp/server.py`*
    *   **`__init__(params: MCPServerSseParams, cache_tools_list: bool = False, name: str | None = None)`**
    *   Implements `connect`, `cleanup`, `list_tools`, `call_tool`.
    *   `invalidate_tools_cache()`: Clears cached tools list.
    *   `create_streams()`: Internal method for creating communication streams.

*   **`MCPUtil`** (class)
    Utilities for MCP interop.
    *Source code in `src/agents/mcp/util.py`*
    *   **Class Methods (async):**
        *   `get_all_function_tools(servers: list[MCPServer], convert_schemas_to_strict: bool) -> list[Tool]`
        *   `get_function_tools(server: MCPServer, convert_schemas_to_strict: bool) -> list[Tool]`
        *   `to_function_tool(tool: MCPTool, server: MCPServer, convert_schemas_to_strict: bool) -> FunctionTool` (Note: MCPTool type assumed)
        *   `invoke_mcp_tool(server: MCPServer, tool: MCPTool, context: RunContextWrapper[Any], input_json: str) -> str` (Note: MCPTool type assumed)

### Tracing Module (`agents.tracing.*` - More Detailed)

*   **Processor Interface (`processor_interface.py`)**
    *   `TracingProcessor` (ABC): Interface for processing spans/traces.
        *   `on_trace_start(trace: Trace) -> None` (abstractmethod)
        *   `on_trace_end(trace: Trace) -> None` (abstractmethod)
        *   `on_span_start(span: Span[Any]) -> None` (abstractmethod)
        *   `on_span_end(span: Span[Any]) -> None` (abstractmethod): Should not block/raise.
        *   `shutdown() -> None` (abstractmethod): Called on application stop.
        *   `force_flush() -> None` (abstractmethod): Immediately export queued items.
    *   `TracingExporter` (ABC): Interface for exporting traces/spans.
        *   `export(items: list[Trace | Span[Any]]) -> None` (abstractmethod)

*   **Span Data (`span_data.py`)**
    *   `SpanData` (ABC): Base class for span data.
        *   `type` (abstractmethod property): -> `str`.
        *   `export()` (abstractmethod): -> `dict[str, Any]`.
    *   **Concrete Classes (inherit from `SpanData`):**
        *   `AgentSpanData`: Agent details (name, handoffs, tools, output type).
        *   `CustomSpanData`: Custom span (name, data dict).
        *   `FunctionSpanData`: Function call details (input, output, MCP data).
        *   `GenerationSpanData`: LLM generation details (input, output, model, config, usage).
        *   `GuardrailSpanData`: Guardrail details (name, triggered status).
        *   `HandoffSpanData`: Handoff details (source/destination agents).
        *   `MCPListToolsSpanData`: MCP `list_tools` call details (server, result).
        *   `ResponseSpanData`: OpenAI Response object details (response, input).
        *   `SpeechGroupSpanData`: Grouping for related speech spans.
        *   `SpeechSpanData`: TTS details (input, output, model, config, timing).
        *   `TranscriptionSpanData`: STT details (input, output, model, config).

*   **Spans (`spans.py`)**
    *   `Span[TSpanData]` (ABC, Generic): Base class for spans.
        *   `start(mark_as_current: bool = False)` (abstractmethod)
        *   `finish(reset_current: bool = False) -> None` (abstractmethod)
    *   **Implementations:** `NoOpSpan`, `SpanImpl`.

*   **Traces (`traces.py`)**
    *   `Trace` (ABC): Base class for traces.
        *   `trace_id` (abstractmethod property): -> `str`.
        *   `name` (abstractmethod property): -> `str` (workflow name).
        *   `start(mark_as_current: bool = False)` (abstractmethod)
        *   `finish(reset_current: bool = False)` (abstractmethod)
        *   `export()` (abstractmethod): -> `dict[str, Any] | None`.
    *   **Implementations:** `NoOpTrace`, `TraceImpl`.

*   **Creation (`create.py`)**
    *   Functions to create spans: `agent_span`, `custom_span`, `function_span`, `generation_span`, `guardrail_span`, `handoff_span`, `mcp_tools_span`, `response_span`, `speech_group_span`, `speech_span`, `transcription_span`. All take optional `span_id`, `parent`, `disabled` args.
    *   `trace(workflow_name: str, trace_id: str | None = None, group_id: str | None = None, metadata: dict[str, Any] | None = None, disabled: bool = False) -> Trace`: Creates a `Trace` object. Use as context manager or manually start/finish.
    *   `get_current_span() -> Span[Any] | None`: Gets current active span from context.
    *   `get_current_trace() -> Trace | None`: Gets current active trace from context.

*   **Processors (`processors.py`)**
    *   `ConsoleSpanExporter(TracingExporter)`: Prints traces/spans to console.
    *   `BackendSpanExporter(TracingExporter)`: Exports to OpenAI backend.
        *   `__init__(api_key=None, organization=None, project=None, endpoint=..., max_retries=3, base_delay=1.0, max_delay=30.0)`
        *   `set_api_key(api_key: str)`
        *   `close()`
    *   `BatchTraceProcessor(TracingProcessor)`: Batches traces/spans for export. Thread-based.
        *   `__init__(exporter, max_queue_size=8192, max_batch_size=128, schedule_delay=5.0, export_trigger_ratio=0.7)`
        *   `shutdown(timeout: float | None = None)`
        *   `force_flush()`
    *   `default_exporter() -> BackendSpanExporter`
    *   `default_processor() -> BatchTraceProcessor`

*   **Scope (`scope.py`)**
    *   `Scope`: Class managing current trace/span using `contextvars`.

*   **Setup (`setup.py`)**
    *   `SynchronousMultiTracingProcessor(TracingProcessor)`: Forwards calls to multiple processors. `add_tracing_processor`, `set_processors` methods.
    *   `TraceProvider`: Central provider managing processors and trace/span creation. `register_processor`, `set_processors`, `get_current_trace`, `get_current_span`, `set_disabled`, `create_trace`, `create_span` methods.

*   **Util (`util.py`)**
    *   `time_iso() -> str`
    *   `gen_trace_id() -> str`
    *   `gen_span_id() -> str`
    *   `gen_group_id() -> str`

### Voice (`agents.voice.*` modules - Detailed)

*   **Pipeline (`pipeline.py`)**
    *   `VoicePipeline`: Orchestrates STT->Workflow->TTS.
        *   `__init__(*, workflow: VoiceWorkflowBase, stt_model: STTModel | str | None = None, tts_model: TTSModel | str | None = None, config: VoicePipelineConfig | None = None)`
        *   `run(audio_input: AudioInput | StreamedAudioInput) -> StreamedAudioResult` (async)

*   **Workflow (`workflow.py`)**
    *   `VoiceWorkflowBase` (ABC): Base for custom voice workflows.
        *   `run(transcription: str) -> AsyncIterator[str]` (abstractmethod, async): Implement agent logic here, yield text for TTS.
    *   `VoiceWorkflowHelper`: Helper class.
        *   `stream_text_from(result: RunResultStreaming) -> AsyncIterator[str]` (classmethod, async): Wraps `RunResultStreaming` to yield text events.
    *   `SingleAgentWorkflowCallbacks`: Interface with `on_run(workflow, transcription)`.
    *   `SingleAgentVoiceWorkflow(VoiceWorkflowBase)`: Simple workflow for one agent. `__init__(agent: Agent[Any], callbacks: SingleAgentWorkflowCallbacks | None = None)`.

*   **Input (`input.py`)**
    *   `AudioInput` (dataclass): Static audio buffer.
        *   **Attributes:** `buffer` (NDArray[int16 | float32]), `frame_rate` (int, default=24000), `sample_width` (int, default=2), `channels` (int, default=1).
        *   **Methods:** `to_audio_file() -> tuple[str, BytesIO, str]`, `to_base64() -> str`.
    *   `StreamedAudioInput`: Streamable audio input.
        *   **Methods:** `add_audio(audio: NDArray[int16 | float32])` (async): Pushes audio chunks.

*   **Result (`result.py`)**
    *   `StreamedAudioResult`: Output of `pipeline.run`.
        *   `__init__(tts_model: TTSModel, tts_settings: TTSModelSettings, voice_pipeline_config: VoicePipelineConfig)`
        *   `stream() -> AsyncIterator[VoiceStreamEvent]` (async): Streams audio/lifecycle/error events.

*   **Pipeline Config (`pipeline_config.py`)**
    *   `VoicePipelineConfig` (dataclass): Configuration for `VoicePipeline`.
        *   **Attributes:** `model_provider` (default=`OpenAIVoiceModelProvider`), `tracing_disabled` (bool, default=False), `trace_include_sensitive_data` (bool, default=True), `trace_include_sensitive_audio_data` (bool, default=True), `workflow_name` (str, default='Voice Agent'), `group_id` (str, default=random), `trace_metadata` (dict | None), `stt_settings` (default=`STTModelSettings()`), `tts_settings` (default=`TTSModelSettings()`).

*   **Events (`events.py`)**
    *   `VoiceStreamEvent`: TypeAlias for voice events.
    *   `VoiceStreamEventAudio` (dataclass): `type='voice_stream_event_audio'`, `data` (NDArray | None).
    *   `VoiceStreamEventLifecycle` (dataclass): `type='voice_stream_event_lifecycle'`, `event` (Literal["turn_started", "turn_ended", "session_ended"]).
    *   `VoiceStreamEventError` (dataclass): `type='voice_stream_event_error'`, `error` (Exception).

*   **Exceptions (`exceptions.py`)**
    *   `STTWebsocketConnectionError(AgentsException)`.

*   **Model (`model.py`)**
    *   `TTSModelSettings` (dataclass): Settings for TTS. Attributes: `voice` (Literal[...] | None), `buffer_size` (int, default=120), `dtype` (DTypeLike, default=int16), `transform_data` (Callable | None), `instructions` (str), `text_splitter` (Callable), `speed` (float | None).
    *   `TTSModel` (ABC): Base for TTS models. Abstract property `model_name`. Abstract method `run(text: str, settings: TTSModelSettings) -> AsyncIterator[bytes]`.
    *   `StreamedTranscriptionSession` (ABC): Base for streaming STT session. Abstract method `transcribe_turns() -> AsyncIterator[str]`. Abstract async method `close()`.
    *   `STTModelSettings` (dataclass): Settings for STT. Attributes: `prompt` (str | None), `language` (str | None), `temperature` (float | None), `turn_detection` (dict[str, Any] | None).
    *   `STTModel` (ABC): Base for STT models. Abstract property `model_name`. Abstract async methods `transcribe(...) -> str` and `create_session(...) -> StreamedTranscriptionSession`.
    *   `VoiceModelProvider` (ABC): Base for voice model providers. Abstract methods `get_stt_model(...) -> STTModel` and `get_tts_model(...) -> TTSModel`.

*   **Utils (`utils.py`)**
    *   `get_sentence_based_splitter(min_sentence_length: int = 20) -> Callable[[str], tuple[str, str]]`: Returns function to split text by sentences.

*   **OpenAI Models (`models/openai_*`)**
    *   `OpenAIVoiceModelProvider(VoiceModelProvider)`: OpenAI implementation. `__init__` takes optional `api_key`, `base_url`, `openai_client`, `organization`, `project`.
    *   `OpenAISTTModel(STTModel)`: OpenAI STT implementation. `__init__` takes `model` name, `openai_client`.
    *   `OpenAITTSModel(TTSModel)`: OpenAI TTS implementation. `__init__` takes `model` name, `openai_client`.

### Extensions (`agents.extensions.*` modules - Detailed)

*   **Handoff Filters (`handoff_filters.py`)**
    *   `remove_all_tools(handoff_input_data: HandoffInputData) -> HandoffInputData`: Filters out tool call items (`FileSearchTool`, `WebSearchTool`, `FunctionTool` calls+output).

*   **Handoff Prompt (`handoff_prompt.py`)**
    *   `RECOMMENDED_PROMPT_PREFIX` (str): Constant string prefix with context about the Agents SDK multi-agent system and handoffs.
    *   `prompt_with_handoff_instructions(prompt: str) -> str`: Prepends `RECOMMENDED_PROMPT_PREFIX` to the given prompt string.

*   **Visualization (`visualization.py`)**
    *   `draw_graph(agent: Agent, filename: str | None = None)`: Generates Graphviz graph object. Requires `openai-agents[viz]` install group.



```markdown
# OpenAI Agents SDK

*(... Content from the previous responses up to here ...)*

---

## Related OpenAI Concepts and Guides (Preserved from Source)

*(This section includes the broader context guides provided in the original text, maintaining their content as presented.)*

### General Agent Concepts

*   **Overview:** Agents represent systems that intelligently accomplish tasks, ranging from executing simple workflows to pursuing complex, open-ended objectives. OpenAI provides a rich set of composable primitives that enable you to build agents. This guide walks through those primitives, and how they come together to form a robust agentic platform. Building agents involves assembling components across several domainssuch as models, tools, knowledge and memory, audio and speech, guardrails, and orchestrationand OpenAI provides composable primitives for each.
    *   **Domain** | **Description** | **OpenAI Primitives**
        ---|---|---
        Models | Core intelligence capable of reasoning, making decisions, and processing different modalities. | o1, o3-mini, GPT-4.5, GPT-4o, GPT-4o-mini
        Tools | Interface to the world, interact with environment, function calling, built-in tools, etc. | Function calling, Web search, File search, Computer use
        Knowledge and memory | Augment agents with external and persistent knowledge. | Vector stores, File search, Embeddings
        Audio and speech | Create agents that can understand audio and respond back in natural language. | Audio generation, realtime, Audio agents
        Guardrails | Prevent irrelevant, harmful, or undesirable behavior. | Moderation, Instruction hierarchy
        Orchestration | Develop, deploy, monitor, and improve agents. | Agents SDK, Tracing, Evaluations, Fine-tuning
        Voice agents | Create agents that can understand audio and respond back in natural language. | Realtime API, Voice support in the Agents SDK
*   **Models for Agents:**
    *   **Model** | **Agentic Strengths**
        ---|---
        o1 and o3-mini | Best for long-term planning, hard tasks, and reasoning.
        GPT-4.5 | Best for agentic execution.
        GPT-4o | Good balance of agentic capability and latency.
        GPT-4o-mini | Best for low-latency.
    *   Large language models (LLMs) are at the core of many agentic systems, responsible for making decisions and interacting with the world. OpenAIs models support a wide range of capabilities: High intelligence, Tools, Multimodality, Low-latency. For detailed model comparisons, visit the [models page](https://platform.openai.com/docs/models).
*   **Tools:** Tools enable agents to interact with the world. OpenAI supports function calling to connect with your code, and built-in tools for common tasks like web searches and data retrieval.
    *   **Tool** | **Description**
        ---|---
        Function calling | Interact with developer-defined code.
        Web search | Fetch up-to-date information from the web.
        File search | Perform semantic search across your documents.
        Computer use | Understand and control a computer or browser.
*   **Knowledge and Memory:** Knowledge and memory help agents store, retrieve, and utilize information beyond their initial training data. Vector stores enable agents to search your documents semantically and retrieve relevant information at runtime. Meanwhile, embeddings represent data efficiently for quick retrieval, powering dynamic knowledge solutions and long-term agent memory. You can integrate your data using OpenAIs vector stores and Embeddings API.
*   **Guardrails:** Guardrails ensure your agents behave safely, consistently, and within your intended boundariescritical for production deployments. Use OpenAIs free Moderation API to automatically filter unsafe content. Further control your agents behavior by leveraging the instruction hierarchy, which prioritizes developer-defined prompts and mitigates unwanted agent behaviors.
*   **Orchestration:** Building agents is a process. OpenAI provides tools to effectively build, deploy, monitor, evaluate, and improve agentic systems.
    *   **Phase** | **Description** | **OpenAI Primitives**
        ---|---|---
        Build and deploy | Rapidly build agents, enforce guardrails, and handle conversational flows using the Agents SDK. | Agents SDK
        Monitor | Observe agent behavior in real-time, debug issues, and gain insights through tracing. | Tracing
        Evaluate and improve | Measure agent performance, identify areas for improvement, and refine your agents. | Evaluations, Fine-tuning
*   **Get started:** Get started by installing the OpenAI Agents SDK for Python via: `pip install openai-agents`. Explore the repository and documentation for more details.
    *(Was this page useful?)*

### Voice Agents (General Guide)

Learn how to build voice agents that can understand audio and respond back in natural language.
*(Copy page)*

Use the OpenAI API and Agents SDK to create powerful, context-aware voice agents for applications like customer support and language tutoring. This guide helps you design and build a voice agent.

*   **Choose the right architecture:** OpenAI provides two primary architectures for building voice agents:
    1.  **Speech-to-speech (multimodal):** The multimodal speech-to-speech (S2S) architecture directly processes audio inputs and outputs, handling speech in real time in a single multimodal model, `gpt-4o-realtime-preview`. The model thinks and responds in speech. It doesn't rely on a transcript of the user's inputit hears emotion and intent, filters out noise, and responds directly in speech. Use this approach for highly interactive, low-latency, conversational use cases.
        *   **Strengths:** Low latency interactions, Rich multimodal understanding (audio and text simultaneously), Natural, fluid conversational flow, Enhanced user experience through vocal context understanding.
        *   **Best for:** Interactive and unstructured conversations, Language tutoring and interactive learning experiences, Conversational search and discovery, Interactive customer service scenarios.
    2.  **Chained (speech-to-text  LLM  text-to-speech):** A chained architecture processes audio sequentially, converting audio to text, generating intelligent responses using large language models (LLMs), and synthesizing audio from text. We recommend this predictable architecture if you're new to building voice agents. Both the user input and model's response are in text, so you have a transcript and can control what happens in your application. It's also a reliable way to convert an existing LLM-based application into a voice agent. (Example chain: `gpt-4o-transcribe`  `gpt-4o`  `gpt-4o-mini-tts`)
        *   **Strengths:** High control and transparency, Robust function calling and structured interactions, Reliable, predictable responses, Support for extended conversational context.
        *   **Best for:** Structured workflows focused on specific user objectives, Customer support, Sales and inbound triage, Scenarios that involve transcripts and scripted responses.
*   **Build a voice agent:** Use OpenAI's APIs and SDKs to create powerful, context-aware voice agents.
    *   **Use a speech-to-speech architecture for realtime processing:** Building a speech-to-speech voice agent requires:
        1.  Establishing a connection for realtime data transfer
        2.  Creating a realtime session with the Realtime API
        3.  Using an OpenAI model with realtime audio input and output capabilities
        To get started, read the [Realtime API guide](/docs/guides/realtime) and the [Realtime API reference](/docs/api-reference/realtime-sessions). Compatible models include `gpt-4o-realtime-preview` and `gpt-4o-mini-realtime-preview`.
    *   **Chain together audio input  text processing  audio output:** The Agents SDK supports extending your existing agents with voice capabilities. Get started by installing the OpenAI Agents SDK for Python with voice support:
        ```bash
        pip install openai-agents[voice]
        ```
        See the [Agents SDK voice agents quickstart](https://openai.github.io/openai-agents-python/voice/quickstart/) in GitHub to follow a complete example. In the example, you'll: Run a speech-to-text model to turn audio into text; Run your code, which is usually an agentic workflow, to produce a result; Run a text-to-speech model to turn the result text back into audio.

### Developer Quickstart (General OpenAI API)

Take your first steps with the OpenAI API.
*(Copy page)*

The OpenAI API provides a simple interface to state-of-the-art AI models for text generation, natural language processing, computer vision, and more. This example generates text output from a prompt, as you might using ChatGPT.

*   **Generate text from a model:**
    ```javascript
    // javascript
    // 1
    // 2
    // 3
    // 4
    // 5
    // 6
    // 7
    // 8
    // 9
    import OpenAI from "openai";
    const client = new OpenAI();

    const response = await client.responses.create({
        model: "gpt-4.1",
        input: "Write a one-sentence bedtime story about a unicorn."
    });

    console.log(response.output_text);
    ```
    *Data retention for model responses*
    *Configure your development environment: Install and configure an official OpenAI SDK to run the code above.*
    *Responses starter app: Start building with the Responses API*
    *Text generation and prompting: Learn more about prompting, message roles, and building conversational apps.*

*   **Analyze image inputs:** You can provide image inputs to the model as well. Scan receipts, analyze screenshots, or find objects in the real world with computer vision.
    *   Analyze the content of an image
        ```javascript
        // javascript
        // 1
        // 2
        // 3
        // 4
        // 5
        // 6
        // 7
        // 8
        // 9
        // 10
        // 11
        // 12
        // 13
        // 14
        // 15
        // 16
        // 17
        // 18
        // 19
        // 20
        import OpenAI from "openai";
        const client = new OpenAI();

        const response = await client.responses.create({
            model: "gpt-4.1",
            input: [
                { role: "user", content: "What two teams are playing in this photo?" },
                {
                    role: "user",
                    content: [
                        {
                            type: "input_image",
                            image_url: "https://upload.wikimedia.org/wikipedia/commons/3/3b/LeBron_James_Layup_%28Cleveland_vs_Brooklyn_2018%29.jpg",
                        }
                    ],
                },
            ],
        });

        console.log(response.output_text);
        ```
    *   *Computer vision guide: Learn to use image inputs to the model and extract meaning from images.*

*   **Extend the model with tools:** Give the model access to new data and capabilities using tools. You can either call your own custom code, or use one of OpenAI's powerful built-in tools. This example uses web search to give the model access to the latest information on the Internet.
    *   Get information for the response from the Internet
        ```javascript
        // javascript
        // 1
        // 2
        // 3
        // 4
        // 5
        // 6
        // 7
        // 8
        // 9
        // 10
        import OpenAI from "openai";
        const client = new OpenAI();

        const response = await client.responses.create({
            model: "gpt-4.1",
            tools: [ { type: "web_search_preview" } ],
            input: "What was a positive news story from today?",
        });

        console.log(response.output_text);
        ```
    *   *Use built-in tools: Learn about powerful built-in tools like web search and file search.*
    *   *Function calling guide: Learn to enable the model to call your own custom code.*

*   **Deliver blazing fast AI experiences:** Using either the new Realtime API or server-sent streaming events, you can build high performance, low-latency experiences for your users.
    *   Stream server-sent events from the API
        ```javascript
        // javascript
        // 1
        // 2
        // 3
        // 4
        // 5
        // 6
        // 7
        // 8
        // 9
        // 10
        // 11
        // 12
        // 13
        // 14
        // 15
        // 16
        // 17
        import { OpenAI } from "openai";
        const client = new OpenAI();

        const stream = await client.responses.create({
            model: "gpt-4.1",
            input: [
                {
                    role: "user",
                    content: "Say 'double bubble bath' ten times fast.",
                },
            ],
            stream: true,
        });

        for await (const event of stream) {
            console.log(event);
        }
        ```
    *   *Use streaming events: Use server-sent events to stream model responses to users fast.*
    *   *Get started with the Realtime API: Use WebRTC or WebSockets for super fast speech-to-speech AI apps.*

*   **Build agents:** Use the OpenAI platform to build agents capable of taking actionlike controlling computerson behalf of your users. Use the Agent SDK for Python to create orchestration logic on the backend.
    ```python
    # 1
    # 2
    # 3
    # 4
    # 5
    # 6
    # 7
    # 8
    # 9
    # 10
    # 11
    # 12
    # 13
    # 14
    # 15
    # 16
    # 17
    # 18
    # 19
    # 20
    # 21
    # 22
    # 23
    # 24
    # 25
    # 26
    # 27
    # 28
    # 29
    from agents import Agent, Runner
    import asyncio

    spanish_agent = Agent(
        name="Spanish agent",
        instructions="You only speak Spanish.",
    )

    english_agent = Agent(
        name="English agent",
        instructions="You only speak English",
    )

    triage_agent = Agent(
        name="Triage agent",
        instructions="Handoff to the appropriate agent based on the language of the request.",
        handoffs=[spanish_agent, english_agent],
    )

    async def main():
        result = await Runner.run(triage_agent, input="Hola, cmo ests?")
        print(result.final_output)

    if __name__ == "__main__":
        asyncio.run(main())

    # Output: Hola! Estoy bien, gracias por preguntar. Y t, cmo ests?
    ```
    *   *Build agents that can take action: Learn how to use the OpenAI platform to build powerful, capable AI agents.*

*   **Explore further:** We've barely scratched the surface of what's possible with the OpenAI platform. Here are some resources you might want to explore next.
    *   Go deeper with prompting and text generation
    *   Analyze the content of images
    *   Generate structured JSON data from the model
    *   Call custom code to help generate a response
    *   Search the web or use your own data in responses
    *   Responses starter app
    *   Build agents
    *   Full API Reference

### Text Generation and Prompting (General Guide)
=============================

Learn how to prompt a model to generate text.

With the OpenAI API, you can use a [large language model](/docs/models) to generate text from a prompt, as you might using [ChatGPT](https://chatgpt.com). Models can generate almost any kind of text responselike code, mathematical equations, structured JSON data, or human-like prose.

Here's a simple example using the [Responses API](/docs/api-reference/responses).

Generate text from a simple prompt

```javascript
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
    model: "gpt-4.1",
    input: "Write a one-sentence bedtime story about a unicorn."
});

console.log(response.output_text);
```

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4.1",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)
```

```bash
curl "https://api.openai.com/v1/responses" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "model": "gpt-4.1",
        "input": "Write a one-sentence bedtime story about a unicorn."
    }'
```

An array of content generated by the model is in the `output` property of the response. In this simple example, we have just one output which looks like this:

```json
[
    {
        "id": "msg_67b73f697ba4819183a15cc17d011509",
        "type": "message",
        "role": "assistant",
        "content": [
            {
                "type": "output_text",
                "text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
                "annotations": []
            }
        ]
    }
]
```

**The `output` array often has more than one item in it!** It can contain tool calls, data about reasoning tokens generated by [reasoning models](/docs/guides/reasoning), and other items. It is not safe to assume that the model's text output is present at `output[0].content[0].text`.

Some of our [official SDKs](/docs/libraries) include an `output_text` property on model responses for convenience, which aggregates all text outputs from the model into a single string. This may be useful as a shortcut to access text output from the model.

In addition to plain text, you can also have the model return structured data in JSON format - this feature is called [**Structured Outputs**](/docs/guides/structured-outputs).

*   **Choosing a model**
    ------------------
    A key choice to make when generating content through the API is which model you want to use - the `model` parameter of the code samples above. [You can find a full listing of available models here](/docs/models). Here are a few factors to consider when choosing a model for text generation.
    *   **[Reasoning models](/docs/guides/reasoning)** generate an internal chain of thought to analyze the input prompt, and excel at understanding complex tasks and multi-step planning. They are also generally slower and more expensive to use than GPT models.
    *   **GPT models** are fast, cost-efficient, and highly intelligent, but benefit from more explicit instructions around how to accomplish tasks.
    *   **Large and small (mini or nano) models** offer trade-offs for speed, cost, and intelligence. Large models are more effective at understanding prompts and solving problems across domains, while small models are generally faster and cheaper to use.
    When in doubt, [`gpt-4.1`](/docs/models/gpt-4.1) offers a solid combination of intelligence, speed, and cost effectiveness.

*   **Prompt engineering**
    ------------------
    **Prompt engineering** is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.
    Because the content generated from a model is non-deterministic, it is a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices you can apply to consistently get good results from a model.
    Some prompt engineering techniques will work with every model, like using message roles. But different model types (like reasoning versus GPT models) might need to be prompted differently to produce the best results. Even different snapshots of models within the same family could produce different results. So as you are building more complex applications, we strongly recommend that you:
    *   Pin your production applications to specific [model snapshots](/docs/models) (like `gpt-4.1-2025-04-14` for example) to ensure consistent behavior.
    *   Build [evals](/docs/guides/evals) that will measure the behavior of your prompts, so that you can monitor the performance of your prompts as you iterate on them, or when you change and upgrade model versions.
    Now, let's examine some tools and techniques available to you to construct prompts.

*   **Message roles and instruction following**
    ---------------------------------------
    You can provide instructions to the model with [differing levels of authority](https://model-spec.openai.com/2025-02-12.html#chain_of_command) using the `instructions` API parameter or **message roles**.
    The `instructions` parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. Any instructions provided this way will take priority over a prompt in the `input` parameter.

    Generate text with instructions
    ```javascript
    // ... (JS code omitted for brevity, same as provided earlier) ...
    ```
    ```python
    # ... (Python code omitted for brevity, same as provided earlier) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, same as provided earlier) ...
    ```
    The example above is roughly equivalent to using the following input messages in the `input` array:

    Generate text with messages using different roles
    ```javascript
    // ... (JS code omitted for brevity, same as provided earlier) ...
    ```
    ```python
    # ... (Python code omitted for brevity, same as provided earlier) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, same as provided earlier) ...
    ```
    Note that the `instructions` parameter only applies to the current response generation request. If you are [managing conversation state](/docs/guides/conversation-state) with the `previous_response_id` parameter, the `instructions` used on previous turns will not be present in the context.
    The [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command) describes how our models give different levels of priority to messages with different roles.
    | Role        | Description                                                                                         |
    |-------------|-----------------------------------------------------------------------------------------------------|
    | `developer` | developer messages are instructions provided by the application developer, prioritized ahead of user messages. |
    | `user`      | user messages are instructions provided by an end user, prioritized behind developer messages.       |
    | `assistant` | Messages generated by the model have the assistant role.                                            |
    A multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about [managing conversation state here](/docs/guides/conversation-state).
    You could think about `developer` and `user` messages like a function and its arguments in a programming language.
    *   `developer` messages provide the system's rules and business logic, like a function definition.
    *   `user` messages provide inputs and configuration to which the `developer` message instructions are applied, like arguments to a function.

*   **Message formatting with Markdown and XML**
    ----------------------------------------
    When writing `developer` and `user` messages, you can help the model understand logical boundaries of your prompt and context data using a combination of [Markdown](https://commonmark.org/help/) formatting and [XML tags](https://www.w3.org/TR/xml/).
    Markdown headers and lists can be helpful to mark distinct sections of a prompt, and to communicate hierarchy to the model. They can also potentially make your prompts more readable during development. XML tags can help delineate where one piece of content (like a supporting document used for reference) begins and ends. XML attributes can also be used to define metadata about content in the prompt that can be referenced by your instructions.
    In general, a developer message will contain the following sections, usually in this order (though the exact optimal content and order may vary by which model you are using):
    *   **Identity:** Describe the purpose, communication style, and high-level goals of the assistant.
    *   **Instructions:** Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should [call custom functions](/docs/guides/function-calling).
    *   **Examples:** Provide examples of possible inputs, along with the desired output from the model.
    *   **Context:** Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.
    Below is an example of using Markdown and XML tags to construct a `developer` message with distinct sections and supporting examples.

    Example prompt: A developer message for code generation
    ```text
    # Identity

    You are coding assistant that helps enforce the use of snake case
    variables in JavaScript code, and writing code that will run in
    Internet Explorer version 6.

    # Instructions

    * When defining variables, use snake case names (e.g. my_variable)
      instead of camel case names (e.g. myVariable).
    * To support old browsers, declare variables using the older
      "var" keyword.
    * Do not give responses with Markdown formatting, just return
      the code as requested.

    # Examples

    <user_query>
    How do I declare a string variable for a first name?
    </user_query>

    <assistant_response>
    var first_name = "Anna";
    </assistant_response>
    ```
    API request: Send a prompt to generate code through the API
    ```javascript
    // ... (JS code omitted for brevity, requires reading prompt.txt) ...
    ```
    ```python
    # ... (Python code omitted for brevity, requires reading prompt.txt) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, requires reading prompt.txt) ...
    ```
    *   **Save on cost and latency with prompt caching:** When constructing a message, you should try and keep content that you expect to use over and over in your API requests at the beginning of your prompt, **and** among the first API parameters you pass in the JSON request body to [Chat Completions](/docs/api-reference/chat) or [Responses](/docs/api-reference/responses). This enables you to maximize cost and latency savings from [prompt caching](/docs/guides/prompt-caching).

*   **Few-shot learning**
    -----------------
    Few-shot learning lets you steer a large language model toward a new task by including a handful of input/output examples in the prompt, rather than [fine-tuning](/docs/guides/fine-tuning) the model. The model implicitly "picks up" the pattern from those examples and applies it to a prompt. When providing examples, try to show a diverse range of possible inputs with the desired outputs.
    Typically, you will provide examples as part of a `developer` message in your API request. Here's an example `developer` message containing examples that show a model how to classify positive or negative customer service reviews.
    ```text
    # Identity

    You are a helpful assistant that labels short product reviews as
    Positive, Negative, or Neutral.

    # Instructions

    * Only output a single word in your response with no additional formatting
      or commentary.
    * Your response should only be one of the words "Positive", "Negative", or
      "Neutral" depending on the sentiment of the product review you are given.

    # Examples

    <product_review id="example-1">
    I absolutely love this headphones  sound quality is amazing!
    </product_review>

    <assistant_response id="example-1">
    Positive
    </assistant_response>

    <product_review id="example-2">
    Battery life is okay, but the ear pads feel cheap.
    </product_review>

    <assistant_response id="example-2">
    Neutral
    </assistant_response>

    <product_review id="example-3">
    Terrible customer service, I'll never buy from them again.
    </product_review>

    <assistant_response id="example-3">
    Negative
    </assistant_response>
    ```

*   **Include relevant context information**
    ------------------------------------
    It is often useful to include additional context information the model can use to generate a response within the prompt you give the model. There are a few common reasons why you might do this:
    *   To give the model access to proprietary data, or any other data outside the data set the model was trained on.
    *   To constrain the model's response to a specific set of resources that you have determined will be most beneficial.
    The technique of adding additional relevant context to the model generation request is sometimes called **retrieval-augmented generation (RAG)**. You can add additional context to the prompt in many different ways, from querying a vector database and including the text you get back into a prompt, or by using OpenAI's built-in [file search tool](/docs/guides/tools-file-search) to generate content based on uploaded documents.
    *   **Planning for the context window:** Models can only handle so much data within the context they consider during a generation request. This memory limit is called a **context window**, which is defined in terms of [tokens](https://blogs.nvidia.com/blog/ai-tokens-explained) (chunks of data you pass in, from text to images). Models have different context window sizes from the low 100k range up to one million tokens for newer GPT-4.1 models. [Refer to the model docs](/docs/models) for specific context window sizes per model.

*   **Prompting GPT-4.1 models**
    ------------------------
    GPT models like [`gpt-4.1`](/docs/models/gpt-4.1) benefit from precise instructions that explicitly provide the logic and data required to complete the task in the prompt. GPT-4.1 in particular is highly steerable and responsive to well-specified prompts. To get the most out of GPT-4.1, refer to the prompting guide in the cookbook.
    [Link: GPT-4.1 prompting guide (Cookbook)](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)
    *   **GPT-4.1 prompting best practices:** While the [cookbook](https://cookbook.openai.com/examples/gpt4-1_prompting_guide) has the best and most comprehensive guidance for prompting this model, here are a few best practices to keep in mind.
        *   **Building agentic workflows - System Prompt Reminders:** We recommend including three key types of reminders in all agent prompts for persistence, tool calling, and planning. These transform behavior from chatbot-like to a more eager agent. Examples:
            ```text
            ## PERSISTENCE
            You are an agent - please keep going until the user's query is completely
            resolved, before ending your turn and yielding back to the user. Only
            terminate your turn when you are sure that the problem is solved.

            ## TOOL CALLING
            If you are not sure about file content or codebase structure pertaining to
            the user's request, use your tools to read files and gather the relevant
            information: do NOT guess or make up an answer.

            ## PLANNING
            You MUST plan extensively before each function call, and reflect
            extensively on the outcomes of the previous function calls. DO NOT do this
            entire process by making function calls only, as this can impair your
            ability to solve the problem and think insightfully.
            ```
        *   **Tool Calls:** GPT-4.1 is better trained on tools passed via the API's `tools` field. Use this exclusively rather than injecting descriptions into the system prompt.
        *   **Diff Generation:** Significantly improved performance. See cookbook for recommended diff format. Should generalize to any well-specified format.
        *   **Using long context:** GPT-4.1 has a 1M token input context window. Useful for structured document parsing, re-ranking, selecting relevant info, multi-hop reasoning.
            *   **Optimal Context Size:** Perfect performance on needle-in-a-haystack up to full context size. Strong performance observed on complex tasks up to hundreds of thousands of tokens.
            *   **Delimiters:** XML and format from Lee et al. tend to perform well. JSON performed worse. See cookbook for examples.
            *   **Prompt Organization:** For long context, place critical instructions (including user query) at both the top and bottom of the prompt for best performance.
        *   **Prompting for chain of thought:** GPT-4.1 is trained for agentic reasoning. Start with a basic CoT instruction like:
            ```text
            First, think carefully step by step about what documents are needed to answer the query. Then, print out the TITLE and ID of each document. Then, format the IDs into a list.
            ```
            Improve based on failures in evals. See cookbook for more opinionated reasoning strategy example.
        *   **Instruction following:** GPT-4.1 follows instructions more literally. May need more explicit specification. Existing prompts might need adjustment.
            *   **Recommended Workflow:** Start with "Response Rules"/"Instructions" section. Add specific sections (e.g., `## Sample Phrases`) if needed. Use ordered lists for steps. Check for conflicting/underspecified instructions (later instructions tend to be followed). Add examples demonstrating desired behavior, cite important behaviors in rules. All-caps/incentives usually not necessary.
            *   **Common Failure Modes:** Instructing absolute behavior ("must call tool") can cause hallucinations; add fallback instructions ("if you don't have enough info, ask user"). Sample phrases can become repetitive; instruct model to vary them. Models might add extra prose/formatting; instruct against it if needed. See cookbook customer service prompt example.

*   **Prompting reasoning models**
    --------------------------
    Differences vs. GPT models: Reasoning models generally provide better results with high-level guidance, while GPT models benefit from precise instructions.
    *   Reasoning model  senior co-worker (give goal, trust details).
    *   GPT model  junior co-worker (needs explicit instructions).
    Refer to [reasoning best practices guide](/docs/guides/reasoning-best-practices).

*   **Next steps**
    ----------
    *   [Build a prompt in the Playground](/playground)
    *   [Generate JSON data with Structured Outputs](/docs/guides/structured-outputs)
    *   [Full API reference](/docs/api-reference/responses)

    *(Was this page useful?)*

### Images and Vision (General Guide)
=================

Learn how to use vision capabilities to understand images.

**Vision** is the ability to use images as input prompts to a model, and generate responses based on the data inside those images. Find out which models are capable of vision [on the models page](/docs/models). To generate images as _output_, see our [specialized model for image generation](/docs/guides/image-generation).

You can provide images as input to generation requests either by providing a fully qualified URL to an image file, or providing an image as a Base64-encoded data URL.

*   **Passing a URL:** Analyze the content of an image
    ```javascript
    // ... (JS code omitted for brevity, same as provided earlier) ...
    ```
    ```python
    # ... (Python code omitted for brevity, same as provided earlier) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, same as provided earlier) ...
    ```

*   **Passing a Base64 encoded image:** Analyze the content of an image
    ```javascript
    // ... (JS code omitted for brevity, requires reading image file) ...
    ```
    ```python
    # ... (Python code omitted for brevity, requires reading image file) ...
    ```
    *(Note: Bash example for Base64 was not provided in the original text)*

*   **Image input requirements**
    ------------------------
    Input images must meet the following requirements to be used in the API.
    | Category        | Requirement                                                                                                                                | Details/Restrictions                                                                      |
    |-----------------|--------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
    | Format          | PNG (.png), JPEG (.jpeg and .jpg), WEBP (.webp), Non-animated GIF (.gif)                                                                     |                                                                                           |
    | Size            | Up to 20MB per image, Up to 500 individual images per request, Up to 50 MB image bytes per request                                             |                                                                                           |
    | Resolution      | Low-resolution: 512px x 512px, High-resolution: 768px (short side) x 2000px (long side)                                                    |                                                                                           |
    | Content         | No watermarks or logos, No text, No NSFW content, Clear enough for a human to understand                                                     |                                                                                           |

*   **Specify image input detail level**
    --------------------------------
    The `detail` parameter tells the model what level of detail to use when processing and understanding the image (`low`, `high`, or `auto` to let the model decide). If you skip the parameter, the model will use `auto`. Put it right after your `image_url`, like this:
    ```json
    {
        "type": "input_image",
        "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
        "detail": "high"
    }
    ```
    You can save tokens and speed up responses by using `"detail": "low"`. This lets the model process the image with a budget of 85 tokens. The model receives a low-resolution 512px x 512px version of the image. This is fine if your use case doesn't require the model to see with high-resolution detail.
    Or give the model more detail by using `"detail": "high"`. This lets the model see the low-resolution image (using 85 tokens) and then creates detailed crops using 170 tokens for each 512px x 512px tile.
    Note that the above token budgets for image processing do not currently apply to the GPT-4o mini model, but the image processing cost is comparable to GPT-4o. For the most precise and up-to-date estimates for image processing, please use the image pricing calculator [here](https://openai.com/api/pricing/).

*   **Provide multiple image inputs**
    -----------------------------
    The [Responses API](https://platform.openai.com/docs/api-reference/responses) can take in and process multiple image inputs. The model processes each image and uses information from all images to answer the question.

    Multiple image inputs example:
    ```javascript
    // ... (JS code omitted for brevity, same as provided earlier) ...
    ```
    ```python
    # ... (Python code omitted for brevity, same as provided earlier) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, same as provided earlier) ...
    ```
    Here, the model is shown two copies of the same image. It can answer questions about both images or each image independently.

*   **Limitations**
    -----------
    While models with vision capabilities are powerful and can be used in many situations, it's important to understand the limitations of these models. Here are some known limitations:
    *   **Medical images**: Not suitable for interpreting specialized images like CT scans; shouldn't be used for medical advice.
    *   **Non-English**: May not perform optimally with images containing non-Latin alphabet text (e.g., Japanese, Korean).
    *   **Small text**: Enlarge text within the image, but avoid cropping important details.
    *   **Rotation**: May misinterpret rotated/upside-down text and images.
    *   **Visual elements**: May struggle with graphs or text where colors/styles (solid, dashed, dotted lines) vary.
    *   **Spatial reasoning**: Struggles with tasks requiring precise spatial localization (e.g., chess positions).
    *   **Accuracy**: May generate incorrect descriptions or captions.
    *   **Image shape**: Struggles with panoramic and fisheye images.
    *   **Metadata and resizing**: Doesn't process original filenames or metadata; images resized before analysis, affecting dimensions.
    *   **Counting**: May give approximate counts for objects.
    *   **CAPTCHAS**: System blocks submission of CAPTCHAs for safety.

*   **Calculating costs**
    -----------------
    Image inputs are metered and charged in tokens, just as text inputs are. How images are converted to text token inputs varies based on the model.
    *   **GPT-4.1:** Token cost based on number of 32x32 patches needed (max 1536). Image scaled down if needed. `gpt-4.1-mini` multiplies image tokens by 1.62, `gpt-4.1-nano` by 2.46 for billing.
        *   Example 1024x1024: 1024 tokens.
        *   Example 1800x2400: Scaled down, requires 1452 tokens.
    *   **GPT 4o and o-series:** Cost depends on size and `detail`.
        *   `"detail": "low"`: Fixed 85 tokens.
        *   `"detail": "high"`: Scale to fit 2048x2048 -> scale shortest side to 768px -> count 512px square tiles (170 tokens each) + add 85 tokens.
            *   Example 1024x1024: 765 tokens.
            *   Example 2048x4096: 1105 tokens.
    We process images at the token level, so each image counts towards your tokens per minute (TPM) limit.

    *(Was this page useful?)*

### Audio and Speech (General Guide)
================

Explore audio and speech features in the OpenAI API.

The OpenAI API provides a range of audio capabilities. If you know what you want to build, find your use case below to get started. If you're not sure where to start, read this page as an overview.

*   **Build with audio**
    ----------------
    *   [Build voice agents](/docs/guides/voice-agents) (Image link provided)
    *   [Transcribe audio](/docs/guides/speech-to-text) (Image link provided)
    *   [Speak text](/docs/guides/text-to-speech) (Image link provided)

*   **A tour of audio use cases**
    -------------------------
    LLMs can process audio by using sound as input, creating sound as output, or both. OpenAI has several API endpoints that help you build audio applications or voice agents.
    *   **Voice agents:** Understand audio, handle tasks, respond naturally. Two approaches: speech-to-speech models (Realtime API) or chained STT->LLM->TTS (Agents SDK supported). S2S is lower latency/more natural, chained is reliable for extending text agents.
    *   **Streaming audio:** Realtime processing for low-latency apps (voice agents, transcription). Use [Realtime API](/docs/guides/realtime). Advanced speech models offer better accuracy, low latency, multilingual support.
    *   **Text to speech (TTS):** Use [Audio API](/docs/api-reference/audio/) `/audio/speech` endpoint. Models: `gpt-4o-mini-tts`, `tts-1`, `tts-1-hd`. `gpt-4o-mini-tts` allows prompting for style/tone.
    *   **Speech to text (STT):** Use [Audio API](/docs/api-reference/audio/) `/audio/transcriptions` endpoint. Models: `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, `whisper-1`. Streaming supported.

*   **Choosing the right API**
    ----------------------
    | API                     | Supported modalities        | Streaming support         |
    |-------------------------|-----------------------------|---------------------------|
    | Realtime API            | Audio and text inputs/outputs | Audio streaming in and out |
    | Chat Completions API    | Audio and text inputs/outputs | Audio streaming out       |
    | Transcription API       | Audio inputs                | Audio streaming out       |
    | Speech API              | Text inputs and audio outputs | Audio streaming out       |
    *   **General use vs. specialized:** Realtime and Chat Completions APIs use latest models, multimodal, function calling. Transcription/Speech APIs are specialized for one purpose/model.
    *   **Talking vs. controlling script:** Realtime/Chat Completions for natural, model-generated audio responses. Audio APIs (with LLM) for predictable, controlled TTS responses (higher latency).
    *   **Recommendations:** Realtime API for real-time interactions/transcription. Chat Completions for voice agents needing features like function calling (non-realtime). Specialized Audio APIs for single-purpose use cases.

*   **Add audio to your existing application**
    --------------------------------------
    Models like GPT-4o/GPT-4o mini are natively multimodal. If using [Chat Completions endpoint](/docs/api-reference/chat/), add audio capabilities by including `audio` in `modalities` array and using an audio model (e.g., `gpt-4o-audio-preview`). (Audio not yet supported in Responses API).

    Audio output from model: Create a human-like audio response
    ```javascript
    // ... (JS code omitted for brevity, uses gpt-4o-audio-preview) ...
    ```
    ```python
    # ... (Python code omitted for brevity, uses gpt-4o-audio-preview) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, uses gpt-4o-audio-preview) ...
    ```
    Audio input to model: Use audio inputs for prompting
    ```javascript
    // ... (JS code omitted for brevity, uses gpt-4o-audio-preview, fetches WAV) ...
    ```
    ```python
    # ... (Python code omitted for brevity, uses gpt-4o-audio-preview, fetches WAV) ...
    ```
    ```bash
    # ... (Bash code omitted for brevity, uses gpt-4o-audio-preview, needs base64 audio) ...
    ```

    *(Was this page useful?)*

### Structured Outputs (General Guide)
==================

Ensure responses adhere to a JSON schema.

*   **Try it out:** Playground or generate schema definition. *(Link provided)*
*   **Introduction:** Ensures model generates responses adhering to your JSON Schema. Benefits: Reliable type-safety, Explicit refusals, Simpler prompting.
*   **Getting a structured response:** Example using Responses API with `text: { format: { type: "json_schema", name: "...", schema: {...} } }`.
    ```javascript
    // ... (JS code omitted for brevity, extracts calendar event) ...
    ```
    ```python
    # ... (Python code omitted for brevity, extracts calendar event) ...
    ```
*   **Supported models:** Latest LLMs (`gpt-4o-2024-08-06`+, `gpt-4o-mini`+). Older models may use [JSON mode](#json-mode-1).
*   **When to use Structured Outputs via function calling vs via text.format:**
    *   Use function calling when bridging models and application functionality (query DB, interact with UI).
    *   Use `text.format` when structuring the model's response to the user (display parts differently in UI).
*   **Structured Outputs vs JSON mode:** Structured Outputs enforces schema adherence; JSON mode only ensures valid JSON. SO requires newer models. Prefer SO when possible.
    | Feature           | Structured Outputs                          | JSON Mode                         |
    |-------------------|---------------------------------------------|-----------------------------------|
    | Outputs valid JSON | Yes                                         | Yes                               |
    | Adheres to schema | Yes (see supported schemas)                 | No                                |
    | Compatible models | gpt-4o-mini, gpt-4o-2024-08-06, and later | gpt-3.5-turbo, gpt-4-*, gpt-4o-* |
    | Enabling          | `text: { format: { type: "json_schema", "strict": true, "schema": ... } }` | `text: { format: { type: "json_object" } }` |
*   **Examples:**
    *   Chain of thought (Math tutoring step-by-step). *(Code examples provided)*
    *   Structured data extraction (Research papers). *(Code examples provided)*
    *   UI generation (HTML structure). *(Code examples provided)*
    *   Moderation (Classifying inputs). *(Code examples provided)*
*   **How to use Structured Outputs with text.format:**
    1.  **Define your schema:** Design the JSON Schema. Tips: Clear names/descriptions, use evals. Check [supported schemas](#supported-schemas-1).
    2.  **Supply schema in API call:** Use `text: { format: { type: "json_schema", "strict": true, "schema":  } }`. Note: First request with a schema has higher latency (processing), subsequent ones are faster.
    3.  **Handle edge cases:** Check for `status: "incomplete"` (e.g., `reason: "max_output_tokens"`) or `refusal` type in output content. *(Code examples provided)*
*   **Refusals with Structured Outputs:** If model refuses for safety, API response includes `refusal` property in output content instead of schema-matching JSON. Detect and handle this. *(Python/JS SDK examples using `parse()` helper provided)*. Example refusal JSON response structure shown.
*   **Tips and best practices:**
    *   **Handling user-generated input:** Prompt model on how to handle inputs incompatible with the schema (e.g., return empty fields or specific message).
    *   **Handling mistakes:** If output matches schema but is logically incorrect, adjust instructions, provide examples, simplify tasks. Use [prompt engineering guide](/docs/guides/prompt-engineering).
*   **Streaming:** Supported. Use SDKs to handle streaming structured outputs. *(Code examples provided)*
*   **Supported schemas:** Subset of JSON Schema.
    *   Supported types: String, Number, Boolean, Integer, Object, Array, Enum, `anyOf`.
    *   Root object must not be `anyOf`. (Issue with Zod discriminated unions).
    *   All fields must be `required` (`strict: true`). Use `["type", "null"]` for optionality.
    *   Limits: Max 100 total properties, max 5 levels nesting.
    *   String length limits: Total length of keys, names, enums, consts <= 15,000 chars.
    *   Enum limits: Max 500 total enum values. Single enum string length limit (7500 chars if >250 values).
    *   `additionalProperties: false` must always be set in objects for strict mode.
    *   Key ordering in output matches schema order.
    *   Unsupported keywords (partial list): `minLength`, `maxLength`, `pattern`, `format`, `minimum`, `maximum`, `multipleOf`, `patternProperties`, `unevaluatedProperties`, `propertyNames`, `minProperties`, `maxProperties`, `unevaluatedItems`, `contains`, `minContains`, `maxContains`, `minItems`, `maxItems`, `uniqueItems`. Using unsupported keywords with `strict: true` causes error.
    *   `anyOf` supported if nested schemas are valid per subset.
    *   Definitions (`$defs`) supported.
    *   Recursive schemas (`$ref: "#"` or explicit defs) supported.
*   **JSON mode:** Legacy feature. Ensures valid JSON, but not schema adherence. Use `text: { format: { type: "json_object" } }`. Requires explicit instruction in prompt to output JSON. Must handle edge cases (max tokens, refusals, content filter) resulting in partial/invalid JSON. *(Code examples for handling edge cases provided)*.
*   **Resources:** Cookbooks for intro and multi-agent systems. *(Links provided)*

    *(Was this page useful?)*

```markdown
*(... Content from the previous responses up to here ...)*

### Function calling (General Guide)
================

Enable models to fetch data and take actions.

**Function calling** provides a powerful and flexible way for OpenAI models to interface with your code or external services. This guide will explain how to connect the models to your own custom code to fetch data or take action.

*   **Examples:**
    *   Get weather (Function calling example with `get_weather` function)
        ```python
        # ... (Python code omitted for brevity, defines get_weather tool, calls Responses API) ...
        # Output:
        # [{'type': 'function_call', 'id': 'fc_12345xyz', 'call_id': 'call_12345xyz', 'name': 'get_weather', 'arguments': '{"location":"Paris, France"}'}]
        ```
        ```javascript
        // ... (JS code omitted for brevity, defines get_weather tool, calls Responses API) ...
        // Output: Same as Python
        ```
        ```bash
        # ... (Bash code omitted for brevity, defines get_weather tool, calls Responses API) ...
        # Output: Same as Python
        ```
    *   Send email (Function calling example with `send_email` function)
        ```python
        # ... (Python code omitted for brevity, defines send_email tool, calls Responses API) ...
        # Output:
        # [{'type': 'function_call', 'id': 'fc_12345xyz', 'call_id': 'call_9876abc', 'name': 'send_email', 'arguments': '{"to":"ilan@example.com","subject":"Hello!","body":"Just wanted to say hi"}'},
        #  {'type': 'function_call', 'id': 'fc_12345xyz', 'call_id': 'call_9876abc', 'name': 'send_email', 'arguments': '{"to":"katia@example.com","subject":"Hello!","body":"Just wanted to say hi"}'}]
        ```
        ```javascript
        // ... (JS code omitted for brevity, defines send_email tool, calls Responses API) ...
        // Output: Same as Python
        ```
        ```bash
        # ... (Bash code omitted for brevity, defines send_email tool, calls Responses API) ...
        # Output: Same as Python
        ```
    *   Search knowledge base (Function calling example with `search_knowledge_base` function)
        ```python
        # ... (Python code omitted for brevity, defines search_knowledge_base tool with nested options, calls Responses API) ...
        # Output:
        # [{'type': 'function_call', 'id': 'fc_12345xyz', 'call_id': 'call_4567xyz', 'name': 'search_knowledge_base', 'arguments': '{"query":"What is ChatGPT?","options":{"num_results":3,"domain_filter":null,"sort_by":"relevance"}}'}]
        ```
        ```javascript
        // ... (JS code omitted for brevity, defines search_knowledge_base tool, calls Responses API) ...
        // Output: Same as Python
        ```
        ```bash
        # ... (Bash code omitted for brevity, defines search_knowledge_base tool, calls Responses API) ...
        # Output: Same as Python
        ```
    *Experiment with function calling and [generate function schemas](/docs/guides/prompt-generation) in the [Playground](/playground)!*

*   **Overview**
    --------
    You can give the model access to your own custom code through **function calling**. Based on the system prompt and messages, the model may decide to call these functions  **instead of (or in addition to) generating text or audio**.
    You'll then execute the function code, send back the results, and the model will incorporate them into its final response.
    *(Function Calling Diagram Steps image provided)*
    Function calling has two primary use cases:
    | Use Case        | Description                                                                                                                                       |
    |-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------|
    | Fetching Data   | Retrieve up-to-date information to incorporate into the model's response (RAG). Useful for searching knowledge bases and APIs (e.g., weather).      |
    | Taking Action   | Perform actions like submitting forms, calling APIs, modifying application state (UI/backend), or taking agentic workflow actions (e.g., handoffs). |

*   **Sample function:** Let's look at the steps to allow a model to use a real `get_weather` function defined below:
    Sample `get_weather` function implemented in your codebase
    ```python
    # ... (Python requests code for get_weather(latitude, longitude) provided) ...
    ```
    ```javascript
    // ... (JS fetch code for getWeather(latitude, longitude) provided) ...
    ```
    Unlike the diagram earlier, this function expects precise `latitude` and `longitude` instead of a general `location` parameter. (However, our models can automatically determine the coordinates for many locations!)

*   **Function calling steps:**
    1.  **Call model with [functions defined](#defining-functions-1)**  along with your system and user messages.
        Step 1: Call model with `get_weather` tool defined
        ```python
        # ... (Python code provided: defines tools list with get_weather schema, sets input_messages, calls client.responses.create) ...
        ```
        ```javascript
        // ... (JS code provided: defines tools list with get_weather schema, sets input, calls openai.responses.create) ...
        ```
    2.  **Model decides to call function(s)**  model returns the **name** and **input arguments**.
        `response.output`
        ```json
        [{
            "type": "function_call",
            "id": "fc_12345xyz",
            "call_id": "call_12345xyz",
            "name": "get_weather",
            "arguments": "{\"latitude\":48.8566,\"longitude\":2.3522}"
        }]
        ```
    3.  **Execute function code**  parse the model's response and [handle function calls](#handling-function-calls-1).
        Step 3: Execute `get_weather` function
        ```python
        # ... (Python code provided: extracts tool_call, parses args, calls get_weather) ...
        ```
        ```javascript
        // ... (JS code provided: extracts toolCall, parses args, calls getWeather) ...
        ```
    4.  **Supply model with results**  so it can incorporate them into its final response.
        Step 4: Supply result and call model again
        ```python
        # ... (Python code provided: appends tool_call and function_call_output message to input_messages, calls client.responses.create again) ...
        ```
        ```javascript
        // ... (JS code provided: pushes toolCall and function_call_output message to input, calls openai.responses.create again) ...
        ```
    5.  **Model responds**  incorporating the result in its output.
        `response_2.output_text`
        ```json
        "The current temperature in Paris is 14C (57.2F)."
        ```

*   **Defining functions**
    ------------------
    Functions can be set in the `tools` parameter of each API request.
    A function is defined by its schema, which informs the model what it does and what input arguments it expects. It comprises the following fields:
    | Field         | Description                                       |
    |---------------|---------------------------------------------------|
    | `type`        | This should always be `function`                  |
    | `name`        | The function's name (e.g. `get_weather`)          |
    | `description` | Details on when and how to use the function     |
    | `parameters`  | JSON schema defining the function's input arguments |
    | `strict`      | Whether to enforce strict mode for the function call |
    Take a look at this example or generate your own below (or in our [Playground](/playground)).
    *(Generate button link provided)*
    Example function schema:
    ```json
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Retrieves current weather for the given location.",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City and country e.g. Bogot, Colombia"
                    },
                    "units": {
                        "type": "string",
                        "enum": [ "celsius", "fahrenheit" ],
                        "description": "Units the temperature will be returned in."
                    }
                },
                "required": [ "location", "units" ],
                "additionalProperties": false
            },
            "strict": true
        }
    }
    ```
    Because the `parameters` are defined by a [JSON schema](https://json-schema.org/), you can leverage many of its rich features like property types, enums, descriptions, nested objects, and, recursive objects.

*   **Best practices for defining functions:**
    1.  **Write clear and detailed function names, parameter descriptions, and instructions.**
        *   Explicitly describe purpose of function/parameters/output.
        *   Use system prompt to describe when/when not to use each function. Tell model *exactly* what to do.
        *   Include examples/edge cases, especially to fix recurring failures. (Note: Examples may hurt performance for [reasoning models](/docs/guides/reasoning).)
    2.  **Apply software engineering best practices.**
        *   Make functions obvious/intuitive ([principle of least surprise](https://en.wikipedia.org/wiki/Principle_of_least_astonishment)).
        *   Use enums/object structure to make invalid states unrepresentable.
        *   Pass the intern test (can a human use it with only the provided info?).
    3.  **Offload the burden from the model and use code where possible.**
        *   Don't make model fill arguments you already know (pass known IDs via code).
        *   Combine functions always called sequentially.
    4.  **Keep the number of functions small for higher accuracy.**
        *   Evaluate performance with different numbers of functions.
        *   Aim for fewer than 20 functions (soft suggestion).
    5.  **Leverage OpenAI resources.**
        *   Generate/iterate on schemas in [Playground](/playground).
        *   Consider [fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) for accuracy with many functions/difficult tasks ([cookbook](https://cookbook.openai.com/examples/fine_tuning_for_function_calling)).

*   **Token Usage:** Under the hood, functions are injected into the system message. They count against context limit and are billed as input tokens. Limit function count or description length if hitting limits. [Fine-tuning](/docs/guides/fine-tuning#fine-tuning-examples) can reduce tokens used for function definitions.

*   **Handling function calls**
    -----------------------
    When the model calls a function, you must execute it and return the result. Since model responses can include zero, one, or multiple calls, it is best practice to assume there are several.
    The response `output` array contains an entry with the `type` having a value of `function_call`. Each entry with a `call_id` (used later to submit the function result), `name`, and JSON-encoded `arguments`.

    Sample response with multiple function calls:
    ```json
    // ... (JSON showing 3 function calls: 2 get_weather, 1 send_email) ...
    ```
    Execute function calls and append results:
    ```python
    # ... (Python code provided: iterates response.output, checks type, calls hypothetical call_function, appends result message to input_messages) ...
    ```
    ```javascript
    // ... (JS code provided: iterates response.output, checks type, calls hypothetical callFunction, pushes result message to input) ...
    ```
    Possible implementation of `call_function`:
    ```python
    # ... (Python code provided: defines call_function(name, args) with if statements for get_weather and send_email) ...
    ```
    ```javascript
    // ... (JS code provided: defines async callFunction(name, args) with if statements for getWeather and sendEmail) ...
    ```
    *   **Formatting results:** Result must be a string (JSON, error codes, plain text, etc.). For functions with no return value (e.g., `send_email`), return a success/failure string (e.g., `"success"`).
    *   **Incorporating results into response:** After appending results to `input`, send them back to the model.
        Send results back to model:
        ```python
        # ... (Python code provided: calls client.responses.create with updated input_messages and tools) ...
        ```
        ```javascript
        // ... (JS code provided: calls openai.responses.create with updated input and tools) ...
        ```
        Final response example:
        ```json
        "It's about 15C in Paris, 18C in Bogot, and I've sent that email to Bob."
        ```

*   **Additional configurations**
    -------------------------
    *   **Tool choice:** By default (`"auto"`), model determines when/how many tools to use. Control with `tool_choice`:
        1.  `"auto"`: (Default) Zero, one, or multiple functions.
        2.  `"required"`: One or more functions.
        3.  `{"type": "function", "name": "get_weather"}`: Exactly one specific function.
        *(Tool Choice diagram provided)*
        Can also set `tool_choice` to `"none"` to prevent tool use.
    *   **Parallel function calling:** Model may call multiple functions per turn. Set `parallel_tool_calls` to `false` to ensure zero or one call. Note: Parallel calls currently disable [strict mode](#strict-mode-1). Note for `gpt-4.1-nano-2025-04-14`: May call same tool multiple times if parallel calls enabled; recommend disabling.
    *   **Strict mode:** Set `strict: true` in function definition for reliable schema adherence (uses [structured outputs](/docs/guides/structured-outputs)). Recommended. Requirements: `additionalProperties: false` for all objects, all fields `required`. Use `["type", "null"]` for optionality. Some JSON schema features not supported. Schemas cached (performance implications, not eligible for ZDR). Playground schemas use strict mode.
        Strict mode enabled example:
        ```json
        // ... (JSON schema for get_weather with strict: true, additionalProperties: false, all fields required, units type is ["string", "null"]) ...
        ```
        Strict mode disabled example:
        ```json
        // ... (JSON schema for get_weather without strict, additionalProperties default/true, units not required) ...
        ```

*   **Streaming**
    ---------
    Surface progress by showing function calls/arguments in real time. Set `stream: true`. Similar to regular response streaming, but aggregates argument chunks.

    Streaming function calls example:
    ```python
    # ... (Python code provided: calls client.responses.create with stream=True, iterates events) ...
    ```
    ```javascript
    // ... (JS code provided: calls openai.responses.create with stream=True, iterates events) ...
    ```
    Output events example:
    ```json
    // ... (JSON sequence showing response.output_item.added, response.function_call_arguments.delta, .done, response.output_item.done events) ...
    ```
    Instead of aggregating `content` string, aggregate encoded `arguments` JSON object.
    *   `response.output_item.added`: Emitted for each function call. Contains `response_id`, `output_index`, `item` (in-progress call with `name`, empty `arguments`, `id`, `call_id`).
    *   `response.function_call_arguments.delta`: Contains `delta` chunk of `arguments` string. Has `response_id`, `item_id`, `output_index`, `delta`.
    *   `response.function_call_arguments.done`: Emitted when arguments finished streaming. Contains full `arguments` string. Has `response_id`, `item_id`, `output_index`, `arguments`.
    *   `response.output_item.done`: Emitted when the entire function call item is complete. Contains the full `item` object. Has `response_id`, `output_index`, `item`.
    Code snippet for accumulating `delta`s:
    ```python
    # ... (Python code provided: initializes dict, iterates stream, appends delta to arguments string based on output_index) ...
    ```
    ```javascript
    // ... (JS code provided: initializes object, iterates stream, appends delta to arguments string based on output_index) ...
    ```
    Accumulated result example:
    ```json
    // ... (JSON showing final accumulated tool_call object) ...
    ```

    *(Was this page useful?)*

### Conversation state (General Guide)
==================

Learn how to manage conversation state during a model interaction.

OpenAI provides a few ways to manage conversation state, which is important for preserving information across multiple messages or turns in a conversation.

*   **Manually manage conversation state**
    ----------------------------------
    While each text generation request is independent and stateless (unless you're using [the Assistants API](/docs/assistants/overview)), you can still implement **multi-turn conversations** by providing additional messages as parameters to your text generation request. Consider a knock-knock joke:

    Manually construct a past conversation:
    ```javascript
    // ... (JS code omitted for brevity, passes 3 messages in input array) ...
    ```
    ```python
    # ... (Python code omitted for brevity, passes 3 messages in input array) ...
    ```
    By using alternating `user` and `assistant` messages, you capture the previous state of a conversation in one request to the model.
    To manually share context across generated responses, include the model's previous response output as input, and append that input to your next request.
    In the following example, we ask the model to tell a joke, followed by a request for another joke. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions.

    Manually manage conversation state with the Responses API:
    ```javascript
    // ... (JS code provided: makes first request, adds user message and model output message(s) to history array, makes second request with history) ...
    ```
    ```python
    # ... (Python code provided: makes first request, adds user message and model output message(s) to history list, makes second request with history) ...
    ```

*   **OpenAI APIs for conversation state**
    ----------------------------------
    Our APIs make it easier to manage conversation state automatically, so you don't have to do pass inputs manually with each turn of a conversation.
    Share context across generated responses with the `previous_response_id` parameter. This parameter lets you chain responses and create a threaded conversation.
    In the following example, we ask the model to tell a joke. Separately, we ask the model to explain why it's funny, and the model has all necessary context to deliver a good response.

    Manually manage conversation state with the Responses API (using `previous_response_id`): *(Note: Title seems incorrect, example uses `previous_response_id`)*
    ```javascript
    // ... (JS code provided: makes first request with store=true, makes second request using response.id as previous_response_id) ...
    ```
    ```python
    # ... (Python code provided: makes first request, makes second request using response.id as previous_response_id) ...
    ```
    Even when using `previous_response_id`, all previous input tokens for responses in the chain are billed as input tokens in the API.

*   **Managing the context window**
    ---------------------------
    Understanding context windows will help you successfully create threaded conversations and manage state across model interactions.
    The **context window** is the maximum number of tokens that can be used in a single request. This max tokens number includes input, output, and reasoning tokens. To learn your model's context window, see [model details](/docs/models).
    *   **Managing context for text generation:** As your inputs become more complex, or you include more turns in a conversation, you'll need to consider both **output token** and **context window** limits. Model inputs and outputs are metered in [**tokens**](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them), which are parsed from inputs to analyze their content and intent and assembled to render logical outputs. Models have limits on token usage during the lifecycle of a text generation request.
        *   **Output tokens:** Tokens generated by a model in response to a prompt. Each model has different [limits](/docs/models). E.g., `gpt-4o-2024-08-06` max 16,384 output tokens.
        *   **Context window:** Total tokens (input + output + reasoning). Compare [limits](/docs/models). E.g., `gpt-4o-2024-08-06` total 128k tokens.
    If you create a very large prompt, you run the risk of exceeding the context window, which might result in truncated outputs.
    Use the [tokenizer tool](/tokenizer), built with the [tiktoken library](https://github.com/openai/tiktoken), to see how many tokens are in a text string.
    For example, when using the Responses API with a reasoning model (like o1), the following count toward the context window total: Input tokens, Output tokens, Reasoning tokens.
    Tokens generated in excess of the limit may be truncated.
    *(context window visualization image provided)*
    You can estimate tokens with the [tokenizer tool](/tokenizer).

*   **Next steps**
    ----------
    *   [Receive JSON responses with Structured Outputs](/docs/guides/structured-outputs)
    *   [Extend the models with function calling](/docs/guides/function-calling)
    *   [Enable streaming for real-time responses](/docs/guides/streaming-responses)
    *   [Build a computer using agent](/docs/guides/tools-computer-use)

    *(Was this page useful?)*

### Streaming API responses (General Guide)
=======================

Learn how to stream model responses from the OpenAI API using server-sent events.

By default, when you make a request to the OpenAI API, we generate the model's entire output before sending it back in a single HTTP response. When generating long outputs, waiting for a response can take time. Streaming responses lets you start printing or processing the beginning of the model's output while it continues generating the full response.

*   **Enable streaming**
    ----------------
    To start streaming responses, set `stream=True` in your request to the Responses endpoint:
    ```javascript
    // ... (JS code omitted for brevity, sets stream: true) ...
    ```
    ```python
    # ... (Python code omitted for brevity, sets stream=True) ...
    ```
    The Responses API uses semantic events for streaming. Each event is typed with a predefined schema, so you can listen for events you care about.
    For a full list of event types, see the [API reference for streaming](/docs/api-reference/responses-streaming). Here are a few examples:
    ```python
    # Type hint showing union of possible StreamingEvent types
    type StreamingEvent =
        | ResponseCreatedEvent
        # ... (Many other event types listed) ...
        | Error
    ```

*   **Read the responses**
    ------------------
    If you're using our SDK, every event is a typed instance. You can also identify individual events using the `type` property of the event.
    Some key lifecycle events are emitted only once, while others are emitted multiple times. Common events to listen for when streaming text are:
    ```text
    - `response.created`
    - `response.output_text.delta`
    - `response.completed`
    - `error`
    ```
    For a full list of events, see the [API reference for streaming](/docs/api-reference/responses-streaming).

*   **Advanced use cases**
    ------------------
    *   [Streaming function calls](/docs/guides/function-calling#streaming)
    *   [Streaming structured output](/docs/guides/structured-outputs#streaming)

*   **Moderation risk**
    ---------------
    Note that streaming the model's output in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate. This may have implications for approved usage.

    *(Was this page useful?)*

### File inputs (General Guide)
===========

Learn how to use PDF files as inputs to the OpenAI API.

OpenAI models with vision capabilities can also accept PDF files as input. Provide PDFs either as Base64-encoded data or as file IDs obtained after uploading files to the `/v1/files` endpoint through the [API](/docs/api-reference/files) or [dashboard](/storage/files/).

*   **How it works**
    ------------
    To help models understand PDF content, we put into the model's context both the extracted text and an image of each page. The model can then use both the text and the images to generate a response. This is useful, for example, if diagrams contain key information that isn't in the text.

*   **Uploading files**
    ---------------
    In the example below, we first upload a PDF using the [Files API](/docs/api-reference/files), then reference its file ID in an API request to the model.

    Upload a file to use in a response:
    ```bash
    # ... (Bash code provided: 1st curl uploads file, 2nd curl uses file_id in Responses API input) ...
    ```
    ```javascript
    // ... (JS code provided: uploads file with client.files.create, uses file.id in client.responses.create) ...
    ```
    ```python
    # ... (Python code provided: uploads file with client.files.create, uses file.id in client.responses.create) ...
    ```

*   **Base64-encoded files**
    --------------------
    You can send PDF file inputs as Base64-encoded inputs as well.

    Base64 encode a file to use in a response:
    ```bash
    # ... (Bash code provided: uses file_data with base64 string in Responses API input) ...
    ```
    ```javascript
    // ... (JS code provided: reads file, base64 encodes, uses data URL in client.responses.create) ...
    ```
    ```python
    # ... (Python code provided: reads file, base64 encodes, uses data URL in client.responses.create) ...
    ```

*   **Usage considerations**
    --------------------
    Below are a few considerations to keep in mind while using PDF inputs.
    *   **Token usage:** Model gets extracted text + image of each page. Understand [pricing](/docs/pricing) implications.
    *   **File size limitations:** Up to 100 pages and 32MB total content per request (across multiple files).
    *   **Supported models:** Only models supporting text and image inputs (e.g., gpt-4o, o1). Check [model features](/docs/models).
    *   **File upload purpose:** Use any [purpose](/docs/api-reference/files/create#files-create-purpose), but `user_data` recommended for model inputs.

*   **Next steps**
    ----------
    *   [Experiment with PDF inputs in the Playground](/playground)
    *   [Full API reference](/docs/api-reference/responses)

    *(Was this page useful?)*

### Reasoning models (General Guide)
================

Explore advanced reasoning and problem-solving models.

**Reasoning models** like o3 and o4-mini are LLMs trained with reinforcement learning to perform reasoning. Reasoning models [think before they answer](https://openai.com/index/introducing-openai-o1-preview/), producing a long internal chain of thought before responding to the user. Reasoning models excel in complex problem solving, coding, scientific reasoning, and multi-step planning for agentic workflows. They're also the best models for [Codex CLI](https://github.com/openai/codex), our lightweight coding agent.

As with our GPT series, we provide smaller, faster models (`o4-mini` and `o3-mini`) that are less expensive per token. The larger models (`o3` and `o1`) are slower and more expensive but often generate better responses for complex tasks and broad domains.

To ensure safe deployment of our latest reasoning models [`o3`](/docs/models/o3) and [`o4-mini`](/docs/models/o4-mini), some developers may need to complete [organization verification](https://help.openai.com/en/articles/10910291-api-organization-verification) before accessing these models. Get started with verification on the [platform settings page](https://platform.openai.com/settings/organization/general).

*   **Get started with reasoning**
    --------------------------
    Reasoning models can be used through the [Responses API](/docs/api-reference/responses/create) as seen here.

    Using a reasoning model in the Responses API:
    ```javascript
    // ... (JS code provided: calls client.responses.create with model="o4-mini", reasoning={effort:"medium"}) ...
    ```
    ```python
    # ... (Python code provided: calls client.responses.create with model="o4-mini", reasoning={"effort":"medium"}) ...
    ```
    ```bash
    # ... (Bash code provided: uses model="o4-mini", reasoning={"effort":"medium"}) ...
    ```
    In the example above, the `reasoning.effort` parameter guides the model on how many reasoning tokens to generate before creating a response. Specify `low`, `medium` (default), or `high`.

*   **How reasoning works**
    -------------------
    Reasoning models introduce **reasoning tokens** (used for internal "thinking") in addition to input and output tokens. Reasoning tokens count towards context window and are billed as output tokens, but are discarded from context after generation and not visible via API.
    *(Reasoning tokens context window diagram provided)*
    *   **Managing the context window:** Ensure enough space for reasoning tokens (can range from hundreds to tens of thousands). Check `usage.output_tokens_details.reasoning_tokens` in response. See [model reference](/docs/models) for context lengths.
    *   **Controlling costs:** Limit total generated tokens (reasoning + output) with `max_output_tokens`. Discard older reasoning items if managing context manually (except between function call and result).
    *   **Allocating space for reasoning:** If `max_output_tokens` or context limit reached, response status is `incomplete`, reason `max_output_tokens`. May happen before visible output generated. Recommend reserving at least 25,000 tokens initially.
        Handling incomplete responses example:
        ```javascript
        // ... (JS code provided: checks response status and incomplete_details after call with max_output_tokens) ...
        ```
        ```python
        # ... (Python code provided: checks response status and incomplete_details after call with max_output_tokens) ...
        ```
    *   **Keeping reasoning items in context:** When doing function calling, pass back reasoning items between last user message and function call output (either via `previous_response_id` or manually adding items to `input`). Simplest is to pass all items from previous response. Systems ignore irrelevant reasoning items. See [conversation state guide](/docs/guides/conversation-state).

*   **Reasoning summaries**
    -------------------
    View a summary of reasoning using `reasoning: { summary: "auto" | "concise" | "detailed" }`. Summary included in `reasoning` output item. Supported by `o4-mini`, `o3`, `o3-mini`, `o1`, computer use model (`concise`). `auto` gives best available summary. Requires org verification for latest models.
    Generate a summary example:
    ```json
    reasoning: {
      effort: "medium", // unchanged
      summary: "auto" // auto gives you the best available summary (detailed > auto > None)
    }
    ```

*   **Advice on prompting**
    -------------------
    Differences vs. GPT: Reasoning models prefer high-level guidance; GPT models prefer precise instructions. Reasoning  senior co-worker, GPT  junior co-worker. See [reasoning best practices guide](/docs/guides/reasoning-best-practices).
    *   **Prompt examples:**
        *   Coding (refactoring): Refactor React component. *(Code examples provided)*
        *   Coding (planning): Plan Python app structure, generate files. *(Code examples provided)*
        *   STEM Research: Ask basic research questions. *(Code examples provided)*

*   **Use case examples**
    -----------------
    Cookbook examples:
    *   [Using reasoning for data validation](https://cookbook.openai.com/examples/o1/using_reasoning_for_data_validation)
    *   [Using reasoning for routine generation](https://cookbook.openai.com/examples/o1/using_reasoning_for_routine_generation)

    *(Was this page useful?)*

### Evaluating model performance (General Guide)
============================

Test and improve model outputs through evaluations.

Evaluations (often called **evals**) test model outputs to ensure they meet style and content criteria that you specify. Writing evals to understand how your LLM applications are performing against your expectations, especially when upgrading or trying new models, is an essential component to building reliable applications.

In this guide, we will focus on **configuring evals programmatically using the [Evals API](/docs/api-reference/evals)**. If you prefer, you can also configure evals [in the OpenAI dashboard](/evaluations).

Broadly, there are three steps to build and run evals for your LLM application.
1.  Describe the task to be done as an eval
2.  Run your eval with test inputs (a prompt and input data)
3.  Analyze the results, then iterate and improve on your prompt
This process is somewhat similar to behavior-driven development (BDD).

*   **Create an eval for a task**
    -------------------------
    Begins by describing the task. Example: Classify IT support tickets into `Hardware`, `Software`, or `Other`.
    Example Chat Completions call for the task:
    ```bash
    # ... (Bash code omitted for brevity) ...
    ```
    ```javascript
    // ... (JS code omitted for brevity) ...
    ```
    ```python
    # ... (Python code omitted for brevity) ...
    ```
    Set up eval via API: Needs `data_source_config` (test data schema) and `testing_criteria`.
    ```bash
    # ... (Bash curl command provided to create an eval with name, data_source_config specifying item_schema {ticket_text, correct_label} and include_sample_schema=true, and testing_criteria using string_check to compare sample.output_text with item.correct_label) ...
    ```
    *   **Explanation: `data_source_config` parameter:** Specifies schema for test data items (`ticket_text`, `correct_label`). `include_sample_schema: true` needed if criteria reference `sample`.
    *   **Explanation: `testing_criteria` parameter:** Defines how to judge correctness. Uses template syntax `{{ }}`. Example uses `string_check` to compare `{{ sample.output_text }}` (model output for this item) with `{{ item.correct_label }}` (ground truth).
    After creating, eval gets an ID (e.g., `eval_67e...`).

*   **Test a prompt with your eval**
    ----------------------------
    Construct a prompt and test against representative data.
    *   **Uploading test data:** Use JSONL format matching schema defined in eval. Upload via dashboard or Files API (`purpose="evals"`).
        Sample `tickets.jsonl`:
        ```json
        { "item": { "ticket_text": "My monitor won't turn on!", "correct_label": "Hardware" } }
        { "item": { "ticket_text": "I'm in vim and I can't quit!", "correct_label": "Software" } }
        { "item": { "ticket_text": "Best restaurants in Cleveland?", "correct_label": "Other" } }
        ```
        Upload command (example):
        ```bash
        # ... (Bash curl command provided to upload tickets.jsonl with purpose="evals") ...
        ```
        ```javascript
        // ... (JS code provided to upload tickets.jsonl) ...
        ```
        ```python
        # ... (Python code provided to upload tickets.jsonl) ...
        ```
        Note the returned file ID (e.g., `file-CwH...`).
    *   **Creating an eval run:** POST to `/v1/evals/{eval_id}/runs`. Provide run `name`, `data_source` (including `model`, `input` template using `{{ item.ticket_text }}`, and `source` file ID).
        ```bash
        # ... (Bash curl command provided to create eval run, referencing eval_id and file_id, providing model and input template) ...
        ```
        Run is queued (`status: "queued"`) and executes asynchronously.

*   **Analyze the results**
    -------------------
    Check run status/results via dashboard (`report_url` in response) or API (GET `/v1/evals/{eval_id}/runs/{run_id}`).
    Completed run response includes `status: "completed"`, `result_counts` (total, passed, failed, errored), `per_model_usage`, `per_testing_criteria_results`. Use results to iterate on prompts/models.

*   **Video: evals in the dashboard**
    -----------------------------
    Video provides overview of dashboard UI (may differ from current UI). *(No actual video embedded)*

*   **Next steps**
    ----------
    *   Cookbook: Detecting prompt regressions
    *   Cookbook: Bulk model and prompt experimentation
    *   Cookbook: Monitoring stored completions
    *   Fine-tuning guide
    *   Model distillation guide

    *(Was this page useful?)*

```markdown
*(... Content from the previous responses up to here ...)*

### Built-in tools (General Guide)
==============

Use built-in tools like web search and file search to extend the model's capabilities.

When generating model responses, you can extend model capabilities using built-in **tools**. These tools help models access additional context and information from the web or your files. The example below uses the [web search tool](/docs/guides/tools-web-search) to use the latest information from the web to generate a model response.

Include web search results for the model response:
```javascript
// ... (JS code omitted for brevity, uses tools: [ { type: "web_search_preview" } ]) ...
```
```python
# ... (Python code omitted for brevity, uses tools=[{"type": "web_search_preview"}]) ...
```
```bash
# ... (Bash code omitted for brevity, uses tools: [{"type": "web_search_preview"}]) ...
```

*   **Available tools**
    ---------------
    Here's an overview of the tools available in the OpenAI platformselect one of them for further guidance on usage.
    *   [Web search](/docs/guides/tools-web-search): Include data from the Internet.
    *   [File search](/docs/guides/tools-file-search): Search contents of uploaded files.
    *   [Computer use](/docs/guides/tools-computer-use): Enable model to control a computer interface.
    *   [Function calling](/docs/guides/function-calling): Enable model to call custom code.

*   **Usage in the API**
    ----------------
    When making a request to generate a [model response](/docs/api-reference/responses/create), you can enable tool access by specifying configurations in the `tools` parameter. Each tool has its own unique configuration requirementssee the [Available tools](#available-tools) section for detailed instructions.
    Based on the provided [prompt](/docs/guides/text), the model automatically decides whether to use a configured tool. For instance, if your prompt requests information beyond the model's training cutoff date and web search is enabled, the model will typically invoke the web search tool.
    You can explicitly control or guide this behavior by setting the `tool_choice` parameter [in the API request](/docs/api-reference/responses/create).

*   **Function calling:** In addition to built-in tools, you can define custom functions using the `tools` array. Learn more in the [function calling guide](/docs/guides/function-calling).

    *(Was this page useful?)*

### Web search (Tool Guide)
==========

Allow models to search the web for the latest information before generating a response.

Using the [Responses API](/docs/api-reference/responses), you can enable web search by configuring it in the `tools` array in an API request to generate content. Like any other tool, the model can choose to search the web or not based on the content of the input prompt.

Web search tool example:
```javascript
// ... (JS code omitted for brevity, uses tools: [ { type: "web_search_preview" } ]) ...
```
```python
# ... (Python code omitted for brevity, uses tools=[{"type": "web_search_preview"}]) ...
```
```bash
# ... (Bash code omitted for brevity, uses tools: [{"type": "web_search_preview"}]) ...
```
*   **Web search tool versions:**
    *   Current default: `web_search_preview`
    *   Points to dated version: `web_search_preview_2025_03_11`
    *   Future versions documented in [API reference](/docs/api-reference/responses/create).
    *   Can force use via `tool_choice: {type: "web_search_preview"}` for consistency/latency.

*   **Output and citations**
    --------------------
    Model responses using web search include:
    *   A `web_search_call` output item (with search call ID).
    *   A `message` output item containing:
        *   Text result (`message.content[0].text`).
        *   Annotations (`message.content[0].annotations`) for cited URLs (`url_citation` type with URL, title, start/end index).
    By default, response includes inline citations. Display citations clearly and make them clickable in UI.
    Example response structure:
    ```json
    // ... (JSON showing web_search_call and message with url_citation annotations) ...
    ```

*   **User location**
    -------------
    Refine results geographically using `user_location` within the tool config.
    *   `city`, `region`: Free text strings.
    *   `country`: Two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1).
    *   `timezone`: [IANA timezone](https://timeapi.io/documentation/iana-timezones).
    Customizing user location example:
    ```python
    # ... (Python code provided: adds user_location dict to web_search_preview tool config) ...
    ```
    ```javascript
    // ... (JS code provided: adds userLocation object to web_search_preview tool config) ...
    ```
    ```bash
    # ... (Bash code provided: adds user_location JSON object to web_search_preview tool config) ...
    ```

*   **Search context size**
    -------------------
    `search_context_size` parameter controls context retrieved (`high`, `medium` (default), `low`).
    *   Impacts: Cost (higher = more expensive), Quality (higher = richer context, more accurate), Latency (higher = slower).
    *   Tokens used by search tool *do not* affect main model context/billing and are *not* carried over turns. Check [pricing page](/docs/pricing).
    Customizing search context size example:
    ```python
    # ... (Python code provided: sets search_context_size: "low") ...
    ```
    ```javascript
    // ... (JS code provided: sets search_context_size: "low") ...
    ```
    ```bash
    # ... (Bash code provided: sets search_context_size: "low") ...
    ```

*   **Limitations**
    -----------
    *   Not supported in `gpt-4.1-nano`.
    *   `gpt-4o-search-preview` / `gpt-4o-mini-search-preview` (Chat Completions) have limited parameter support.
    *   Tiered rate limits apply (same as preview search models).
    *   Data handling per [Your Data guide](/docs/guides/your-data).

    *(Was this page useful?)*

### File search (Tool Guide)
===========

Allow models to search your files for relevant information before generating a response.

*   **Overview:** File search (Responses API tool) enables semantic/keyword search over uploaded files in Vector Stores. Augments model knowledge. Hosted tool (OpenAI handles execution). Learn more about vector stores/semantic search in [Retrieval guide](/docs/guides/retrieval).
*   **How to use:** Requires setting up a Vector Store and uploading files first.
    *   **Create vector store and upload file steps:**
        1.  Upload file via Files API (`purpose="assistants"` - although `user_data` might be better now). Example uses URL or local path. *(Python/JS code provided)*. Note the `file_id`.
        2.  Create vector store. *(Python/JS code provided)*. Note the `vector_store.id`.
        3.  Add file to vector store. *(Python/JS code provided)*.
        4.  Check status until file is `completed`. *(Python/JS code provided)*.
    *   **Using the tool:** Include `file_search` in `tools` array, specifying `vector_store_ids` (currently only one ID supported per call).
        File search tool example:
        ```python
        # ... (Python code provided: uses tools=[{"type": "file_search", "vector_store_ids": ["<vector_store_id>"]}]) ...
        ```
        ```javascript
        // ... (JS code provided: uses tools:[{type: "file_search", vector_store_ids: ["<vector_store_id>"]}]) ...
        ```
    *   **Response structure:** Output includes `file_search_call` item (search details) and `message` item (text response with `file_citation` annotations).
        Example response:
        ```json
        // ... (JSON showing file_search_call and message with file_citation annotations) ...
        ```

*   **Retrieval customization**
    -----------------------
    *   **Limiting number of results:** Use `max_num_results` parameter in tool config (default 10, max 50).
        Limit results example:
        ```python
        # ... (Python code provided: adds max_num_results: 2 to tool config) ...
        ```
        ```javascript
        // ... (JS code provided: adds max_num_results: 2 to tool config) ...
        ```
    *   **Include search results:** Use `include=["file_search_call.results"]` parameter in `responses.create` call to get actual text chunks in `file_search_call.search_results`.
        Include results example:
        ```python
        # ... (Python code provided: adds include=["file_search_call.results"] to responses.create call) ...
        ```
        ```javascript
        // ... (JS code provided: adds include:["file_search_call.results"] to responses.create call) ...
        ```
    *   **Metadata filtering:** Filter results based on file `attributes`. Define attributes when adding files. Use `filters` parameter in tool config (comparison/compound filters). See [Retrieval guide](/docs/guides/retrieval) for details.
        Metadata filtering example:
        ```python
        # ... (Python code provided: adds filters object to tool config) ...
        ```
        ```javascript
        // ... (JS code provided: adds filters object to tool config) ...
        ```

*   **Supported files:** List of formats (.c, .cpp, .docx, .html, .java, .json, .md, .pdf, .php, .pptx, .py, .rb, .tex, .txt, etc.) and MIME types provided. Text files need utf-8, utf-16, or ascii encoding.

*   **Limitations:**
    *   Project storage limit: 100GB total for Files.
    *   Vector store file limit: 10k files per store.
    *   Individual file limit: 512MB (~5M tokens).

    *(Was this page useful?)*

### Computer use (Tool Guide - Beta)
============

Build a computer-using agent that can perform tasks on your behalf.

*   **Overview:** Practical application of Computer-Using Agent (CUA) model (`computer-use-preview`). Combines vision (GPT-4o) + reasoning to simulate controlling interfaces. Available via **Responses API only**. Beta status - use cautiously, avoid high-stakes/authenticated environments. Comply with Usage Policy/Business Terms.
*   **How it works:** Continuous loop: Model suggests action (`computer_call`: click, type, scroll) -> Your code executes action in environment -> Return screenshot (`computer_call_output`) -> Model sees state, suggests next action. Automates tasks like booking flights, filling forms. See [sample app repo](https://github.com/openai/openai-cua-sample-app).
*   **Setting up your environment:** Prepare environment to capture screenshots, execute actions. Sandboxed recommended. Examples: Local browser (Playwright/Selenium) or local VM (Docker).
    *   **Local browsing environment:** Use automation framework (Playwright/Selenium). Recommendations: Sandboxed env, empty `env` object, disable extensions/filesystem flags.
        Start browser instance example (Playwright):
        ```javascript
        // ... (JS code provided: launches chromium, sets viewport, goes to bing.com) ...
        ```
        ```python
        # ... (Python code provided: launches chromium, sets viewport, goes to bing.com) ...
        ```
    *   **Local virtual machine (Docker):** Use Docker for non-browser interfaces.
        1.  Start Docker.
        2.  Create Dockerfile (example provided: Ubuntu + Xfce + VNC server + Firefox ESR).
        3.  Build Docker image (`docker build -t cua-image .`).
        4.  Run container (`docker run ... -p 5900:5900 ... cua-image`).
        5.  Execute commands on container (helper function examples provided for Python/JS using `docker exec`).

*   **Integrating the CUA loop:**
    1.  **Send request:** Call Responses API with `computer-use-preview` model, `computer_use_preview` tool (specify display size, environment: browser/mac/windows/ubuntu), initial prompt. Optionally include initial screenshot (`input_image`), request reasoning summary (`reasoning: { summary: "concise" | "detailed" }`), set `truncation: "auto"`. *(Python/JS code examples provided)*
    2.  **Receive response:** Check `output` for `computer_call` items (action: click, scroll, keypress, type, wait, etc.). May also contain `reasoning` items and `pending_safety_checks`. *(Example JSON output provided)*
        *   **Reasoning items:** If managing history manually (not using `previous_response_id`), include these `reasoning` items in next request to CUA model. Filter out if sending history to other models.
        *   **Safety checks:** See [Acknowledge safety checks](#acknowledge-safety-checks-1) section.
    3.  **Execute action:** Map `computer_call.action` to code execution in your environment (Playwright/Docker examples provided for click, scroll, keypress, type, wait, screenshot).
    4.  **Capture screenshot:** Get updated state as image bytes/base64. *(Playwright/Docker examples provided)*
    5.  **Repeat:** Send screenshot back as `computer_call_output` (using `call_id` from previous step), including `acknowledged_safety_checks` if needed. Loop until no `computer_call` received. *(Python/JS loop examples provided)*
        *   **Handling conversation history:** Recommend using `previous_response_id`. If managing manually, include all output items (incl. reasoning) from previous response in next input.
*   **Acknowledge safety checks:** If response includes `pending_safety_checks` (codes: `malicious_instructions`, `irrelevant_domain`, `sensitive_domain`), pass them back in next request's `computer_call_output` via `acknowledged_safety_checks` array. Hand over to end user for confirmation/monitoring when checks fire. Providing `current_url` in output helps check accuracy. *(Python/JS examples provided)*
*   **Final code:** Refer to [CUA sample app repo](https://github.com/openai/openai-cua-sample-app) for end-to-end examples.
*   **Limitations:** Beta. Recommend browser tasks. OS automation less reliable (OSWorld 38.1%). See model system card. Specific rate limits/features for `computer-use-preview`. Data handling per policies.
*   **Risks and safety:** Unique risks vs. standard APIs. Best practices:
    *   Human in loop for high-stakes/accuracy tasks.
    *   Beware prompt injection (use sandboxed env).
    *   Use block/allowlists (websites, actions, users).
    *   Send end-user IDs (optional param).
    *   Use/acknowledge safety checks (hand over to user when fired). Provide `current_url`.
    *   Implement additional safety (parallel guardrails).
    *   Comply with Usage Policy/Business Terms.

    *(Was this page useful?)*

### Realtime API (Beta Guide)
====================

Build low-latency, multi-modal experiences with the Realtime API.

The OpenAI Realtime API enables low-latency, multimodal interactions including speech-to-speech conversational experiences and real-time transcription.
This API works with natively multimodal models such as [GPT-4o](/docs/models/gpt-4o-realtime) and [GPT-4o mini](/docs/models/gpt-4o-mini-realtime), offering capabilities such as real-time text and audio processing, function calling, and speech generation, and with the latest transcription models [GPT-4o Transcribe](/docs/models/gpt-4o-transcribe) and [GPT-4o mini Transcribe](/docs/models/gpt-4o-mini-transcribe).

*   **Get started with the Realtime API**
    ---------------------------------
    Connect in two ways:
    *   [WebRTC](#connect-with-webrtc-1): Ideal for client-side (web apps).
    *   [WebSockets](#connect-with-websockets-1): Great for server-to-server (backends, telephony).
    *   **Example applications:** Realtime Console, Solar System demo, Twilio demo, Realtime Agents demo. *(Links provided)*
    *   **Partner integrations:** LiveKit, Twilio, Agora, Pipecat, Client-side tool calling (Cloudflare Workers). *(Links provided)*

*   **Use cases**
    ---------
    *   Real-time speech-to-speech conversations ([Voice agents](/docs/guides/voice-agents)).
    *   Real-time transcription / turn detection.
    *   Benefits from built-in [Voice Activity Detection (VAD)](/docs/guides/realtime-vad).
    *   See dedicated guides: [Realtime Conversations](/docs/guides/realtime-conversations), [Realtime Transcription](/docs/guides/realtime-transcription).
    *   Initialization differs based on use case (use switcher in original doc).

*   **Connect with WebRTC**
    -------------------
    Recommended for insecure clients (browsers). Handles variable connections, convenient media APIs. Requires ephemeral API key for security.
    *   **Overview:** Browser requests ephemeral key from your server -> Your server gets key from OpenAI REST API (using standard key) -> Browser uses ephemeral key to establish WebRTC peer connection with OpenAI Realtime API. *(Diagram provided)*. Standard keys leak if used client-side.
    *   **Connection details:**
        *   URL: `https://api.openai.com/v1/realtime`
        *   Query Params: `model` (e.g., `gpt-4o-realtime-preview-2024-12-17`)
        *   Headers: `Authorization: Bearer EPHEMERAL_KEY`
    *   **Initializing session:** Example JS code shows creating `RTCPeerConnection`, setting up remote audio track (`ontrack`), adding local audio track (`getUserMedia`, `addTrack`), creating data channel (`createDataChannel`, `onmessage`), creating SDP offer, setting local description, POSTing offer to Realtime API endpoint with ephemeral key, setting remote description with answer SDP. *(JS code provided)*. See MDN WebRTC docs for UI building.
    *   **Creating an ephemeral token:** Requires backend endpoint. Uses standard API key to POST to `/v1/realtime/sessions` (specify model, voice). Returns session info including `client_secret` (the ephemeral key) to browser. Example Node.js express server provided. Use standard keys server-side only.
    *   **Sending/receiving events:** Via data channel. See [Realtime conversations guide](/docs/guides/realtime-conversations#handling-audio-with-webrtc).

*   **Connect with WebSockets**
    -----------------------
    Good for server-to-server. Can use standard API key (secure backend) or ephemeral key (insecure client - WebRTC preferred).
    *   **Overview:** Backend connects directly via WebSocket using standard API key. *(Diagram provided)*.
    *   **Connection details (Speech-to-Speech):**
        *   URL: `wss://api.openai.com/v1/realtime`
        *   Query Params: `model` (e.g., `gpt-4o-realtime-preview-2024-12-17`)
        *   Headers: `Authorization: Bearer YOUR_API_KEY` (standard or ephemeral), `OpenAI-Beta: realtime=v1` (required).
        *   Connection examples provided for: `ws` module (Node.js), `websocket-client` (Python), Standard `WebSocket` (browsers/Deno/Workers - uses subprotocol array for auth).
    *   **Connection details (Transcription):**
        *   URL: `wss://api.openai.com/v1/realtime`
        *   Query Params: `intent=transcription`
        *   Headers: Same as Speech-to-Speech.
        *   Connection examples provided (similar to S2S, just different URL query param).
    *   **Sending/receiving events:** Via WebSocket `send`/`onmessage`. See [Realtime conversations guide](/docs/guides/realtime-conversations#handling-audio-with-websockets) or [Realtime transcription guide](/docs/guides/realtime-transcription#handling-transcriptions).

    *(Was this page useful?)*

### Realtime conversations (Beta Guide)
==============================

Learn how to manage Realtime speech-to-speech conversations.

Once connected (via [WebRTC](/docs/guides/realtime-webrtc) or [WebSocket](/docs/guides/realtime-websocket)), call Realtime models (e.g., `gpt-4o-realtime-preview`) for S2S conversations. Requires sending client events and listening for server events. (Use [transcription mode](/docs/guides/realtime-transcription) if no response needed).

*   **Realtime speech-to-speech sessions**
    ----------------------------------
    Stateful interaction. Key components:
    *   **Session:** Controls parameters (model, voice, config).
    *   **Conversation:** User input Items + model output Items.
    *   **Responses:** Model-generated audio/text Items added to Conversation.
    *   **Input audio buffer (WebSockets only):** Manually send base64 audio chunks via JSON events.
    *(Realtime state diagram provided)*

*   **Session lifecycle events**
    ------------------------
    *   Server sends `session.created` after connection.
    *   Client sends `session.update` to change config (most props updatable anytime, except `voice` after first audio output). Max duration 30 mins.
        Update system instructions example:
        ```javascript
        // ... (JS code sending session.update event via dataChannel.send) ...
        ```
        ```python
        # ... (Python code sending session.update event via ws.send) ...
        ```
    *   Server sends `session.updated` after successful update.
    *   **Event Flow:** Client `session.update` -> Server `session.updated`, `session.created`.

*   **Text inputs and outputs**
    -----------------------
    Requires `text` modality enabled in session config (default).
    1.  Client sends `conversation.item.create` with `message` type, `user` role, `input_text` content. *(JS/Python code provided)*
    2.  Client sends `response.create` to trigger model response. Can specify `modalities: ["text"]` for text-only. *(JS/Python code provided)*
    3.  Server emits lifecycle events (see below).
    4.  Server sends `response.done` when complete, containing full text output. Listen for this event. *(JS/Python code provided)*
    *   **Event Flow:** Client `conversation.item.create`, `response.create` -> Server `conversation.item.created`, `response.created`, `response.output_item.added`, `response.content_part.added`, `response.text.delta`, `response.text.done`, `response.content_part.done`, `response.output_item.done`, `response.done`, `rate_limits.updated`.

*   **Audio inputs and outputs**
    ------------------------
    Low-latency voice-to-voice. Model gets tone/inflection.
    *   **Voice options:** Set `voice` on session creation/update or `response.create`. Options: `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`. Cannot change `voice` after first audio output in session.
    *   **Handling audio with WebRTC:** Realtime API is peer connection. Remote audio delivered as `MediaStream` (`pc.ontrack`). Local audio captured (`getUserMedia`), added as track (`pc.addTrack`). Browser Media Capture APIs allow mic control. *(JS code snippet provided)*
    *   **Client/Server events (WebRTC Audio):** No client events needed by default to send audio (just add track). Server events received: `input_audio_buffer.speech_started`, `input_audio_buffer.speech_stopped`, `response.audio_transcript.delta`, `response.done`. Lower-level events available if needed (see WebSockets).
    *   **Handling audio with WebSockets:** Requires manual interaction with input audio buffer.
        *   **Event Flow:**
            *   Session init: `session.update` -> `session.created`, `session.updated`
            *   User audio input: `conversation.item.create` (full msg) / `input_audio_buffer.append` (stream chunks) / `input_audio_buffer.commit` (no VAD) / `response.create` (no VAD) -> `input_audio_buffer.speech_started`, `speech_stopped`, `committed`
            *   Server audio output: `input_audio_buffer.clear` (no VAD) -> `conversation.item.created`, `response.created`, `output_item.created`, `content_part.added`, `audio.delta`, `audio_transcript.delta`, `text.delta`, `audio.done`, `audio_transcript.done`, `text.done`, `content_part.done`, `output_item.done`, `response.done`, `rate_limits.updated`
    *   **Streaming audio input (WebSockets):** Send `input_audio_buffer.append` events with Base64-encoded audio chunks (max 15MB). Configure format via `session.input_audio_format` or `response.input_audio_format`. *(JS/Python code for encoding/sending chunks provided)*
    *   **Sending full audio messages (WebSockets/WebRTC):** Use `conversation.item.create` with `input_audio` content type. *(JS/Python code provided)*
    *   **Working with audio output (WebSockets):** Recommend WebRTC for client playback. If using WebSockets server-side, listen for `response.audio.delta` events (contain Base64 audio chunks). Buffer/process chunks. `response.audio.done`/`response.done` contain transcripts, not audio bytes. Configure format via `session.output_audio_format` or `response.output_audio_format`. *(JS/Python listener examples provided)*

*   **Voice activity detection (VAD)**
    ------------------------
    Enabled by default. API determines start/stop of speech, responds automatically. See [VAD guide](/docs/guides/realtime-vad).
    *   **Disable VAD:** Set `turn_detection: null` in `session.update`. Requires manual client events: `input_audio_buffer.commit`, `response.create`, `input_audio_buffer.clear`. Useful for push-to-talk.
    *   **Keep VAD, disable auto-response:** Set `turn_detection.interrupt_response: false`, `turn_detection.create_response: false` in `session.update`. Retains VAD events, but client must send `response.create`. Useful for moderation/validation/RAG patterns.

*   **Create responses outside the default conversation**
    -------------------------------------------------
    Generate responses not added to default conversation state, or concurrently. Set `response.conversation: "none"` in `response.create`. Use `response.metadata` to identify corresponding server events.
    Create out-of-band response example: *(JS/Python code provided, sets conversation="none", adds metadata)*
    Listen for out-of-band response example: *(JS/Python code provided, checks event type and metadata)*
    *   **Create custom context:** Provide `input` array (with new messages or `item_reference` to existing items) in `response.create` when `conversation: "none"`. *(JS/Python code provided)*
    *   **Create responses with no context:** Set `input: []` in `response.create` to ignore prior history for this specific response. *(JS/Python code provided)*

*   **Function calling**
    ----------------
    Supported by Realtime models.
    1.  **Configure:** Specify available functions in `session.tools` (via `session.update`) or `response.tools` (via `response.create`). Schema includes `name`, `description`, `parameters`. *(Example `session.update` JSON provided)*
    2.  **Detect:** Model responds with `function_call` type item in `output` array (contains `name`, `arguments` JSON string, `call_id`) if it decides to call a function based on input. Listen for `response.function_call_arguments.delta` or check `response.done`. *(Example `conversation.item.create`, `response.create`, and resulting `response.done` JSON provided)*
    3.  **Provide results:** Execute function code using arguments. Create new conversation item with `type: "function_call_output"`, matching `call_id`, and `output` (JSON string result). Send via `conversation.item.create`. *(Example `conversation.item.create` JSON provided)*
    4.  **Generate final response:** Send `response.create` again. Model uses function result to generate final response.

*   **Error handling**
    --------------
    Server emits `error` event on issues. Include `event_id` in client events to correlate errors back to specific client requests. *(JS example sending invalid event type and resulting error event JSON provided)*

    *(Was this page useful?)*

### Realtime transcription (Beta Guide)
==============================

Learn how to transcribe audio in real-time with the Realtime API.

Use Realtime API for transcription-only (subtitles, live transcripts). Connect via [WebSockets](/docs/guides/realtime?use-case=transcription#connect-with-websockets) or [WebRTC](/docs/guides/realtime?use-case=transcription#connect-with-webrtc). Does not generate model responses. (Use [conversation mode](/docs/guides/realtime-conversations) for responses).

*   **Realtime transcription sessions**
    -------------------------------
    Create session specifically for transcription (use `intent=transcription` query param for WebSocket). Session object differs from conversation sessions.
    Transcription session object example:
    ```json
    // ... (JSON structure for transcription_session provided, includes input_audio_format, input_audio_transcription array [model, prompt, language], turn_detection, input_audio_noise_reduction, include) ...
    ```
    Properties: `input_audio_transcription.model` (`gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, `whisper-1`), `.prompt`, `.language` (ISO-639-1 recommended). `input_audio_noise_reduction` (`near_field` (default), `far_field`, `null`). `include` (e.g., `item.input_audio_transcription.logprobs`). Input formats: `pcm16` (default), `g711_ulaw`, `g711_alaw`. See [API reference](/docs/api-reference/realtime-sessions/transcription_session_object).

*   **Handling transcriptions**
    -----------------------
    Listen for server events: `conversation.item.input_audio_transcription.delta` and `conversation.item.input_audio_transcription.completed`.
    *   For `whisper-1`, `delta` contains full turn transcript (same as `completed`).
    *   For `gpt-4o-transcribe` / `gpt-4o-mini-transcribe`, `delta` contains incremental transcripts.
    Example delta event: *(JSON provided)*
    Example completed event: *(JSON provided)*
    Note: Order between `completed` events not guaranteed. Use `item_id` to match with `input_audio_buffer.committed` and `previous_item_id` for ordering.
    Send audio data via `input_audio_buffer.append`. Options: streaming mic input or streaming from file.

*   **Voice activity detection (VAD)**
    ------------------------
    Enabled by default. Controls when buffer committed / transcription begins. See [VAD guide](/docs/guides/realtime-vad). Can disable (`turn_detection: null`) for manual control (`input_audio_buffer.commit`).

*   **Additional configurations**
    -------------------------
    *   **Noise reduction:** Configure via `input_audio_noise_reduction` (`near_field`, `far_field`, `null`).
    *   **Using logprobs:** Request via `include: ["item.input_audio_transcription.logprobs"]` in session config. Use for confidence scores. *(Example session update JSON provided)*

    *(Was this page useful?)*

### Voice activity detection (VAD) (Beta Guide)
======================================

Learn about automatic voice activity detection in the Realtime API.

*   **Overview:** Feature in Realtime API to auto-detect start/stop of speech. Enabled by default ([speech-to-speech](/docs/guides/realtime-conversations) / [transcription](/docs/guides/realtime-transcription)). Optional. Sends events: `input_audio_buffer.speech_started`, `input_audio_buffer.speech_stopped`. Configure via `turn_detection` in `session.update`.
*   **Modes:**
    *   `server_vad` (default): Chunks based on silence duration. Configurable properties:
        *   `threshold` (float 0-1): Activation threshold (higher = needs louder audio).
        *   `prefix_padding_ms` (int): Audio to include before speech start.
        *   `silence_duration_ms` (int): Silence duration to detect speech stop (shorter = faster detection).
        *(Example server_vad config JSON provided)*
    *   `semantic_vad`: Uses semantic classifier based on words uttered. Less likely to interrupt / chunk prematurely. Activated by `turn_detection.type: "semantic_vad"`. Configurable properties:
        *   `eagerness` (Literal["low", "medium", "high", "auto"], optional, default="auto"/"medium"): Controls max wait timeout. `high` = interrupt/chunk sooner, `low` = let user speak longer / larger chunks.
        *(Example semantic_vad config JSON provided)*

    *(Was this page useful?)*

```markdown
*(... Content from the previous responses up to here ...)*

### Image generation (General Guide)
================

Learn how to generate or manipulate images with DALLE.

*   **Introduction:** The [Image generation API](/docs/api-reference/images) has three endpoints:
    *   **Generations:** Images from scratch (text prompt).
    *   **Edits:** Edited versions (inpainting) based on image + mask + text prompt.
    *   **Variations:** Variations of an existing image.
    This guide covers basics. Try DALLE 3 without code at [ChatGPT](https://chatgpt.com/).

*   **Which model should I use?**
    | Model     | Capabilities                   | Pros                                                                    |
    |-----------|--------------------------------|-------------------------------------------------------------------------|
    | DALLE 2  | Generations, edits, variations | More options (edits/variations), more control in prompting, more requests at once |
    | DALLE 3  | Only image generations         | Higher quality, larger sizes for generated images                       |

*   **Generations:** Creates original image from text prompt via `/v1/images/generations`. Returns URL (expires in 1 hour) or Base64 data (`response_format`).
    *   **Size and quality options:** Square, standard quality fastest. Default `1024x1024`.
        | Model     | Sizes                         | Quality              | `n` (Images per request) |
        |-----------|-------------------------------|----------------------|--------------------------|
        | DALLE 2  | 256x256, 512x512, 1024x1024 | Only standard        | Up to 10                 |
        | DALLE 3  | 1024x1024, 1024x1792, 1792x1024 | `standard` (default), `hd` | 1 (use parallel calls for more) |
    *   Generate an image example (DALLE 3):
        ```javascript
        // ... (JS code provided: calls openai.images.generate with model="dall-e-3") ...
        ```
        ```python
        # ... (Python code provided: calls client.images.generate with model="dall-e-3") ...
        ```
        ```bash
        # ... (Bash code provided: calls /v1/images/generations with model="dall-e-3") ...
        ```
    *   **DALLE 3 prompting:** Model automatically rewrites prompts for safety/detail. Cannot disable. To get closer to original prompt, add: `I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:`. Rewritten prompt in `revised_prompt` field. See [What's new with DALLE 3](https://cookbook.openai.com/articles/what_is_new_with_dalle_3) cookbook.

*   **Edits (DALLE 2 only)**
    ---------------------
    `/v1/images/edits` endpoint for inpainting. Upload image + mask (transparent areas indicate edit region). Prompt describes *full* new image.
    Edit an image example:
    ```javascript
    // NOTE: Original JS example here was incorrect (showed generate). Assuming edits call.
    // Example structure (replace with actual SDK call if available)
    // const response = await openai.images.edit({
    //   model: "dall-e-2",
    //   image: fs.createReadStream("sunlit_lounge.png"),
    //   mask: fs.createReadStream("mask.png"),
    //   prompt: "A sunlit indoor lounge area with a pool containing a flamingo",
    //   n: 1,
    //   size: "1024x1024",
    // });
    ```
    ```python
    # ... (Python code provided: calls client.images.edit with image, mask, prompt) ...
    ```
    ```bash
    # ... (Bash code provided: calls /v1/images/edits with image, mask, prompt) ...
    ```
    | Image           | Mask        | Output           |
    |-----------------|-------------|------------------|
    | *(img omitted)* | *(img omitted)* | *(img omitted)*  |
    *Prompt: a sunlit indoor lounge area with a pool containing a flamingo*
    Uploaded image/mask must be square PNG < 4MB, same dimensions. Mask transparency defines edit area; non-transparent areas not used for generation.

*   **Variations (DALLE 2 only)**
    --------------------------
    `/v1/images/variations` endpoint generates variations of a given image.
    Generate an image variation example:
    ```javascript
    // ... (JS code provided: calls openai.images.createVariation with image) ...
    ```
    ```python
    # ... (Python code provided: calls client.images.create_variation with image) ...
    ```
    ```bash
    # ... (Bash code provided: calls /v1/images/variations with image) ...
    ```
    | Image           | Output           |
    |-----------------|------------------|
    | *(img omitted)* | *(img omitted)*  |
    Input image must be square PNG < 4MB.

*   **Content moderation:** Prompts/images filtered based on [content policy](https://labs.openai.com/policies/content-policy). Error returned if flagged.

*   **Language-specific tips:**
    *   **Node.js:**
        *   *Using in-memory image data:* Use `Buffer` object, set `buffer.name = "image.png"`. *(JS code provided)*
        *   *Working with TypeScript:* Cast `fs.createReadStream` or `Buffer` to `any` to handle type mismatches. *(JS code snippets provided)*
        *   *Error handling:* Use `try...catch`. Access details via `error.response` or `error.message`. *(JS code provided)*
    *   **Python:**
        *   *Using in-memory image data:* Use `BytesIO` object, pass `byte_stream.getvalue()`. *(Python code provided)*
        *   *Operating on image data:* Use libraries like `PIL` to manipulate, then convert to `BytesIO`. *(Python code provided)*
        *   *Error handling:* Use `try...except openai.OpenAIError as e`. Access details via `e.http_status`, `e.error`. *(Python code provided)*

    *(Was this page useful?)*

### Text to speech (General Guide)
==============

Learn how to turn text into lifelike spoken audio.

The Audio API provides a [`speech`](/docs/api-reference/audio/createSpeech) endpoint based on our [GPT-4o mini TTS (text-to-speech) model](/docs/models/gpt-4o-mini-tts). It comes with 11 built-in voices and can be used to: Narrate blog posts, Produce audio in multiple languages, Give realtime audio output via streaming.

*(Audio sample provided in original)*

Our [usage policies](https://openai.com/policies/usage-policies) require clear disclosure that TTS voice is AI-generated.

*   **Quickstart**
    ----------
    `/v1/audio/speech` endpoint takes 3 key inputs: `model`, `input` (text), `voice`.
    Generate spoken audio example:
    ```javascript
    // ... (JS code provided: calls openai.audio.speech.create, saves to file) ...
    ```
    ```python
    # ... (Python code provided: calls client.audio.speech.with_streaming_response.create, streams to file) ...
    ```
    ```bash
    # ... (Bash code provided: calls /v1/audio/speech, outputs to file) ...
    ```
    Default output is MP3. Configure other [supported formats](#supported-output-formats).

*   **Text-to-speech models:**
    *   `gpt-4o-mini-tts`: Newest, most reliable. Supports prompting via `instructions` parameter for accent, emotion, intonation, impressions, speed, tone, whispering.
    *   `tts-1`: Lower latency, lower quality.
    *   `tts-1-hd`: Higher quality than `tts-1`.

*   **Voice options:** 11 built-in voices (optimized for English). Hear samples at [OpenAI.fm](https://openai.fm).
    *   `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.
    *   (Note: Different voices available in [Realtime API](/docs/guides/realtime)).

*   **Streaming realtime audio:** Supported via chunk transfer encoding. Play audio before full file generated.
    Stream spoken audio example:
    ```javascript
    // ... (JS code provided: uses openai/helpers/audio playAudio helper) ...
    ```
    ```python
    # ... (Python code provided: uses with_streaming_response, plays with openai.helpers LocalAudioPlayer) ...
    ```
    ```bash
    # ... (Bash code provided: pipes output to ffplay) ...
    ```
    Recommend `wav` or `pcm` format for fastest streaming response times.

*   **Supported output formats**
    ------------------------
    *   `mp3` (default): General use.
    *   `opus`: Internet streaming/communication, low latency.
    *   `aac`: Digital audio compression (YouTube, Android, iOS).
    *   `flac`: Lossless compression, archiving.
    *   `wav`: Uncompressed, low latency (no decoding overhead).
    *   `pcm`: Raw samples (24kHz, 16-bit signed, little-endian), no header.

*   **Supported languages**
    -------------------
    Generally follows Whisper model support. Performs well despite voices being English-optimized. Supported languages list (Afrikaans, Arabic, ..., Welsh) provided. Generate by providing input text in the desired language. See [Whisper language list](https://github.com/openai/whisper#available-models-and-languages).

*   **Customization and ownership**
    ---------------------------
    *   **Custom voices:** Not supported. Cannot create copy of own voice.
    *   **Who owns the output?** Creator owns the output. Required to inform end users they are hearing AI audio.

    *(Was this page useful?)*

### Speech to text (General Guide)
==============

Learn how to turn audio into text.

The Audio API provides two speech to text endpoints:
*   `transcriptions`: `/v1/audio/transcriptions`
*   `translations`: `/v1/audio/translations`

Historically backed by open source [Whisper model](https://openai.com/blog/whisper/) (`whisper-1`). `transcriptions` now also supports higher quality snapshots: `gpt-4o-mini-transcribe`, `gpt-4o-transcribe` (limited parameters).

Endpoints can: Transcribe audio in original language; Translate and transcribe audio into English.
File uploads limited to 25 MB. Supported input types: `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, `webm`.

*   **Quickstart**
    ----------
    *   **Transcriptions:** Input audio file + output format. `whisper-1` supports `json`, `text`, `srt`, `verbose_json`, `vtt`. `gpt-4o*` snapshots support `json` or `text`. Default output is `json` with raw text.
        Transcribe audio example:
        ```javascript
        // ... (JS code provided: calls openai.audio.transcriptions.create) ...
        ```
        ```python
        # ... (Python code provided: calls client.audio.transcriptions.create) ...
        ```
        ```bash
        # ... (Bash code provided: calls /v1/audio/transcriptions) ...
        ```
        Default JSON response example: `{ "text": "Imagine the wildest idea..." }`
        Additional options example (requesting text format):
        ```javascript
        // ... (JS code provided: adds response_format: "text") ...
        ```
        ```python
        # ... (Python code provided: adds response_format="text") ...
        ```
        ```bash
        # ... (Bash code provided: adds -F response_format=text) ...
        ```
        See [API Reference](/docs/api-reference/audio) for full parameters. `gpt-4o*` snapshots currently limited (only `json`/`text` output format). Params like `timestamp_granularities` require `verbose_json` (only `whisper-1`).
    *   **Translations:** Input audio file (any supported language), outputs English transcription. Uses `whisper-1` only.
        Translate audio example:
        ```javascript
        // ... (JS code provided: calls openai.audio.translations.create) ...
        ```
        ```python
        # ... (Python code provided: calls client.audio.translations.create) ...
        ```
        ```bash
        # ... (Bash code provided: calls /v1/audio/translations) ...
        ```
        Example output (from German input): `Hello, my name is Wolfgang...`
        Only supports translation *into* English currently.

*   **Supported languages**
    -------------------
    Currently support languages listed [here](https://github.com/openai/whisper#available-models-and-languages) (Afrikaans, Arabic, ..., Welsh). List includes languages exceeding <50% WER benchmark. Model may return results for unlisted languages, but quality will be low. GPT-4o models support some ISO 639-1/3 codes; otherwise use prompt for language.

*   **Timestamps**
    ----------
    [`timestamp_granularities[]`](/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp_granularities) parameter enables structured timestamped JSON output (segment, word, or both). Requires `response_format="verbose_json"`. Useful for word-level precision.
    Timestamp options example (`whisper-1` only):
    ```javascript
    // ... (JS code provided: sets response_format: "verbose_json", timestamp_granularities: ["word"]) ...
    ```
    ```python
    # ... (Python code provided: sets response_format="verbose_json", timestamp_granularities=["word"]) ...
    ```
    ```bash
    # ... (Bash code provided: adds -F "timestamp_granularities[]=word", -F response_format="verbose_json") ...
    ```

*   **Longer inputs**
    -------------
    Default limit 25 MB. Break longer files into chunks (<25MB) or use compressed format. Avoid breaking mid-sentence. Use libraries like [PyDub](https://github.com/jiaaro/pydub) to split audio.
    PyDub example:
    ```python
    # ... (Python code provided: uses pydub to export first 10 mins of mp3) ...
    ```
    *(Disclaimer about 3rd party software provided)*

*   **Prompting**
    ---------
    Use [prompt](/docs/api-reference/audio/createTranscription#audio/createTranscription-prompt) parameter to improve transcript quality by giving context.
    Prompting example:
    ```javascript
    // ... (JS code provided: adds prompt string) ...
    ```
    ```python
    # ... (Python code provided: adds prompt string) ...
    ```
    ```bash
    # ... (Bash code provided: adds -F prompt="...") ...
    ```
    *   For `gpt-4o-transcribe` / `gpt-4o-mini-transcribe`: Use prompt like other GPT-4o models to give context. Examples: Correct specific words/acronyms; preserve context across segments (prompt with preceding transcript); encourage punctuation; keep filler words; specify writing style (e.g., simplified/traditional Chinese).
    *   For `whisper-1`: Prompt helps correct specific words. Model tries to match prompt style (capitalization, punctuation). Only considers final 224 tokens of prompt. Limited control vs. GPT models. See more examples in [improving reliability](#improving-reliability-1).

*   **Streaming transcriptions**
    ------------------------
    Two ways depending on use case:
    1.  **Streaming transcription of completed audio:** Use Transcription API with `stream=True`. Receives stream of [transcript events](/docs/api-reference/audio/transcript-text-delta-event) (`transcript.text.delta`, `transcript.text.done`). Not supported for `whisper-1`. Can include logprobs (`include[]`).
        Stream transcriptions example:
        ```javascript
        // ... (JS code provided: sets stream: true, iterates events) ...
        ```
        ```python
        # ... (Python code provided: sets stream=True, iterates events) ...
        ```
        ```bash
        # ... (Bash code provided: adds -F stream=True) ...
        ```
    2.  **Streaming transcription of ongoing audio:** Use Realtime API (`intent=transcription`). Connect via WebSocket. Send audio via `input_audio_buffer.append`. API sends `input_audio_buffer.committed` (with `item_id`, `previous_item_id` for ordering), `conversation.item.input_audio_transcription.delta`, `conversation.item.input_audio_transcription.completed`. See [Realtime transcription guide](/docs/guides/realtime-transcription).
        Example Realtime session setup JSON: *(JSON provided)*
        Example Realtime audio append JSON: *(JSON provided)*

*   **Improving reliability**
    ---------------------
    Common challenge: Misrecognition of uncommon words/acronyms (Whisper).
    *   **Using the prompt parameter:** Pass dictionary of correct spellings. Limited to first 224 prompt tokens (Whisper).
        Prompt parameter example:
        ```javascript
        // ... (JS code provided: sets long prompt with specific names/acronyms) ...
        ```
        ```python
        # ... (Python code provided: sets long prompt with specific names/acronyms) ...
        ```
        ```bash
        # ... (Bash code provided: sets long prompt with specific names/acronyms) ...
        ```
    *   **Post-processing with GPT-4:** Use GPT-4/3.5-Turbo to correct transcript. Provide system prompt with instructions and known correct spellings. More scalable due to larger context window, more reliable due to instruction following.
        Post-processing example:
        ```javascript
        // ... (JS code provided: defines system prompt, calls transcribe, calls chat completions with transcript) ...
        ```
        ```python
        # ... (Python code provided: defines system_prompt, defines function using transcribe and chat completions) ...
        ```

    *(Was this page useful?)*

### Vector embeddings (General Guide)
=================

Learn how to turn text into numbers, unlocking use cases like search.

New embedding models: `text-embedding-3-small` and `text-embedding-3-large` available. Lower cost, higher multilingual performance, controllable dimensions.

*   **What are embeddings?** OpenAIs text embeddings measure relatedness of text strings. Commonly used for: Search, Clustering, Recommendations, Anomaly detection, Diversity measurement, Classification. An embedding is a vector (list) of floating point numbers. Distance between vectors measures relatedness (small distance = high relatedness). [Pricing](https://openai.com/api/pricing/) based on input [tokens](/tokenizer).
*   **How to get embeddings:** Send text string + model name (e.g., `text-embedding-3-small`) to [/v1/embeddings](/docs/api-reference/embeddings) endpoint.
    Example: Getting embeddings
    ```javascript
    // ... (JS code provided: calls openai.embeddings.create) ...
    ```
    ```python
    # ... (Python code provided: calls client.embeddings.create) ...
    ```
    ```bash
    # ... (Bash code provided: calls /v1/embeddings) ...
    ```
    Response contains embedding vector + metadata. Extract vector, save in vector DB.
    Example response JSON: *(JSON provided showing embedding vector)*
    Default vector length: 1536 (`small`), 3072 (`large`). Use [`dimensions`](/docs/api-reference/embeddings/create#embeddings-create-dimensions) parameter to reduce length.

*   **Embedding models:** Third-generation models (`-3`). See [announcement blog post](https://openai.com/blog/new-embedding-models-and-api-updates). Priced per input token.
    | Model                   | ~ Pages per dollar | MTEB eval | Max input |
    |-------------------------|--------------------|-----------|-----------|
    | text-embedding-3-small  | 62,500             | 62.3%     | 8191      |
    | text-embedding-3-large  | 9,615              | 64.6%     | 8191      |
    | text-embedding-ada-002  | 12,500             | 61.0%     | 8191      |
    *(Assumes ~800 tokens/page)*

*   **Use cases:** Examples use [Amazon fine-food reviews dataset](https://www.kaggle.com/snap/amazon-fine-food-reviews) (subset of 1000).
    *   **Obtaining the embeddings:** Combine review summary + text. Encode combined text. Save embeddings.
        [Get\_embeddings\_from\_dataset.ipynb](https://cookbook.openai.com/examples/get_embeddings_from_dataset)
        ```python
        # ... (Python code provided: defines get_embedding, applies to df, saves df) ...
        # ... (Python code provided: loads df from csv, converts string embedding back to numpy array) ...
        ```
    *   **Reducing embedding dimensions:** Use `dimensions` API parameter (recommended). Or shorten manually + re-normalize (L2 norm). Technique allows trading off performance/cost. Example: Shorten `large` (3072) to 256 still outperforms `ada-002` (1536). Allows using better models with vector DBs having dimension limits (e.g., 1024).
        Manual shortening/normalization example:
        ```python
        # ... (Python code provided: defines normalize_l2, gets embedding, slices, normalizes) ...
        ```
    *   **Question answering using embeddings-based search:** Alternative to putting info directly in context window (RAG). Retrieve relevant documents based on query embedding similarity, then use LLM to answer based on retrieved docs.
        [Question\_answering\_using\_embeddings.ipynb](https://cookbook.openai.com/examples/question_answering_using_embeddings)
        *(Example using context window directly shown)*
    *   **Text search using embeddings:** Retrieve most relevant documents using cosine similarity between query embedding and document embeddings.
        [Semantic\_text\_search\_using\_embeddings.ipynb](https://cookbook.openai.com/examples/semantic_text_search_using_embeddings)
        ```python
        # ... (Python code provided: defines search_reviews using get_embedding and cosine_similarity) ...
        ```
    *   **Code search using embeddings:** Embed functions, embed natural language query, find most similar functions via cosine similarity.
        [Code\_search.ipynb](https://cookbook.openai.com/examples/code_search_using_embeddings)
        ```python
        # ... (Python code provided: defines search_functions using get_embedding and cosine_similarity) ...
        ```
    *   **Recommendations using embeddings:** Rank items by embedding similarity to a source item. Example using AG news dataset.
        [Recommendation\_using\_embeddings.ipynb](https://cookbook.openai.com/examples/recommendation_using_embeddings)
        ```python
        # ... (Python code provided: defines recommendations_from_strings using distances_from_embeddings and indices_of_nearest_neighbors_from_distances from embeddings_utils) ...
        ```
    *   **Data visualization in 2D:** Use t-SNE to reduce high-dimensional embeddings to 2D for plotting. Example colors Amazon reviews by star rating, showing clusters.
        [Visualizing\_embeddings\_in\_2D.ipynb](https://cookbook.openai.com/examples/visualizing_embeddings_in_2d)
        *(t-SNE visualization image provided)*
        ```python
        # ... (Python code provided: uses TSNE, matplotlib to plot embeddings colored by score) ...
        ```
    *   **Embedding as a text feature encoder for ML algorithms:** Use embeddings as features for ML models (regression, classification). Improves performance if relevant inputs are free text. Can also encode categorical features (good for numerous, meaningful categories like job titles). Reducing dimensionality (PCA/SVD) usually hurts performance.
        Code splits data into train/test sets:
        ```python
        # ... (Python code provided: uses train_test_split) ...
        ```
        *   **Regression using the embedding features:** Predict numerical value (e.g., review score) from text embedding. Example predicts score (1-5 continuous), achieves MAE 0.39.
            [Regression\_using\_embeddings.ipynb](https://cookbook.openai.com/examples/regression_using_embeddings)
            ```python
            # ... (Python code provided: uses RandomForestRegressor) ...
            ```
        *   **Classification using the embedding features:** Predict discrete class (e.g., exact star rating 1-5) from text embedding. Example shows model better at predicting extremes (1/5 stars).
            [Classification\_using\_embeddings.ipynb](https://cookbook.openai.com/examples/classification_using_embeddings)
            ```python
            # ... (Python code provided: uses RandomForestClassifier) ...
            ```
    *   **Zero-shot classification:** Classify text without labeled training data. Embed class names/descriptions. Compare new text embedding to class embeddings, predict class with highest similarity.
        [Zero-shot\_classification\_with\_embeddings.ipynb](https://cookbook.openai.com/examples/zero-shot_classification_with_embeddings)
        ```python
        # ... (Python code provided: defines labels, gets label embeddings, defines label_score using cosine_similarity, makes prediction) ...
        ```
    *   **Obtaining user and product embeddings for cold-start recommendation:** Average embeddings of user's reviews for user embedding; average embeddings of product reviews for product embedding. Evaluate by plotting user/product embedding similarity vs. rating on test set. Shows predictive power even before user receives product.
        [User\_and\_product\_embeddings.ipynb](https://cookbook.openai.com/examples/user_and_product_embeddings)
        *(Boxplot image provided)*
        ```python
        # ... (Python code provided: uses groupby().apply(np.mean)) ...
        ```
    *   **Clustering:** Unsupervised grouping of textual data using embeddings. Example finds clusters in Amazon reviews (dog food, negative reviews, positive reviews).
        [Clustering.ipynb](https://cookbook.openai.com/examples/clustering)
        *(Clustering visualization image provided)*
        ```python
        # ... (Python code provided: uses KMeans) ...
        ```

*   **FAQ**
    ---
    *   **How can I tell how many tokens a string has?** Use OpenAI's [`tiktoken`](https://github.com/openai/tiktoken) library. Use `cl100k_base` encoding for v3 models. See [cookbook guide](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken). *(Python code provided)*
    *   **How can I retrieve K nearest embedding vectors quickly?** Use a vector database. See [Cookbook examples](https://cookbook.openai.com/examples/vector_databases/readme).
    *   **Which distance function should I use?** Recommend [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). Since OpenAI embeddings are normalized to length 1, cosine similarity and Euclidean distance give identical rankings; cosine can be computed faster via dot product.
    *   **Can I share my embeddings online?** Yes, customers own input/output. Ensure content does not violate laws or [Terms of Use](https://openai.com/policies/terms-of-use).
    *   **Do V3 embedding models know about recent events?** No, knowledge cutoff September 2021. Usually not a limitation for embeddings, but may affect edge cases.

    *(Was this page useful?)*

```markdown
*(... Content from the previous responses up to here ...)*

### Moderation (General Guide)
==========

Identify potentially harmful content in text and images.

Use the [moderations](/docs/api-reference/moderations) endpoint to check whether text or images are potentially harmful. If harmful content is identified, you can take corrective action, like filtering content or intervening with user accounts creating offending content. The moderation endpoint is free to use.

You can use two models for this endpoint:
*   `omni-moderation-latest`: This model and all snapshots support more categorization options and multi-modal inputs.
*   `text-moderation-latest` **(Legacy)**: Older model that supports only text inputs and fewer input categorizations. The newer omni-moderation models will be the best choice for new applications.

*   **Quickstart**
    ----------
    Use the tabs below to see how you can moderate text inputs or image inputs, using our [official SDKs](/docs/libraries) and the [omni-moderation-latest model](/docs/models#moderation):

    *   **Moderate text inputs:** Get classification information for a text input
        ```python
        # ... (Python code provided: calls client.moderations.create with model, input text) ...
        ```
        ```javascript
        // ... (JS code provided: calls openai.moderations.create with model, input text) ...
        ```
        ```bash
        # ... (Bash code provided: calls /v1/moderations with model, input text) ...
        ```
    *   **Moderate images and text:** Get classification information for image and text input
        ```python
        # ... (Python code provided: calls client.moderations.create with model, list input [text dict, image_url dict]) ...
        ```
        ```javascript
        // ... (JS code provided: calls openai.moderations.create with model, array input [text object, image_url object]) ...
        ```
        ```bash
        # ... (Bash code provided: calls /v1/moderations with model, JSON array input [text object, image_url object]) ...
        ```

    Here's a full example output, where the input is an image from a single frame of a war movie. The model correctly predicts indicators of violence in the image, with a `violence` category score of greater than 0.8.
    ```json
    {
      "id": "modr-970d409ef3bef3b70c73d8232df86e7d",
      "model": "omni-moderation-latest",
      "results": [
        {
          "flagged": true,
          "categories": {
            "sexual": false,
            "sexual/minors": false,
            "harassment": false,
            "harassment/threatening": false,
            "hate": false,
            "hate/threatening": false,
            "illicit": false,
            "illicit/violent": false,
            "self-harm": false,
            "self-harm/intent": false,
            "self-harm/instructions": false,
            "violence": true,
            "violence/graphic": false
          },
          "category_scores": {
            "sexual": 2.34135824776394e-7,
            "sexual/minors": 1.6346470245419304e-7,
            "harassment": 0.0011643905680426018,
            "harassment/threatening": 0.0022121340080906377,
            "hate": 3.1999824407395835e-7,
            "hate/threatening": 2.4923252458203563e-7,
            "illicit": 0.0005227032493135171,
            "illicit/violent": 3.682979260160596e-7,
            "self-harm": 0.0011175734280627694,
            "self-harm/intent": 0.0006264858507989037,
            "self-harm/instructions": 7.368592981140821e-8,
            "violence": 0.8599265510337075,
            "violence/graphic": 0.37701736389561064
          },
          "category_applied_input_types": {
            "sexual": [ "image" ],
            "sexual/minors": [],
            "harassment": [],
            "harassment/threatening": [],
            "hate": [],
            "hate/threatening": [],
            "illicit": [],
            "illicit/violent": [],
            "self-harm": [ "image" ],
            "self-harm/intent": [ "image" ],
            "self-harm/instructions": [ "image" ],
            "violence": [ "image" ],
            "violence/graphic": [ "image" ]
          }
        }
      ]
    }
    ```
    The output has several categories in the JSON response:
    | Field                           | Description                                                                                                                                                           |
    |---------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | `flagged`                       | Set to true if the model classifies the content as potentially harmful, false otherwise.                                                                               |
    | `categories`                    | Contains a dictionary of per-category violation flags. For each category, the value is true if the model flags the corresponding category as violated, false otherwise. |
    | `category_scores`               | Contains a dictionary of per-category scores output by the model, denoting the model's confidence that the input violates the OpenAI's policy for the category. Value is 0-1. |
    | `category_applied_input_types`  | Contains information on which input types were flagged for each category (e.g., `["image", "text"]`). Only available on omni models.                                   |
    We plan to continuously upgrade the moderation endpoint's underlying model. Therefore, custom policies that rely on `category_scores` may need recalibration over time.

*   **Content classifications**
    -----------------------
    The table below describes the types of content that can be detected, which models support them, and applicable input types. Categories marked "Text only" return score 0 for image-only inputs to `omni-moderation-latest`.
    | Category                 | Description                                                                                                                                                           | Supported Models | Input Types      |
    |--------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|------------------|
    | `harassment`             | Content that expresses, incites, or promotes harassing language towards any target.                                                                                  | All              | Text only        |
    | `harassment/threatening`| Harassment content that also includes violence or serious harm towards any target.                                                                                   | All              | Text only        |
    | `hate`                   | Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harassment. | All              | Text only        |
    | `hate/threatening`      | Hateful content that also includes violence or serious harm towards the targeted group based on the protected characteristics.                                            | All              | Text only        |
    | `illicit`                | Content that gives advice or instruction on how to commit illicit acts (e.g., "how to shoplift").                                                                         | Omni only        | Text only        |
    | `illicit/violent`        | Illicit content that also includes references to violence or procuring a weapon.                                                                                         | Omni only        | Text only        |
    | `self-harm`              | Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.                                                         | All              | Text and images  |
    | `self-harm/intent`       | Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm.                                                                    | All              | Text and images  |
    | `self-harm/instructions` | Content that encourages performing acts of self-harm, or gives instructions/advice on how to commit such acts.                                                         | All              | Text and images  |
    | `sexual`                 | Content meant to arouse sexual excitement, description of sexual activity, or promotes sexual services (excluding sex education/wellness).                            | All              | Text and images  |
    | `sexual/minors`          | Sexual content that includes an individual who is under 18 years old.                                                                                                 | All              | Text only        |
    | `violence`               | Content that depicts death, violence, or physical injury.                                                                                                              | All              | Text and images  |
    | `violence/graphic`       | Content that depicts death, violence, or physical injury in graphic detail.                                                                                            | All              | Text and images  |

    *(Was this page useful?)*

### Retrieval (General Guide)
=========

Search your data using semantic similarity.

The **Retrieval API** allows you to perform [**semantic search**](#semantic-search-1) over your data, which is a technique that surfaces semantically similar results  even when they match few or no keywords. Retrieval is useful on its own, but is especially powerful when combined with our models to synthesize responses.
*(Retrieval depiction image provided)*
The Retrieval API is powered by [**vector stores**](#vector-stores-1), which serve as indices for your data. This guide will cover how to perform semantic search, and go into the details of vector stores.

*   **Quickstart**
    ----------
    1.  **Create vector store** and upload files.
        Create vector store with files example:
        ```python
        # ... (Python code provided: creates vector store, uploads file using upload_and_poll) ...
        ```
        ```javascript
        // ... (JS code provided: creates vector store, uploads file using uploadAndPoll) ...
        ```
    2.  **Send search query** to get relevant results.
        Search query example:
        ```python
        # ... (Python code provided: calls client.vector_stores.search) ...
        ```
        ```javascript
        // ... (JS code provided: calls client.vectorStores.search) ...
        ```
    To learn how to use results with models, see [synthesizing responses](#synthesizing-responses-1).

*   **Semantic search**
    ---------------
    **Semantic search** leverages [vector embeddings](/docs/guides/embeddings) to surface semantically relevant results, including those with few/no shared keywords.
    Example query `"When did we go to the moon?"`:
    | Text                                            | Keyword Similarity | Semantic Similarity |
    |-------------------------------------------------|--------------------|---------------------|
    | The first lunar landing occured in July of 1969.  | 0%                 | 65%                 |
    | The first man on the moon was Neil Armstrong.     | 27%                | 43%                 |
    | When I ate the moon cake, it was delicious.     | 40%                | 28%                 |
    *([Jaccard](https://en.wikipedia.org/wiki/Jaccard_index) used for keyword, [cosine](https://en.wikipedia.org/wiki/Cosine_similarity) with `text-embedding-3-small` used for semantic.)*
    Powered by [vector stores](#vector-stores-1).
    *   **Performing semantic search:** Query vector store using `search` function with natural language `query`. Returns list of results (chunks, scores, file origin).
        Search query example:
        ```python
        # ... (Python code provided: calls client.vector_stores.search) ...
        ```
        ```javascript
        // ... (JS code provided: calls client.vectorStores.search) ...
        ```
        Results example JSON: *(JSON provided showing search results with file_id, filename, score, attributes, content chunks)*
        Returns 10 results max by default, configurable up to 50 via `max_num_results`.
    *   **Query rewriting:** Set `rewrite_query=true` in `search` call to automatically rewrite query for optimal performance. Rewritten query in `search_query` field of result.
        | Original                                                      | Rewritten                               |
        |---------------------------------------------------------------|-----------------------------------------|
        | I'd like to know the height of the main office building.      | primary office building height          |
        | What are the safety regulations for transporting hazardous materials? | safety regulations for hazardous materials |
        | How do I file a complaint about a service issue?              | service complaint filing process        |
    *   **Attribute filtering:** Narrow results using `attribute_filter`. Define criteria on file `attributes`. Use comparison filters (`eq`, `ne`, `gt`, `gte`, `lt`, `lte`) and compound filters (`and`, `or`).
        Comparison filter structure: `{ "type": "op", "property": "key", "value": "val" }`
        Compound filter structure: `{ "type": "and"|"or", "filters": [...] }`
        Example filters provided for: Region, Date range (unix timestamps), Filenames (using `or`), Complex combination (project codes + confidentiality OR language).
    *   **Ranking:** Adjust `ranking_options` to improve relevance. Specify `ranker` (`auto`, `default-2024-08-21`) and `score_threshold` (0.0-1.0). Higher threshold = more relevant, potentially fewer results.

*   **Vector stores**
    -------------
    Containers powering semantic search (Retrieval API, Assistants API file search tool). Adding a file automatically chunks, embeds, indexes it. Contain `vector_store_file` objects, backed by `file` objects.
    | Object type         | Description                                                               |
    |---------------------|---------------------------------------------------------------------------|
    | `file`              | Content uploaded via Files API (used for vector stores, fine-tuning, etc.) |
    | `vector_store`      | Container for searchable files.                                           |
    | `vector_store.file` | File chunked/embedded, associated with a `vector_store`. Contains attributes. |
    *   **Pricing:** Charged based on total storage across all stores (parsed chunks + embeddings).
        | Storage                      | Cost             |
        |------------------------------|------------------|
        | Up to 1 GB (across all stores) | Free             |
        | Beyond 1 GB                  | $0.10/GB/day     |
        See [expiration policies](#expiration-policies-1) to minimize costs.
    *   **Vector store operations:** Examples provided for Create, Retrieve, Update, Delete, List using Python/JS SDKs (`client.vector_stores.*`).
    *   **Vector store file operations:** Examples provided for Create (`create_and_poll`), Upload (`upload_and_poll`), Retrieve, Update (attributes), Delete, List using Python/JS SDKs (`client.vector_stores.files.*`). Note `create`/`upload` are async; use `_and_poll` helpers to block until ready or check status manually.
    *   **Batch operations:** Examples provided for Create (`create_and_poll`), Retrieve, Cancel, List using Python/JS SDKs (`client.vector_stores.file_batches.*`).
    *   **Attributes:** `vector_store.file` can have `attributes` dictionary (max 16 keys, max 256 chars/key). Used for filtering. Example provided for creating file with attributes.
    *   **Expiration policies:** Set `expires_after` on `vector_store` (via create/update). Deletes associated files automatically after specified days of inactivity (`anchor: "last_active_at"`). Example provided.
    *   **Limits:** Max file size 512MB (~5M tokens/file).
    *   **Chunking:** Default `max_chunk_size_tokens=800`, `chunk_overlap_tokens=400`. Adjust via `chunking_strategy` when adding files. Limits: max chunk size 100-4096, overlap >= 0 and <= max_chunk_size / 2.
    *   **Supported file types:** List provided (.c, .cpp, .docx, .html, .java, .json, .md, .pdf, .php, .pptx, .py, .rb, .tex, .txt, etc.) and MIME types. Text files need utf-8, utf-16, or ascii.

*   **Synthesizing responses**
    ----------------------
    Combine search results with original query to generate a grounded response using LLMs.
    1.  Perform search query to get results. *(Python/JS code provided)*
    2.  Format results (example function `format_results` provided using XML-like tags).
    3.  Call Chat Completions/Responses API with prompt including formatted results and original query. *(Python/JS code provided)*
    Example synthesized response: `"Our return policy allows returns within 30 days of purchase."`

    *(Was this page useful?)*

### Batch API (General Guide)
=========

Process jobs asynchronously with Batch API.

Learn how to use OpenAI's Batch API to send asynchronous groups of requests with 50% lower costs, a separate pool of significantly higher rate limits, and a clear 24-hour turnaround time. The service is ideal for processing jobs that don't require immediate responses. You can also [explore the API reference directly here](/docs/api-reference/batch).

*   **Overview**
    --------
    Useful when immediate response not needed or rate limits constrain synchronous calls. Ideal use cases: Evaluations, Classifying large datasets, Embedding content repositories.
    Endpoints allow: Collect requests in file -> Start batch job -> Query status -> Retrieve results.
    Compared to standard endpoints:
    *   **Better cost efficiency:** 50% discount.
    *   **Higher rate limits:** [Separate, higher limits](/settings/organization/limits).
    *   **Fast completion times:** Within 24 hours (often faster).

*   **Getting started**
    ---------------
    1.  **Prepare your batch file:** `.jsonl` file, each line = one API request. Available endpoints: `/v1/responses`, `/v1/chat/completions`, `/v1/embeddings`, `/v1/completions`. Line format: `{"custom_id": "unique-id", "method": "POST", "url": "/v1/endpoint", "body": { ... endpoint parameters ... }}`. Each input file uses only one model.
        Example `batchinput.jsonl`:
        ```jsonl
        {"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "system", "content": "You are a helpful assistant."},{"role": "user", "content": "Hello world!"}],"max_tokens": 1000}}
        {"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-3.5-turbo-0125", "messages": [{"role": "system", "content": "You are an unhelpful assistant."},{"role": "user", "content": "Hello world!"}],"max_tokens": 1000}}
        ```
    2.  **Upload your batch input file:** Use [Files API](/docs/api-reference/files) with `purpose="batch"`. Get `file_id`.
        Upload files example: *(JS/Python/Bash code provided)*
    3.  **Create the batch:** POST to `/v1/batches`. Provide `input_file_id`, `endpoint` (e.g., `/v1/chat/completions`), `completion_window` (`"24h"`). Optional `metadata`. Returns [Batch object](/docs/api-reference/batch/object).
        Create batch example: *(JS/Python/Bash code provided)*
        Example Batch object response: *(JSON provided)*
    4.  **Check the status of a batch:** GET `/v1/batches/{batch_id}`. Returns Batch object.
        Check status example: *(JS/Python/Bash code provided)*
        Batch object statuses: `validating`, `failed`, `in_progress`, `finalizing`, `completed`, `expired`, `cancelling`, `cancelled`.
    5.  **Retrieve the results:** Once `status` is `completed`, download output file using `output_file_id` via Files API content endpoint (GET `/v1/files/{file_id}/content`). Errors logged to file referenced by `error_file_id`.
        Retrieve results example: *(JS/Python/Bash code provided)*
        Output `.jsonl` file: One line per successful request. Order may differ from input; use `custom_id` to match request/response.
        Example `batch_output.jsonl`:
        ```jsonl
        {"id": "batch_req_123", "custom_id": "request-2", "response": {"status_code": 200, "request_id": "req_123", "body": { ... chat completion object ... }}, "error": null}
        {"id": "batch_req_456", "custom_id": "request-1", "response": {"status_code": 200, "request_id": "req_789", "body": { ... chat completion object ... }}, "error": null}
        ```
        Output file deleted 30 days after batch completion.
    6.  **Cancel a batch:** POST to `/v1/batches/{batch_id}/cancel`. Status becomes `cancelling` (up to 10 mins), then `cancelled`.
        Cancel batch example: *(JS/Python/Bash code provided)*
    7.  **Get a list of all batches:** GET `/v1/batches`. Use `limit`, `after` for pagination.
        List batches example: *(JS/Python/Bash code provided)*

*   **Model availability**
    ------------------
    Widely available, but check [model reference docs](/docs/models) for specific support.

*   **Rate limits**
    -----------
    Separate from standard limits. Two types:
    1.  **Per-batch limits:** Max 50,000 requests/batch, max 200MB input file size. Embeddings batches max 50,000 *inputs* total.
    2.  **Enqueued prompt tokens per model:** Limit on total prompt tokens across all queued batches per model. See [Platform Settings](/settings/organization/limits).
    No output token or submitted request limits currently. Does not consume standard rate limits.

*   **Batch expiration**
    ----------------
    Batches not completed in 24h move to `expired` status. Unfinished requests cancelled. Completed request results available in output file. Charged for tokens consumed by completed requests. Expired requests logged in error file with code `batch_expired`.

*   **Other resources**
    ---------------
    [OpenAI Cookbook Batch Processing Examples](https://cookbook.openai.com/examples/batch_processing) (classification, sentiment analysis, summary generation).

    *(Was this page useful?)*

### Prompt generation (Playground Feature)
=================

Generate prompts and schemas in Playground.

The **Generate** button in the [Playground](/playground/prompts) lets you generate prompts, [functions](/docs/guides/function-calling), and [schemas](/docs/guides/structured-outputs#supported-schemas) from just a description of your task. This guide will walk through exactly how it works.

*   **Overview:** Generates prompts/schemas from task descriptions to speed up development. Uses:
    1.  **Prompts:** Meta-prompts incorporating best practices.
    2.  **Schemas:** Meta-schemas producing valid JSON/function syntax.
    May integrate advanced techniques (DSPy, "Gradient Descent") in future.

*   **Prompts:**
    *   **Meta-prompt:** Instructs model to create/improve prompts based on task description. Draws from [prompt engineering guide](/docs/guides/prompt-engineering) and user experience. Specific meta-prompts for different output types (e.g., audio).
    *   **Meta-prompts examples:**
        *   Text-out: *(Python code for text meta-prompt provided, including guidelines on understanding task, minimal changes, reasoning before conclusions, examples, clarity, formatting, preserving user content, constants, output format)*. Defines `generate_prompt(task_or_prompt)` function.
        *   Audio-out: *(Python code for audio meta-prompt provided, similar structure but emphasizes short, conversational, emotive, fast audio output constraints and multi-turn examples)*. Defines `generate_prompt(task_or_prompt)` function.
    *   **Prompt edits:** Uses modified meta-prompt including `<reasoning>` section at start. Model analyzes existing prompt (clarity, CoT order, structure, examples, complexity, specificity), suggests changes, then outputs edited prompt. *(Python code for text edit meta-prompt and audio edit meta-prompt provided)*.

*   **Schemas:**
    *   Uses [Structured Outputs](/guides/structured-outputs) to generate JSON schemas / function schemas. Requires meta-schema (schema describing a schema).
    *   **Defining constrained meta-schema:** Goal is to generate schemas usable with `strict=true` Structured Outputs, using strict mode itself. Challenge: Official JSON meta-schemas use features unsupported in strict mode. Solution: Define **pseudo-meta-schema** (uses unsupported features *only* to describe features *supported* by strict mode). Allows generating strict-compatible schemas without using strict mode for generation itself.
        *   *Deep dive on pseudo-meta-schema design provided in original text.*
    *   **Output cleaning:** Since generation isn't in strict mode, post-process generated schema: Set `additionalProperties: false` everywhere, mark all properties `required`, wrap in appropriate object (`json_schema` or `function`).
    *   **Meta-schemas examples:** Include prompts with few-shot examples. Allows using `gpt-4o-mini`.
        *   Structured output schema: *(Python code provided showing META_SCHEMA for schema generation and META_PROMPT with instructions/examples)*. Defines `generate_schema(description)`.
        *   Function schema: *(Python code provided showing META_SCHEMA for function generation and META_PROMPT with instructions/examples)*. Defines `generate_function_schema(description)`.

    *(Was this page useful?)*

```markdown
*(... Content from the previous responses up to here ...)*

### Production best practices (General Guide)
=========================

Transition AI projects to production with best practices.

This guide provides a comprehensive set of best practices to help you transition from prototype to production. Whether you are a seasoned machine learning engineer or a recent enthusiast, this guide should provide you with the tools you need to successfully put the platform to work in a production setting: from securing access to our API to designing a robust architecture that can handle high traffic volumes. Use this guide to help develop a plan for deploying your application as smoothly and effectively as possible.

If you want to explore best practices for going into production further, please check out our Developer Day talk: *(Video link not provided in text)*

*   **Setting up your organization**
    ----------------------------
    Once you [log in](/login) to your OpenAI account, you can find your organization name and ID in your [organization settings](/settings/organization/general). The organization name is the label for your organization, shown in user interfaces. The organization ID is the unique identifier for your organization which can be used in API requests.
    Users who belong to multiple organizations can [pass a header](/docs/api-reference/requesting-organization) to specify which organization is used for an API request. Usage from these API requests will count against the specified organization's quota. If no header is provided, the [default organization](/settings/organization/api-keys) will be billed. You can change your default organization in your [user settings](/settings/organization/api-keys).
    You can invite new members to your organization from the [Team page](/settings/organization/team). Members can be **readers** or **owners**.
    *   **Readers:** Can make API requests, view basic org info, create/update/delete resources (like Assistants) unless noted otherwise.
    *   **Owners:** Have all reader permissions, can modify billing info, manage members.

*   **Managing billing limits:**
    *   Enter [billing information](/settings/organization/billing/overview) to use the API.
    *   Initial approved usage limit: $100/month (set by OpenAI). Limit increases automatically with usage tiers. Review current limit in [limits page](/settings/organization/limits).
    *   Set notification threshold on [usage limits page](/settings/organization/limits) to get email alert when usage exceeds amount. Owners receive notification.
    *   Set monthly budget on [usage limits page](/settings/organization/limits) to reject subsequent API requests once reached. Note: Limits enforced with ~5-10 min delay.

*   **API keys:**
    *   Used for authentication. Retrieve from [API keys page](/settings/organization/api-keys).
    *   **Security:** Must secure keys. Avoid exposing in code/public repos. Store securely. Use environment variables or secret management service. Read [Best practices for API key safety](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).
    *   **Usage monitoring:** Monitor key usage on [Usage page](/usage) (requires tracking enabled). Enable tracking for older keys (pre Dec 20, 2023) on [API key management dashboard](/api-keys). Keys post Dec 20, 2023 have tracking enabled. Untracked usage shown as `Untracked`.

*   **Staging projects:**
    *   Create separate projects (via dashboard) for staging/production environments.
    *   Isolates dev/test work. Limit access to production project. Set custom rate/spend limits per project.

*   **Scaling your solution architecture**
    ----------------------------------
    Consider how to scale to meet traffic demands. Key areas:
    *   **Horizontal scaling:** Add more servers/containers to distribute load. Ensure architecture handles multiple nodes, load balancing in place.
    *   **Vertical scaling:** Increase resources (CPU, RAM) of single nodes. Ensure application can utilize additional resources.
    *   **Caching:** Store frequently accessed data (e.g., common API responses) to improve response times. Use DB, filesystem, in-memory cache. Invalidate cache when data changes.
    *   **Load balancing:** Distribute requests evenly across servers (load balancer hardware/service, DNS round-robin). Improves performance, reduces bottlenecks.

*   **Managing rate limits:** Understand and plan for [rate limits](/docs/guides/rate-limits).

*   **Improving latencies**
    -------------------
    Check out our most up-to-date guide on [latency optimization](/docs/guides/latency-optimization).
    Latency = time for request processing + response return. Influenced mainly by model and number of generated tokens.
    Lifecycle of completion request: Network (User->API) -> Server (Process prompt tokens) -> Server (Generate completion tokens) -> Network (API->User).
    Bulk of latency usually in token generation (tokens generated one by one). Prompt tokens add little latency.
    *   **Common factors affecting latency & mitigation:** (Ordered roughly by impact)
        *   **Model:** More capable models (`gpt-4`) are slower than smaller ones (`gpt-4o-mini`). Choose based on speed/cost/quality trade-off.
        *   **Number of completion tokens:** More tokens = higher latency.
            *   *Lower max tokens:* Use lower `max_tokens` if possible.
            *   *Include stop sequences:* Prevent generating unneeded tokens (e.g., stop list generation at item 11 with `stop="11."`). See [stop sequences help article](https://help.openai.com/en/articles/5072263-how-do-i-use-stop-sequences).
            *   *Generate fewer completions:* Lower `n` (number of completions) and `best_of` (candidates generated). Default `n=1`, `best_of=1` generates at most `max_tokens`. If `n` or `best_of > 1`, tokens generated  `max_tokens * max(n, best_of)`.
        *   **Streaming:** Set `stream: true`. Returns tokens as available, reducing time-to-first-token. Improves user experience.
        *   **Infrastructure:** OpenAI servers currently in US. Locate your relevant infrastructure in US to minimize network roundtrip time if possible.
        *   **Batching:** Sending multiple prompts in one request ([batching prompts](/docs/guides/rate-limits#batching-requests), up to 20). May help by reducing request overhead, but test carefully as total generation time might increase.

*   **Managing costs**
    --------------
    *   Monitor costs via [Usage dashboard](/settings/organization/usage).
    *   Set [notification threshold](/settings/organization/limits) and [monthly budget](/settings/organization/limits) (beware potential disruption).
    *   **Text generation:** Pay-as-you-go per 1k tokens (~750 words). Estimate costs based on traffic, interaction frequency, data volume.
    *   **Cost reduction framework:** Cost = (Tokens) * (Cost/Token).
        *   Reduce Cost/Token: Use smaller/cheaper models for some tasks.
        *   Reduce Tokens: Shorter prompts, [fine-tuning](/docs/guides/fine-tuning), caching common queries.
    *   Use [tokenizer tool](/tokenizer) for estimation. API/Playground responses include token counts. Experiment with cheaper models after achieving functionality. See [token usage help article](https://help.openai.com/en/articles/6614209-how-do-i-check-my-token-usage).

*   **MLOps strategy**
    --------------
    Consider managing end-to-end ML model lifecycle. Areas:
    *   **Data and model management:** Manage training/fine-tuning data, track versions/changes.
    *   **Model monitoring:** Track performance over time, detect issues/degradation.
    *   **Model retraining:** Keep model updated (data changes, evolving requirements), retrain/fine-tune as needed.
    *   **Model deployment:** Automate deployment of model and artifacts.

*   **Security and compliance**
    -----------------------
    Assess requirements. Examine data handling, API processing, regulations.
    *   Refer to OpenAI [security practices](https://www.openai.com/security) and [trust and compliance portal](https://trust.openai.com/). See [Privacy Policy](https://openai.com/privacy/) and [Terms of Use](https://openai.com/api/policies/terms/).
    *   Consider: Data storage, transmission, retention. Implement privacy protections (encryption, anonymization). Follow secure coding best practices (input sanitization, error handling).
    *   **Safety best practices:** Refer to [safety best practices guide](/docs/guides/safety-best-practices).

*   **Business considerations**
    -----------------------
    Consider product value and business impact. See Developer Day talk video link *(Link not provided)*.

    *(Was this page useful?)*

### Safety best practices (General Guide)
=====================

Implement safety measures like moderation and human oversight.

*   **Use our free Moderation API:** [Moderation API](/docs/guides/moderation) helps reduce unsafe content frequency. Alternatively, develop own filtration system.
*   **Adversarial testing:** "Red-team" your application with wide range of inputs (representative and adversarial) to test robustness against prompt injections, off-topic drift, etc.
*   **Human in the loop (HITL):** Recommend human review before outputs used, especially in high-stakes domains and code generation. Humans need awareness of limitations and access to verification info (e.g., original notes for summaries).
*   **Prompt engineering:** Constrain topic/tone of output. Use context/examples to steer model.
*   **Know your customer (KYC):** Require user registration/login. Linking to existing accounts (Gmail, LinkedIn) or requiring payment/ID can reduce risk.
*   **Constrain user input and limit output tokens:** Limit prompt input length (helps vs. injection). Limit output tokens (reduces misuse chance). Narrow input/output ranges (e.g., validated dropdowns vs. open text, routing to existing articles vs. generating from scratch).
*   **Allow users to report issues:** Provide easy method (email, ticketing) for users to report problems. Monitor and respond.
*   **Understand and communicate limitations:** Models can hallucinate, be offensive, biased. Not suitable for all use cases without modification. Evaluate performance on wide range of inputs. Calibrate customer expectations.
*   **Report issues:** Submit safety/security issues via [Coordinated Vulnerability Disclosure Program](https://openai.com/security/disclosure/).

*   **End-user IDs**
    ------------
    Sending end-user IDs helps OpenAI monitor/detect abuse, enabling more actionable feedback on policy violations.
    *   IDs should be unique string per user. Recommend hashing username/email (avoid sending PII). Use session ID for non-logged-in users.
    *   Include via `user` parameter in API requests.
        Example: Providing a user identifier
        ```python
        # ... (Python code provided: adds user="user_123456" to completions.create) ...
        ```
        ```bash
        # ... (Bash code provided: adds "user": "user123456" to JSON data) ...
        ```

    *(Was this page useful?)*

### Prompt caching (General Guide)
==============

Reduce latency and cost with prompt caching.

Model prompts often contain repetitive content. OpenAI routes API requests to servers that recently processed the same prompt, making it cheaper (up to 50%) and faster (up to 80% latency reduction for long prompts). Works automatically on all API requests (no code changes) for recent models (`gpt-4o`+). No extra fees. This guide explains how it works for optimization.

*   **Structuring prompts**
    -------------------
    Cache hits require exact prefix matches. Place static content (instructions, examples) at the beginning, variable content (user info) at the end. Applies to images and tools too (must be identical).
    *(Prompt Caching visualization image provided)*

*   **How it works**
    ------------
    Automatic for prompts >= 1024 tokens.
    1.  **Cache Lookup:** System checks if prompt prefix is cached.
    2.  **Cache Hit:** Uses cached result for prefix. Reduces latency/cost.
    3.  **Cache Miss:** Processes full prompt. Caches prefix for future requests.
    Cached prefixes active ~5-10 mins inactivity (up to 1 hour off-peak).

*   **Requirements**
    ------------
    *   Enabled for prompts >= 1024 tokens.
    *   Cache hits occur in 128-token increments (cached tokens = 1024, 1152, 1280, ...).
    *   All requests (even < 1024 tokens) return `cached_tokens` field in `usage.prompt_tokens_details` (part of [Chat Completions object](/docs/api-reference/chat/object)). Value is 0 if < 1024 tokens or no hit.
        Example usage object:
        ```json
        // ... (JSON showing usage with prompt_tokens_details.cached_tokens) ...
        ```

*   **What can be cached**
    ------------------
    *   **Messages:** Full array (system, user, assistant).
    *   **Images:** In user messages (links or base64). `detail` parameter must match.
    *   **Tool use:** Messages array and `tools` list.
    *   **Structured outputs:** Schema (serves as prefix) can be cached.

*   **Best practices**
    ----------------
    *   Structure prompts: Static content first, dynamic last.
    *   Monitor metrics: Cache hit rates, latency, % tokens cached.
    *   Increase hits: Use longer prompts, make requests during off-peak.
    *   Maintain consistent requests with same prefix to minimize cache evictions (unused prompts automatically removed).

*   **Frequently asked questions**
    --------------------------
    1.  **Data privacy?** Caches not shared between orgs. Only members of same org access cache for identical prompts.
    2.  **Affects output?** No. Caching only affects prompt processing, not output token generation or final response content. Output identical regardless of cache hit.
    3.  **Manual cache clearing?** Not currently available. Automatic eviction based on inactivity (5-10 mins typical, up to 1 hour off-peak).
    4.  **Extra cost?** No. Automatic, no extra fees.
    5.  **Affects TPM limits?** No. Rate limits unchanged.
    6.  **Available on Scale Tier / Batch API?** Yes for Scale Tier (including spilled-over tokens). No for Batch API.
    7.  **Works with Zero Data Retention?** Yes, compliant with ZDR policies.

    *(Was this page useful?)*

### Predicted Outputs (General Guide - Duplicated Section)
=================

*(This section appears twice in the original input. Content is identical to the previous Predicted Outputs guide provided above under the "Related OpenAI Concepts" heading. It covers the definition, code refactoring example, streaming example, position independence, and limitations.)*

    *(Was this page useful?)*

### Latency optimization (General Guide)
====================

Improve latency across a wide variety of LLM-related use cases.

This guide covers the core set of principles you can apply to improve latency. Techniques come from working with customers/developers on production applications.

*   **Seven principles:**
    1.  [Process tokens faster.](#process-tokens-faster-1)
    2.  [Generate fewer tokens.](#generate-fewer-tokens-1)
    3.  [Use fewer input tokens.](#use-fewer-input-tokens-1)
    4.  [Make fewer requests.](#make-fewer-requests-1)
    5.  [Parallelize.](#parallelize-1)
    6.  [Make your users wait less.](#make-your-users-wait-less-1)
    7.  [Don't default to an LLM.](#don-t-default-to-an-llm-1)

*   **Process tokens faster**
    ---------------------
    Focuses on **inference speed** (TPM/TPS). Main factor is **model size** (smaller usually faster/cheaper). Maintain quality with smaller models via: detailed prompts, more few-shot examples, fine-tuning/distillation. Use inference optimizations like [**Predicted outputs**](/docs/guides/predicted-outputs) (speeds up generation when most output known, e.g., code editing).
    *(Deep dive link on Compute capacity & additional optimizations provided)*

*   **Generate fewer tokens**
    ---------------------
    Usually highest latency step. Cutting 50% output tokens  50% latency reduction.
    *   **Natural language:** Ask model to be concise ("under 20 words", "be brief"), use few-shot examples, fine-tune for shorter responses.
    *   **Structured output:** Minimize syntax (shorter names, omit args, coalesce params).
    *   Use `max_tokens` or `stop_tokens` to end generation early (less common).

*   **Use fewer input tokens**
    ----------------------
    Less impact than output tokens (cutting 50% prompt  1-5% latency improvement). Consider if working with massive contexts or need maximum optimization. Techniques:
    *   **Fine-tuning:** Replace lengthy instructions/examples.
    *   **Filtering context:** Prune RAG results, clean HTML.
    *   **Maximize shared prefix:** Put dynamic content (RAG, history) later in prompt. Improves KV cache friendliness. See [prompt caching](/docs/guides/prompt-engineering#prompt-caching).

*   **Make fewer requests**
    -------------------
    Reduce round-trip latency. If steps are sequential, combine into single prompt/response instead of multiple requests. Use structured output (e.g., JSON with named fields) to parse results easily.

*   **Parallelize**
    -----------
    *   **Non-sequential steps:** Split into parallel calls.
    *   **Sequential steps:** Use speculative execution if one outcome likely (e.g., moderation). Start step 1 & 2 -> Verify step 1 -> If expected, keep step 2; if not, cancel step 2. Zero added latency if guess correct.

*   **Make your users wait less**
    -------------------------
    Difference between waiting vs. watching progress. Techniques:
    *   **Streaming:** Most effective. Reduces perceived wait time significantly.
    *   **Chunking:** If output needs backend processing (moderation, translation), stream to backend, process in chunks, send processed chunks to frontend.
    *   **Show steps:** Surface multi-step/tool use progress to user.
    *   **Loading states:** Use spinners, progress bars.
    Note: Streaming/chunking genuinely reduce user time-to-finish-reading. Loading states/showing steps primarily psychological.

*   **Don't default to an LLM**
    -----------------------
    Use faster classical methods where appropriate. Examples:
    *   **Hard-coding:** For constrained outputs (confirmations, refusals, standard requests). Use variations.
    *   **Pre-computing:** For constrained inputs (e.g., category selection), generate responses in advance.
    *   **Leveraging UI:** Use bespoke UI for metrics, reports, search results instead of LLM text.
    *   **Traditional optimization:** Apply CS fundamentals (binary search, caching, hash maps, runtime complexity).

*   **Example**
    -------
    Walk-through optimizing hypothetical customer service bot.
    *   **Architecture and prompts:** Initial architecture diagram shown (User msg -> Contextualize Query (GPT-4) -> Retrieval Check (GPT-4) -> Retrieve Info -> Assistant Response (GPT-4, multi-step JSON reasoning) -> User). Prompts provided for each step.
    *   **Analysis and optimizations:**
        1.  *Part 1 (Retrieval Prompts):* Identify consecutive GPT-4 calls. **Combine** contextualization + retrieval check into single prompt (make fewer requests). **Switch** combined prompt to smaller, fine-tuned GPT-3.5 (process tokens faster). *(Diagrams updated)*.
        2.  *Part 2 (Assistant Prompt):* Identify multiple reasoning steps in JSON. Option 1: Parallelize (assume testing showed worse results). Option 2: Split prompt - use fine-tuned GPT-3.5 for reasoning fields, GPT-4 for final `response` generation (process most tokens faster, but adds request). Assume testing favors splitting. **Parallelize** retrieval and GPT-3.5 reasoning step (since reasoning no longer depends on retrieval). *(Diagrams updated)*.
        3.  *Part 3 (Optimizing Structured Output):* Shorten field names in reasoning JSON (e.g., `message_is_conversation_continuation` -> `cont`). Move descriptions to comments/prompt. (generate fewer tokens). Small change, bigger impact for larger outputs or slower models like GPT-4. *(Diagram and token count comparison image provided)*. Could go further (single chars, array) but risk quality loss - requires testing.
    *   **Example wrap-up:** Summary of optimizations applied (Combine prompts, Switch models, Split prompt, Parallelize, Shorten JSON). *(Final optimized diagram provided)*.

    *(Was this page useful?)*

### Optimizing LLM Accuracy (General Guide)
=======================

Maximize correctness and consistent behavior when working with LLMs.

*   **Why optimization is hard:** Knowing how/when to start, which method to use, defining "good enough". Need cost/benefit analysis for specific use case.
*   **LLM optimization context:** Not linear. Use prompt engineering baseline -> evaluate failures -> optimize based on diagnosis. Matrix axes:
    *   **Context optimization:** Fix knowledge gaps (outdated, missing, proprietary). Use RAG. Maximizes **response accuracy**.
    *   **LLM optimization:** Fix inconsistency (format, tone, reasoning). Use Prompt Eng (examples), Fine-tuning. Maximizes **consistency of behavior**.
    Optimization flow is iterative (evaluate -> hypothesize -> apply -> evaluate). *(Example optimization journey diagram provided)*.

*   **Prompt engineering:** Best starting point. Defines accuracy for use case. Optimize by adding context, instructions, examples. Use strategies from [Prompt Engineering guide](/docs/guides/prompt-engineering). Example: Icelandic error correction. Baseline GPT-4 -> Add few-shot examples (improves BLEU score significantly) -> Indicates behavior optimization needed. Scalability issues: Need dynamic context (RAG) or more consistency (Fine-tuning).
    *   *Deep dive link on using long context provided.*
    *   **Evaluation:** Output should be good prompt + eval set (20+ questions/answers). Diagnose failures. Automate evals: ROUGE/BERTScore (quick check), GPT-4 as evaluator (G-Eval). See [evaluation cookbook](https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarization).

*   **Understanding the tools:** Classify failures:
    *   **In-context memory problem:** Model lacks knowledge. Analogy: Textbook lookup during exam. Solution: Provide context (Prompt Eng, RAG).
    *   **Learned memory problem:** Model inconsistency/incorrect behavior despite knowledge. Analogy: Attending class, learning concepts. Solution: Show examples (Prompt Eng, Fine-tuning).
    *(Memory problem classification diagram provided)*
    Techniques are additive.

*   **Retrieval-augmented generation (RAG):** Retrieve context -> Augment prompt -> Generate answer. Gives model domain-specific context. Powerful accuracy/consistency tool. *(RAG diagram provided)*. Introduces retrieval optimization axis. Evaluate retrieval quality AND generation quality. Fix retrieval issues (wrong/noisy context) or LLM issues (misinterprets context). *(RAG evaluation grid diagram provided)*.

*   **Fine-tuning:** Continue training on domain-specific dataset. Reasons: Improve accuracy (learned memory), improve efficiency (same accuracy, smaller/cheaper model). Process: Prepare representative dataset (use prompt baking from logs) -> Train model (hyperparameters) -> Evaluate on hold-out set -> Deploy fine-tuned model. *(Fine-tuning process diagram provided)*. Best practices: Start with prompt eng baseline; Start small (50+ quality examples), scale up if needed; Ensure representative data (incl. RAG context if used). See [fine-tuning guide](/docs/guides/fine-tuning#analyzing-your-fine-tuned-model).

*   **All of the above:** Techniques stack. Fine-tuning minimizes prompt tokens, teaches complex behavior; RAG injects context. Trade-offs: RAG needs retrieval tuning, Fine-tuning needs dataset management/retraining. Squeeze max value from basic methods first.
*   **Using tools example (Icelandic Correction Revisited):**
    *   Hypothesis: Behavior problem.
    *   Fine-tuning: Train GPT-3.5-turbo / GPT-4 on 1000 examples. Results: GPT-3.5 FT (78 BLEU) outperforms GPT-4 few-shot (70). GPT-4 FT best (87 BLEU). Confirms behavior optimization effective.
    *   RAG + Fine-tuning: Embed 1000 external examples, retrieve top N, add to GPT-4 FT prompt. Result: Accuracy *decreased* (83 BLEU). Confirms RAG not needed/helpful when model learned task well via fine-tuning. *(Bar chart showing BLEU scores provided)*.

*   **How much accuracy is good enough for production:**
    *   **Business context:** Identify success/failure cases, estimate cost/value. Measure baseline metrics (CSAT, decision accuracy, time-to-resolution) for human vs. AI. Determine acceptable accuracy/failure rate based on business value/risk. Example: Customer service bot cost/saving analysis table provided. Use humans for highest-risk tasks (e.g., fraud).
    *   **Technical context:** Build solution to handle failures gracefully. Options: Prompt for clarification, allow UX self-healing (retry/escalate), hand off to human proactively. Trade off UX speed vs. accuracy vs. cost based on business decision.

*   **Taking this forward:** Use framework to optimize. See customer stories ([Morgan Stanley](https://openai.com/customer-stories/morgan-stanley), [Klarna](https://openai.com/customer-stories/klarna)) for inspiration.

    *(Was this page useful?)*

### Advanced usage (General Guide)
==============

Use advanced techniques for reproducibility and parameter tuning.

OpenAI's text generation models understand natural language, code, images. Provide text outputs (completions) in response to inputs (prompts). Designing prompts = "programming" the model.

*   **Reproducible outputs**
    --------------------
    Chat Completions non-deterministic by default. Control offered via `seed` parameter and `system_fingerprint` response field.
    To get (mostly) deterministic outputs:
    *   Set `seed` parameter to consistent integer value across requests.
    *   Ensure all other parameters (`prompt`, `temperature`, etc.) are identical.
    Changes OpenAI makes to model configs can impact determinism. Check `system_fingerprint` field; different value means system changes may cause different outputs.
    [Link: Deterministic outputs cookbook](https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter)

*   **Managing tokens**
    ---------------
    Models read/write text in chunks (tokens). English: token  1 char to 1 word. Other languages vary. Rule of thumb: 1 token  4 chars / 0.75 words (English).
    Use [Tokenizer tool](https://platform.openai.com/tokenizer) to check strings.
    Example: `"ChatGPT is great!"` = 6 tokens: `["Chat", "G", "PT", " is", " great", "!"]`.
    Total tokens affect: Cost (pay per token), Latency (more tokens = more time), Validity (must be < model max limit, e.g., 4097 for `gpt-3.5-turbo`).
    Input + Output tokens count. Check `usage` field in response (e.g., `response['usage']['total_tokens']`). Some models price input/output tokens differently ([pricing page](https://openai.com/api/pricing)).
    Chat models (`gpt-3.5-turbo`, `gpt-4-turbo-preview`) use tokens similarly, but message formatting makes counting harder.
    *   **Deep dive: Counting tokens for chat API calls:** Use OpenAIs [`tiktoken`](https://github.com/openai/tiktoken) Python library. Example code in [Cookbook guide](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken). Each message consumes tokens for content, role, fields + formatting overhead. Truncate/omit messages if exceeding model max limit (loses knowledge of removed messages). Long conversations risk incomplete replies.

*   **Parameter details**
    -----------------
    *   **Frequency and presence penalties:** Reduce likelihood of repetitive token sequences. Found in Chat Completions / Legacy Completions APIs.
        *   *Deep dive: Penalties behind the scenes:* Reasonable values ~0.1-1 for slight reduction. Up to 2 for strong suppression (can degrade quality). Negative values increase repetition likelihood.
    *   **Token log probabilities:** `logprobs` parameter (Chat Completions / Legacy Completions) provides log probabilities of output tokens + top alternatives. Useful for assessing confidence, examining alternatives.
    *   **Other parameters:** See full [API reference](https://platform.openai.com/docs/api-reference/chat).

    *(Was this page useful?)*

### Responses vs. Chat Completions (General Guide)
==============================

Compare the Responses API and Chat Completions API.

*   **Why the Responses API?** Newest core API, agentic primitive. Combines Chat Completions simplicity + agentic tasks. Flexible foundation for action-oriented apps. Built-in tools: Web search, File search, Computer use. Recommended for new users.
    | Capabilities         | Chat Completions API | Responses API |
    |----------------------|----------------------|---------------|
    | Text generation      |                    |             |
    | Audio                |                    | Coming soon   |
    | Vision               |                    |             |
    | Structured Outputs   |                    |             |
    | Function calling     |                    |             |
    | Web search           |                      |             |
    | File search          |                      |             |
    | Computer use         |                      |             |
    | Code interpreter     |  (Assistants)      | Coming soon   |
*   **Chat Completions API is not going away:** Industry standard, support continues indefinitely. Responses API simplifies tool use, code execution, state management; future direction for OpenAI platform enhancements.
*   **Stateful API and semantic events:** Responses API has predictable, event-driven architecture (clear events for specific changes). Chat Completions appends to content field (requires manual diffing). Responses API easier for multi-step logic/reasoning.
*   **Model availability:** New models added to both where possible. Models using built-in tools (Computer use) or multi-turn reasoning (o1-pro) may be Responses API only. Check [model pages](/docs/models).

*   **Compare the code:**
    *   **Text generation example:** Responses API uses `input` (string or array), Chat Completions uses `messages` array. *(Python examples provided for both)*
    *   **Response structure:** Responses API returns typed `response` object with `id`, stored by default. Chat Completions returns `choices` array with `message`, stored by default for new accounts. *(Example JSON responses provided for both)*
    *   **Other noteworthy differences:** `output` (Responses) vs. `choices` (Chat); Structured Outputs param (`text.format` vs. `response_format`); Function calling shape (request config, response format); Reasoning param (`reasoning.effort` vs. `reasoning_effort`); Responses SDK `output_text` helper; Conversation state (`previous_response_id` in Responses vs. manual in Chat); Storage default (`store: false` to disable).

*   **What this means for existing APIs:**
    *   **Chat Completions:** Remains widely used. Support continues. Use if built-in tools not needed. New models added if capabilities don't depend on tools/multi-turn. Responses API recommended for advanced agent workflows.
    *   **Assistants:** Responses API incorporates feedback from Assistants beta (more flexible, faster, easier). Responses API is future direction. Working towards feature parity (Assistant/Thread objects, Code Interpreter). Plan to deprecate Assistants API (target H1 2026 sunset). Migration guide will be provided. Continue adding new models to Assistants API until deprecation announced.


